{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8e8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c16950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = pd.read_csv('train.csv', index_col=['sequence', 'subject', 'step'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28f53a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sensor_00</th>\n",
       "      <th>sensor_01</th>\n",
       "      <th>sensor_02</th>\n",
       "      <th>sensor_03</th>\n",
       "      <th>sensor_04</th>\n",
       "      <th>sensor_05</th>\n",
       "      <th>sensor_06</th>\n",
       "      <th>sensor_07</th>\n",
       "      <th>sensor_08</th>\n",
       "      <th>sensor_09</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence</th>\n",
       "      <th>subject</th>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">47</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.196291</td>\n",
       "      <td>0.112395</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329204</td>\n",
       "      <td>-1.004660</td>\n",
       "      <td>-0.131638</td>\n",
       "      <td>-0.127505</td>\n",
       "      <td>0.368702</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.963873</td>\n",
       "      <td>-0.985069</td>\n",
       "      <td>0.531893</td>\n",
       "      <td>4.751492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.447450</td>\n",
       "      <td>0.134454</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.658407</td>\n",
       "      <td>0.162495</td>\n",
       "      <td>0.340314</td>\n",
       "      <td>-0.209472</td>\n",
       "      <td>-0.867176</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.301301</td>\n",
       "      <td>0.082733</td>\n",
       "      <td>-0.231481</td>\n",
       "      <td>0.454390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.326893</td>\n",
       "      <td>-0.694328</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.330088</td>\n",
       "      <td>0.473678</td>\n",
       "      <td>1.280479</td>\n",
       "      <td>-0.094718</td>\n",
       "      <td>0.535878</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.002168</td>\n",
       "      <td>0.449221</td>\n",
       "      <td>-0.586420</td>\n",
       "      <td>-4.736147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.523184</td>\n",
       "      <td>0.751050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976991</td>\n",
       "      <td>-0.563287</td>\n",
       "      <td>-0.720269</td>\n",
       "      <td>0.793260</td>\n",
       "      <td>0.951145</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.995665</td>\n",
       "      <td>-0.434290</td>\n",
       "      <td>1.344650</td>\n",
       "      <td>0.429241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.272025</td>\n",
       "      <td>1.074580</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.136283</td>\n",
       "      <td>0.398579</td>\n",
       "      <td>0.044877</td>\n",
       "      <td>0.560109</td>\n",
       "      <td>-0.541985</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>1.055636</td>\n",
       "      <td>0.812631</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>-0.223359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sensor_00  sensor_01  sensor_02  sensor_03  sensor_04  \\\n",
       "sequence subject step                                                          \n",
       "0        47      0     -0.196291   0.112395        1.0   0.329204  -1.004660   \n",
       "                 1     -0.447450   0.134454        1.0  -0.658407   0.162495   \n",
       "                 2      0.326893  -0.694328        1.0   0.330088   0.473678   \n",
       "                 3      0.523184   0.751050        1.0   0.976991  -0.563287   \n",
       "                 4      0.272025   1.074580        1.0  -0.136283   0.398579   \n",
       "\n",
       "                       sensor_05  sensor_06  sensor_07  sensor_08  sensor_09  \\\n",
       "sequence subject step                                                          \n",
       "0        47      0     -0.131638  -0.127505   0.368702       -0.1  -0.963873   \n",
       "                 1      0.340314  -0.209472  -0.867176        0.2  -0.301301   \n",
       "                 2      1.280479  -0.094718   0.535878        1.4   1.002168   \n",
       "                 3     -0.720269   0.793260   0.951145       -0.3  -0.995665   \n",
       "                 4      0.044877   0.560109  -0.541985       -0.9   1.055636   \n",
       "\n",
       "                       sensor_10  sensor_11  sensor_12  \n",
       "sequence subject step                                   \n",
       "0        47      0     -0.985069   0.531893   4.751492  \n",
       "                 1      0.082733  -0.231481   0.454390  \n",
       "                 2      0.449221  -0.586420  -4.736147  \n",
       "                 3     -0.434290   1.344650   0.429241  \n",
       "                 4      0.812631   0.123457  -0.223359  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88aff5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25963</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25964</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25965</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25966</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25967</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25968 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          state\n",
       "sequence       \n",
       "0             0\n",
       "1             1\n",
       "2             1\n",
       "3             1\n",
       "4             1\n",
       "...         ...\n",
       "25963         1\n",
       "25964         0\n",
       "25965         1\n",
       "25966         1\n",
       "25967         0\n",
       "\n",
       "[25968 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = pd.read_csv('train_labels.csv', index_col=['sequence'])\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb1f7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scs = {}\n",
    "for col in train_series.columns:\n",
    "    sc = StandardScaler()\n",
    "    train_series[col] = sc.fit_transform(train_series[col].values.reshape(-1,1))\n",
    "    scs[col] = sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76b4d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sensor_00</th>\n",
       "      <th>sensor_01</th>\n",
       "      <th>sensor_02</th>\n",
       "      <th>sensor_03</th>\n",
       "      <th>sensor_04</th>\n",
       "      <th>sensor_05</th>\n",
       "      <th>sensor_06</th>\n",
       "      <th>sensor_07</th>\n",
       "      <th>sensor_08</th>\n",
       "      <th>sensor_09</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence</th>\n",
       "      <th>subject</th>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">47</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.073994</td>\n",
       "      <td>0.025755</td>\n",
       "      <td>0.529941</td>\n",
       "      <td>0.084226</td>\n",
       "      <td>-0.595617</td>\n",
       "      <td>-0.081710</td>\n",
       "      <td>-0.037993</td>\n",
       "      <td>0.113685</td>\n",
       "      <td>-0.022186</td>\n",
       "      <td>-0.372260</td>\n",
       "      <td>-0.513943</td>\n",
       "      <td>0.118173</td>\n",
       "      <td>0.121766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.168462</td>\n",
       "      <td>0.030764</td>\n",
       "      <td>0.529941</td>\n",
       "      <td>-0.166807</td>\n",
       "      <td>0.097598</td>\n",
       "      <td>0.214962</td>\n",
       "      <td>-0.062496</td>\n",
       "      <td>-0.267356</td>\n",
       "      <td>0.044458</td>\n",
       "      <td>-0.116728</td>\n",
       "      <td>0.042977</td>\n",
       "      <td>-0.050247</td>\n",
       "      <td>0.011916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.122789</td>\n",
       "      <td>-0.157416</td>\n",
       "      <td>0.529941</td>\n",
       "      <td>0.084451</td>\n",
       "      <td>0.282421</td>\n",
       "      <td>0.805957</td>\n",
       "      <td>-0.028192</td>\n",
       "      <td>0.165228</td>\n",
       "      <td>0.311034</td>\n",
       "      <td>0.385976</td>\n",
       "      <td>0.234122</td>\n",
       "      <td>-0.128556</td>\n",
       "      <td>-0.120775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.196619</td>\n",
       "      <td>0.170766</td>\n",
       "      <td>0.529941</td>\n",
       "      <td>0.248882</td>\n",
       "      <td>-0.333470</td>\n",
       "      <td>-0.451728</td>\n",
       "      <td>0.237261</td>\n",
       "      <td>0.293261</td>\n",
       "      <td>-0.066615</td>\n",
       "      <td>-0.384521</td>\n",
       "      <td>-0.226680</td>\n",
       "      <td>0.297488</td>\n",
       "      <td>0.011273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.102151</td>\n",
       "      <td>0.244225</td>\n",
       "      <td>0.529941</td>\n",
       "      <td>-0.034093</td>\n",
       "      <td>0.237816</td>\n",
       "      <td>0.029248</td>\n",
       "      <td>0.167563</td>\n",
       "      <td>-0.167094</td>\n",
       "      <td>-0.199903</td>\n",
       "      <td>0.406597</td>\n",
       "      <td>0.423661</td>\n",
       "      <td>0.028061</td>\n",
       "      <td>-0.005410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">25967</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">327</th>\n",
       "      <th>55</th>\n",
       "      <td>-0.106549</td>\n",
       "      <td>-0.276191</td>\n",
       "      <td>-0.630265</td>\n",
       "      <td>0.149683</td>\n",
       "      <td>-0.551688</td>\n",
       "      <td>-0.282470</td>\n",
       "      <td>-0.194542</td>\n",
       "      <td>0.113685</td>\n",
       "      <td>0.088887</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>-0.377539</td>\n",
       "      <td>-0.077258</td>\n",
       "      <td>-0.023085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.048959</td>\n",
       "      <td>0.079657</td>\n",
       "      <td>-0.630265</td>\n",
       "      <td>-0.082005</td>\n",
       "      <td>0.461579</td>\n",
       "      <td>-0.208184</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.037665</td>\n",
       "      <td>-0.044400</td>\n",
       "      <td>0.248039</td>\n",
       "      <td>0.360436</td>\n",
       "      <td>-0.134457</td>\n",
       "      <td>-0.012885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.218166</td>\n",
       "      <td>0.097783</td>\n",
       "      <td>-0.630265</td>\n",
       "      <td>0.081752</td>\n",
       "      <td>0.184530</td>\n",
       "      <td>0.178760</td>\n",
       "      <td>-0.153159</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>-0.355406</td>\n",
       "      <td>-0.164101</td>\n",
       "      <td>0.373708</td>\n",
       "      <td>0.360135</td>\n",
       "      <td>0.024033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.480893</td>\n",
       "      <td>0.388758</td>\n",
       "      <td>-0.567891</td>\n",
       "      <td>0.204569</td>\n",
       "      <td>-0.272446</td>\n",
       "      <td>-0.033754</td>\n",
       "      <td>0.719161</td>\n",
       "      <td>0.013423</td>\n",
       "      <td>0.422107</td>\n",
       "      <td>0.108987</td>\n",
       "      <td>-0.477354</td>\n",
       "      <td>0.081175</td>\n",
       "      <td>0.005705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.427448</td>\n",
       "      <td>-0.840493</td>\n",
       "      <td>-0.485211</td>\n",
       "      <td>-0.194250</td>\n",
       "      <td>-0.254322</td>\n",
       "      <td>-0.056792</td>\n",
       "      <td>-0.751041</td>\n",
       "      <td>-0.007053</td>\n",
       "      <td>-0.244332</td>\n",
       "      <td>-0.252715</td>\n",
       "      <td>-0.218453</td>\n",
       "      <td>-0.319900</td>\n",
       "      <td>-0.039615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1558080 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       sensor_00  sensor_01  sensor_02  sensor_03  sensor_04  \\\n",
       "sequence subject step                                                          \n",
       "0        47      0     -0.073994   0.025755   0.529941   0.084226  -0.595617   \n",
       "                 1     -0.168462   0.030764   0.529941  -0.166807   0.097598   \n",
       "                 2      0.122789  -0.157416   0.529941   0.084451   0.282421   \n",
       "                 3      0.196619   0.170766   0.529941   0.248882  -0.333470   \n",
       "                 4      0.102151   0.244225   0.529941  -0.034093   0.237816   \n",
       "...                          ...        ...        ...        ...        ...   \n",
       "25967    327     55    -0.106549  -0.276191  -0.630265   0.149683  -0.551688   \n",
       "                 56     0.048959   0.079657  -0.630265  -0.082005   0.461579   \n",
       "                 57    -0.218166   0.097783  -0.630265   0.081752   0.184530   \n",
       "                 58     0.480893   0.388758  -0.567891   0.204569  -0.272446   \n",
       "                 59    -0.427448  -0.840493  -0.485211  -0.194250  -0.254322   \n",
       "\n",
       "                       sensor_05  sensor_06  sensor_07  sensor_08  sensor_09  \\\n",
       "sequence subject step                                                          \n",
       "0        47      0     -0.081710  -0.037993   0.113685  -0.022186  -0.372260   \n",
       "                 1      0.214962  -0.062496  -0.267356   0.044458  -0.116728   \n",
       "                 2      0.805957  -0.028192   0.165228   0.311034   0.385976   \n",
       "                 3     -0.451728   0.237261   0.293261  -0.066615  -0.384521   \n",
       "                 4      0.029248   0.167563  -0.167094  -0.199903   0.406597   \n",
       "...                          ...        ...        ...        ...        ...   \n",
       "25967    327     55    -0.282470  -0.194542   0.113685   0.088887   0.002817   \n",
       "                 56    -0.208184   0.029800   0.037665  -0.044400   0.248039   \n",
       "                 57     0.178760  -0.153159   0.003774  -0.355406  -0.164101   \n",
       "                 58    -0.033754   0.719161   0.013423   0.422107   0.108987   \n",
       "                 59    -0.056792  -0.751041  -0.007053  -0.244332  -0.252715   \n",
       "\n",
       "                       sensor_10  sensor_11  sensor_12  \n",
       "sequence subject step                                   \n",
       "0        47      0     -0.513943   0.118173   0.121766  \n",
       "                 1      0.042977  -0.050247   0.011916  \n",
       "                 2      0.234122  -0.128556  -0.120775  \n",
       "                 3     -0.226680   0.297488   0.011273  \n",
       "                 4      0.423661   0.028061  -0.005410  \n",
       "...                          ...        ...        ...  \n",
       "25967    327     55    -0.377539  -0.077258  -0.023085  \n",
       "                 56     0.360436  -0.134457  -0.012885  \n",
       "                 57     0.373708   0.360135   0.024033  \n",
       "                 58    -0.477354   0.081175   0.005705  \n",
       "                 59    -0.218453  -0.319900  -0.039615  \n",
       "\n",
       "[1558080 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6cf0c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1558080"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25968*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02186c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1558080, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "848e7e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25968"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_series.index.get_level_values(0).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fd4848a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels['state'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f87334b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, series, labels=None):\n",
    "        self.X = series.values.reshape(-1,60,13).transpose([0,2,1]).copy()\n",
    "        if labels is None:\n",
    "            self.y = None\n",
    "        else:\n",
    "            self.y = labels['state'].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' input tensor shape is N*C*L '''\n",
    "        X = self.X[idx]\n",
    "        if self.y is None:\n",
    "            y = -1\n",
    "        else:\n",
    "            y = self.y[idx]\n",
    "        return (torch.tensor(X, dtype=torch.float32), torch.tensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7615c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = MyDataset(train_series, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ba73d6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sensor_00</th>\n",
       "      <th>sensor_01</th>\n",
       "      <th>sensor_02</th>\n",
       "      <th>sensor_03</th>\n",
       "      <th>sensor_04</th>\n",
       "      <th>sensor_05</th>\n",
       "      <th>sensor_06</th>\n",
       "      <th>sensor_07</th>\n",
       "      <th>sensor_08</th>\n",
       "      <th>sensor_09</th>\n",
       "      <th>sensor_10</th>\n",
       "      <th>sensor_11</th>\n",
       "      <th>sensor_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"60\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>-0.055973</td>\n",
       "      <td>-0.022184</td>\n",
       "      <td>0.275540</td>\n",
       "      <td>0.156882</td>\n",
       "      <td>0.146349</td>\n",
       "      <td>0.104004</td>\n",
       "      <td>-0.043711</td>\n",
       "      <td>0.219830</td>\n",
       "      <td>0.177746</td>\n",
       "      <td>-0.076044</td>\n",
       "      <td>0.054113</td>\n",
       "      <td>0.127479</td>\n",
       "      <td>-0.002021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.030065</td>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>0.101771</td>\n",
       "      <td>0.288743</td>\n",
       "      <td>0.081906</td>\n",
       "      <td>-0.009678</td>\n",
       "      <td>0.087796</td>\n",
       "      <td>0.288819</td>\n",
       "      <td>-0.474808</td>\n",
       "      <td>0.135277</td>\n",
       "      <td>0.071642</td>\n",
       "      <td>-0.000517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.166427</td>\n",
       "      <td>0.014784</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>-0.092577</td>\n",
       "      <td>-0.175286</td>\n",
       "      <td>-0.169161</td>\n",
       "      <td>-0.106330</td>\n",
       "      <td>-0.144736</td>\n",
       "      <td>-0.666411</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>-0.123275</td>\n",
       "      <td>-0.023690</td>\n",
       "      <td>-0.004865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100989</td>\n",
       "      <td>-0.039357</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>-0.059736</td>\n",
       "      <td>-0.203118</td>\n",
       "      <td>-0.049269</td>\n",
       "      <td>0.099498</td>\n",
       "      <td>-0.018350</td>\n",
       "      <td>0.288819</td>\n",
       "      <td>0.328015</td>\n",
       "      <td>-0.057277</td>\n",
       "      <td>-0.092693</td>\n",
       "      <td>0.010510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.120464</td>\n",
       "      <td>0.057476</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>0.018993</td>\n",
       "      <td>0.035463</td>\n",
       "      <td>-0.029522</td>\n",
       "      <td>0.118284</td>\n",
       "      <td>0.021190</td>\n",
       "      <td>0.155531</td>\n",
       "      <td>0.046288</td>\n",
       "      <td>0.018720</td>\n",
       "      <td>0.019209</td>\n",
       "      <td>-0.002937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.211190</td>\n",
       "      <td>0.021223</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>-0.011374</td>\n",
       "      <td>0.074745</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>-0.128383</td>\n",
       "      <td>-0.070599</td>\n",
       "      <td>-0.310976</td>\n",
       "      <td>-0.329904</td>\n",
       "      <td>0.023114</td>\n",
       "      <td>0.025792</td>\n",
       "      <td>-0.001520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.001908</td>\n",
       "      <td>-0.100891</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>-0.075931</td>\n",
       "      <td>0.075476</td>\n",
       "      <td>-0.030463</td>\n",
       "      <td>-0.061680</td>\n",
       "      <td>-0.019762</td>\n",
       "      <td>0.466537</td>\n",
       "      <td>0.018980</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>-0.103134</td>\n",
       "      <td>0.000921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.044308</td>\n",
       "      <td>0.059623</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>0.088725</td>\n",
       "      <td>0.118581</td>\n",
       "      <td>0.069682</td>\n",
       "      <td>0.045863</td>\n",
       "      <td>0.072968</td>\n",
       "      <td>-0.222118</td>\n",
       "      <td>-0.105303</td>\n",
       "      <td>0.089839</td>\n",
       "      <td>0.100695</td>\n",
       "      <td>-0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.051575</td>\n",
       "      <td>0.070117</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>0.127414</td>\n",
       "      <td>-0.196704</td>\n",
       "      <td>-0.110860</td>\n",
       "      <td>0.040145</td>\n",
       "      <td>0.070379</td>\n",
       "      <td>0.111102</td>\n",
       "      <td>0.324671</td>\n",
       "      <td>-0.191485</td>\n",
       "      <td>0.127252</td>\n",
       "      <td>-0.002708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.311433</td>\n",
       "      <td>-0.023615</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>-0.099100</td>\n",
       "      <td>-0.078496</td>\n",
       "      <td>0.059339</td>\n",
       "      <td>0.134347</td>\n",
       "      <td>-0.029882</td>\n",
       "      <td>0.088887</td>\n",
       "      <td>0.462886</td>\n",
       "      <td>-0.091246</td>\n",
       "      <td>-0.124016</td>\n",
       "      <td>0.006347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.303623</td>\n",
       "      <td>0.025039</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>-0.051638</td>\n",
       "      <td>0.019515</td>\n",
       "      <td>0.364474</td>\n",
       "      <td>-0.153976</td>\n",
       "      <td>-0.115787</td>\n",
       "      <td>-0.310976</td>\n",
       "      <td>-0.231537</td>\n",
       "      <td>0.029326</td>\n",
       "      <td>-0.029365</td>\n",
       "      <td>-0.002860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.052775</td>\n",
       "      <td>-0.083480</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.053725</td>\n",
       "      <td>0.408669</td>\n",
       "      <td>-0.057868</td>\n",
       "      <td>0.084030</td>\n",
       "      <td>0.555395</td>\n",
       "      <td>-0.430500</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.040999</td>\n",
       "      <td>0.001466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.285564</td>\n",
       "      <td>0.068686</td>\n",
       "      <td>0.384887</td>\n",
       "      <td>0.038563</td>\n",
       "      <td>0.183993</td>\n",
       "      <td>0.497530</td>\n",
       "      <td>0.216025</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>-0.488694</td>\n",
       "      <td>0.604725</td>\n",
       "      <td>0.148989</td>\n",
       "      <td>-0.015746</td>\n",
       "      <td>-0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.044346</td>\n",
       "      <td>0.068924</td>\n",
       "      <td>0.347956</td>\n",
       "      <td>0.053184</td>\n",
       "      <td>-0.013223</td>\n",
       "      <td>0.122810</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>-0.005170</td>\n",
       "      <td>0.444322</td>\n",
       "      <td>-0.264976</td>\n",
       "      <td>-0.023551</td>\n",
       "      <td>0.111363</td>\n",
       "      <td>-0.003961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.219038</td>\n",
       "      <td>-0.127365</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>-0.051638</td>\n",
       "      <td>-0.362672</td>\n",
       "      <td>0.122810</td>\n",
       "      <td>-0.146625</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>-0.288762</td>\n",
       "      <td>-0.365015</td>\n",
       "      <td>-0.243528</td>\n",
       "      <td>-0.129917</td>\n",
       "      <td>0.008472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.005068</td>\n",
       "      <td>0.019554</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>-0.036567</td>\n",
       "      <td>-0.041639</td>\n",
       "      <td>-0.097696</td>\n",
       "      <td>-0.026558</td>\n",
       "      <td>-0.058360</td>\n",
       "      <td>-0.133259</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>-0.037172</td>\n",
       "      <td>0.023068</td>\n",
       "      <td>-0.002991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.065855</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>-0.119345</td>\n",
       "      <td>0.153920</td>\n",
       "      <td>-0.202542</td>\n",
       "      <td>-0.041805</td>\n",
       "      <td>-0.171566</td>\n",
       "      <td>0.266605</td>\n",
       "      <td>0.117347</td>\n",
       "      <td>0.180972</td>\n",
       "      <td>-0.079982</td>\n",
       "      <td>-0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.023708</td>\n",
       "      <td>-0.051997</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>0.039913</td>\n",
       "      <td>0.387180</td>\n",
       "      <td>-0.384495</td>\n",
       "      <td>-0.008045</td>\n",
       "      <td>0.096033</td>\n",
       "      <td>0.199961</td>\n",
       "      <td>-0.101123</td>\n",
       "      <td>0.201441</td>\n",
       "      <td>-0.030273</td>\n",
       "      <td>-0.000735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.439909</td>\n",
       "      <td>0.108993</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>0.369225</td>\n",
       "      <td>-0.101155</td>\n",
       "      <td>-0.440915</td>\n",
       "      <td>0.331463</td>\n",
       "      <td>0.448361</td>\n",
       "      <td>-0.244332</td>\n",
       "      <td>0.178652</td>\n",
       "      <td>-0.170864</td>\n",
       "      <td>0.189445</td>\n",
       "      <td>-0.002348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.326876</td>\n",
       "      <td>-0.054621</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>-0.282427</td>\n",
       "      <td>-0.376066</td>\n",
       "      <td>-0.188437</td>\n",
       "      <td>-0.328222</td>\n",
       "      <td>-0.313015</td>\n",
       "      <td>0.222175</td>\n",
       "      <td>-0.100009</td>\n",
       "      <td>-0.242907</td>\n",
       "      <td>-0.121746</td>\n",
       "      <td>0.003155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.234115</td>\n",
       "      <td>-0.004058</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>0.060157</td>\n",
       "      <td>-0.114706</td>\n",
       "      <td>-0.109450</td>\n",
       "      <td>0.282729</td>\n",
       "      <td>0.088266</td>\n",
       "      <td>0.088887</td>\n",
       "      <td>0.273397</td>\n",
       "      <td>0.020069</td>\n",
       "      <td>0.066875</td>\n",
       "      <td>0.005421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.332690</td>\n",
       "      <td>-0.087773</td>\n",
       "      <td>0.204464</td>\n",
       "      <td>0.116392</td>\n",
       "      <td>0.501879</td>\n",
       "      <td>-0.057262</td>\n",
       "      <td>-0.350002</td>\n",
       "      <td>0.122158</td>\n",
       "      <td>-0.444264</td>\n",
       "      <td>-0.469792</td>\n",
       "      <td>0.210229</td>\n",
       "      <td>0.102511</td>\n",
       "      <td>-0.004440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.048706</td>\n",
       "      <td>0.015261</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>-0.051413</td>\n",
       "      <td>0.214732</td>\n",
       "      <td>-0.023410</td>\n",
       "      <td>0.018909</td>\n",
       "      <td>-0.173449</td>\n",
       "      <td>0.288819</td>\n",
       "      <td>0.148278</td>\n",
       "      <td>-0.007627</td>\n",
       "      <td>-0.027776</td>\n",
       "      <td>-0.004876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.333815</td>\n",
       "      <td>0.172435</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>-0.061310</td>\n",
       "      <td>-0.726922</td>\n",
       "      <td>-0.003663</td>\n",
       "      <td>0.280278</td>\n",
       "      <td>-0.130850</td>\n",
       "      <td>-0.044400</td>\n",
       "      <td>0.462050</td>\n",
       "      <td>-0.170955</td>\n",
       "      <td>-0.101545</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.185321</td>\n",
       "      <td>-0.166957</td>\n",
       "      <td>0.059297</td>\n",
       "      <td>-0.143638</td>\n",
       "      <td>-0.301046</td>\n",
       "      <td>0.043353</td>\n",
       "      <td>-0.217140</td>\n",
       "      <td>-0.048240</td>\n",
       "      <td>0.466537</td>\n",
       "      <td>-0.219554</td>\n",
       "      <td>0.033690</td>\n",
       "      <td>-0.131052</td>\n",
       "      <td>0.010183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.089362</td>\n",
       "      <td>0.044120</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>0.234711</td>\n",
       "      <td>0.399425</td>\n",
       "      <td>0.140677</td>\n",
       "      <td>0.080440</td>\n",
       "      <td>0.322916</td>\n",
       "      <td>-0.488694</td>\n",
       "      <td>-0.147660</td>\n",
       "      <td>0.244167</td>\n",
       "      <td>0.254588</td>\n",
       "      <td>-0.007382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.026615</td>\n",
       "      <td>0.037442</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>0.052284</td>\n",
       "      <td>0.557859</td>\n",
       "      <td>0.107295</td>\n",
       "      <td>0.031161</td>\n",
       "      <td>0.011776</td>\n",
       "      <td>-0.066615</td>\n",
       "      <td>-0.056259</td>\n",
       "      <td>0.137777</td>\n",
       "      <td>0.032601</td>\n",
       "      <td>-0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.240838</td>\n",
       "      <td>-0.057245</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>-0.194250</td>\n",
       "      <td>-0.204886</td>\n",
       "      <td>-0.129197</td>\n",
       "      <td>-0.131650</td>\n",
       "      <td>-0.273475</td>\n",
       "      <td>0.799757</td>\n",
       "      <td>-0.072700</td>\n",
       "      <td>-0.303572</td>\n",
       "      <td>-0.150573</td>\n",
       "      <td>0.000387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.248067</td>\n",
       "      <td>0.031002</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>-0.024645</td>\n",
       "      <td>-0.496597</td>\n",
       "      <td>0.115288</td>\n",
       "      <td>0.150138</td>\n",
       "      <td>0.040960</td>\n",
       "      <td>-0.266547</td>\n",
       "      <td>0.235778</td>\n",
       "      <td>-0.212211</td>\n",
       "      <td>-0.069767</td>\n",
       "      <td>0.002991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.201017</td>\n",
       "      <td>-0.068931</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>0.080402</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>-0.122614</td>\n",
       "      <td>-0.151798</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>-0.310976</td>\n",
       "      <td>0.049911</td>\n",
       "      <td>0.081112</td>\n",
       "      <td>0.084807</td>\n",
       "      <td>0.003275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.309980</td>\n",
       "      <td>0.156932</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>0.485071</td>\n",
       "      <td>-0.010246</td>\n",
       "      <td>0.151772</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>0.022244</td>\n",
       "      <td>0.243580</td>\n",
       "      <td>0.277560</td>\n",
       "      <td>0.055299</td>\n",
       "      <td>-0.003122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.013788</td>\n",
       "      <td>-0.008351</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>0.111894</td>\n",
       "      <td>0.351590</td>\n",
       "      <td>0.145848</td>\n",
       "      <td>0.027077</td>\n",
       "      <td>0.192764</td>\n",
       "      <td>0.044458</td>\n",
       "      <td>-0.303988</td>\n",
       "      <td>0.015827</td>\n",
       "      <td>0.074366</td>\n",
       "      <td>-0.004669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.246361</td>\n",
       "      <td>-0.084911</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>-0.148587</td>\n",
       "      <td>-0.470319</td>\n",
       "      <td>-0.128727</td>\n",
       "      <td>-0.089722</td>\n",
       "      <td>-0.252528</td>\n",
       "      <td>0.288819</td>\n",
       "      <td>-0.264419</td>\n",
       "      <td>-0.262406</td>\n",
       "      <td>-0.173952</td>\n",
       "      <td>0.003275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.158579</td>\n",
       "      <td>-0.074417</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>-0.062435</td>\n",
       "      <td>-0.442755</td>\n",
       "      <td>-0.426810</td>\n",
       "      <td>-0.138185</td>\n",
       "      <td>-0.036472</td>\n",
       "      <td>-0.088830</td>\n",
       "      <td>-0.171903</td>\n",
       "      <td>-0.038959</td>\n",
       "      <td>-0.048431</td>\n",
       "      <td>0.002828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.093431</td>\n",
       "      <td>0.102792</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>0.016294</td>\n",
       "      <td>0.037943</td>\n",
       "      <td>0.279375</td>\n",
       "      <td>0.042051</td>\n",
       "      <td>0.078146</td>\n",
       "      <td>0.199961</td>\n",
       "      <td>0.134624</td>\n",
       "      <td>0.102839</td>\n",
       "      <td>0.067102</td>\n",
       "      <td>0.003340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.242835</td>\n",
       "      <td>0.042689</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>-0.082680</td>\n",
       "      <td>0.431358</td>\n",
       "      <td>0.440641</td>\n",
       "      <td>0.150955</td>\n",
       "      <td>-0.127790</td>\n",
       "      <td>0.155531</td>\n",
       "      <td>0.507472</td>\n",
       "      <td>0.225516</td>\n",
       "      <td>-0.057283</td>\n",
       "      <td>-0.004222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.022799</td>\n",
       "      <td>-0.051282</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>0.067805</td>\n",
       "      <td>0.180041</td>\n",
       "      <td>0.381870</td>\n",
       "      <td>0.019454</td>\n",
       "      <td>0.158638</td>\n",
       "      <td>-0.777484</td>\n",
       "      <td>-0.030622</td>\n",
       "      <td>-0.106700</td>\n",
       "      <td>0.017393</td>\n",
       "      <td>-0.005258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.160032</td>\n",
       "      <td>-0.107807</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>0.129214</td>\n",
       "      <td>-0.316485</td>\n",
       "      <td>0.306644</td>\n",
       "      <td>-0.067125</td>\n",
       "      <td>0.119804</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.374768</td>\n",
       "      <td>-0.164001</td>\n",
       "      <td>0.069826</td>\n",
       "      <td>0.007121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.049578</td>\n",
       "      <td>0.075125</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>-0.128792</td>\n",
       "      <td>-0.123453</td>\n",
       "      <td>0.037711</td>\n",
       "      <td>-0.088089</td>\n",
       "      <td>-0.273710</td>\n",
       "      <td>0.510966</td>\n",
       "      <td>0.113167</td>\n",
       "      <td>-0.031475</td>\n",
       "      <td>-0.030954</td>\n",
       "      <td>0.000823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.171621</td>\n",
       "      <td>-0.011452</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>0.102671</td>\n",
       "      <td>0.235030</td>\n",
       "      <td>-0.022470</td>\n",
       "      <td>0.075539</td>\n",
       "      <td>0.140280</td>\n",
       "      <td>0.155531</td>\n",
       "      <td>0.084744</td>\n",
       "      <td>0.186730</td>\n",
       "      <td>0.013081</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.088818</td>\n",
       "      <td>0.157409</td>\n",
       "      <td>-0.050050</td>\n",
       "      <td>0.131013</td>\n",
       "      <td>0.183604</td>\n",
       "      <td>0.334854</td>\n",
       "      <td>0.098137</td>\n",
       "      <td>0.030840</td>\n",
       "      <td>-0.666411</td>\n",
       "      <td>-0.100009</td>\n",
       "      <td>0.037189</td>\n",
       "      <td>0.203064</td>\n",
       "      <td>-0.006609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.038242</td>\n",
       "      <td>-0.134997</td>\n",
       "      <td>-0.013843</td>\n",
       "      <td>-0.256558</td>\n",
       "      <td>-0.277083</td>\n",
       "      <td>-0.037515</td>\n",
       "      <td>-0.117765</td>\n",
       "      <td>-0.100724</td>\n",
       "      <td>0.222175</td>\n",
       "      <td>0.068581</td>\n",
       "      <td>-0.182349</td>\n",
       "      <td>-0.282449</td>\n",
       "      <td>0.003264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.166099</td>\n",
       "      <td>0.058907</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>0.123365</td>\n",
       "      <td>-0.148184</td>\n",
       "      <td>-0.317262</td>\n",
       "      <td>0.092964</td>\n",
       "      <td>0.188293</td>\n",
       "      <td>-0.177688</td>\n",
       "      <td>0.032355</td>\n",
       "      <td>-0.050368</td>\n",
       "      <td>0.163115</td>\n",
       "      <td>0.002959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.128059</td>\n",
       "      <td>-0.055575</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>0.121116</td>\n",
       "      <td>0.113407</td>\n",
       "      <td>-0.478998</td>\n",
       "      <td>-0.062224</td>\n",
       "      <td>0.048256</td>\n",
       "      <td>0.311034</td>\n",
       "      <td>-0.147660</td>\n",
       "      <td>0.138383</td>\n",
       "      <td>0.037368</td>\n",
       "      <td>0.001335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.015570</td>\n",
       "      <td>0.058192</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>-0.111022</td>\n",
       "      <td>0.237057</td>\n",
       "      <td>-0.402832</td>\n",
       "      <td>0.018909</td>\n",
       "      <td>-0.143559</td>\n",
       "      <td>-0.088830</td>\n",
       "      <td>0.178095</td>\n",
       "      <td>0.188093</td>\n",
       "      <td>-0.060688</td>\n",
       "      <td>-0.002686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.087618</td>\n",
       "      <td>-0.055814</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>-0.079306</td>\n",
       "      <td>-0.091890</td>\n",
       "      <td>-0.305978</td>\n",
       "      <td>0.010469</td>\n",
       "      <td>-0.050593</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.050468</td>\n",
       "      <td>-0.187636</td>\n",
       "      <td>-0.129009</td>\n",
       "      <td>-0.002969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000708</td>\n",
       "      <td>-0.041265</td>\n",
       "      <td>0.131826</td>\n",
       "      <td>-0.094152</td>\n",
       "      <td>-0.243197</td>\n",
       "      <td>-0.024350</td>\n",
       "      <td>-0.036632</td>\n",
       "      <td>-0.093428</td>\n",
       "      <td>-0.088830</td>\n",
       "      <td>0.014521</td>\n",
       "      <td>-0.165319</td>\n",
       "      <td>-0.069086</td>\n",
       "      <td>0.004669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.203051</td>\n",
       "      <td>-0.004535</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>0.149683</td>\n",
       "      <td>-0.008984</td>\n",
       "      <td>-0.060083</td>\n",
       "      <td>-0.057051</td>\n",
       "      <td>0.130630</td>\n",
       "      <td>0.044458</td>\n",
       "      <td>-0.502395</td>\n",
       "      <td>0.024978</td>\n",
       "      <td>0.113633</td>\n",
       "      <td>0.002021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.091687</td>\n",
       "      <td>0.053898</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>-0.128343</td>\n",
       "      <td>0.210706</td>\n",
       "      <td>0.158073</td>\n",
       "      <td>0.074178</td>\n",
       "      <td>-0.142617</td>\n",
       "      <td>0.066673</td>\n",
       "      <td>0.298198</td>\n",
       "      <td>0.214820</td>\n",
       "      <td>-0.028911</td>\n",
       "      <td>-0.000725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.248939</td>\n",
       "      <td>-0.055098</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>0.279474</td>\n",
       "      <td>0.201829</td>\n",
       "      <td>0.103534</td>\n",
       "      <td>0.088608</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>-0.066615</td>\n",
       "      <td>0.142705</td>\n",
       "      <td>0.077537</td>\n",
       "      <td>0.103646</td>\n",
       "      <td>-0.003220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.073994</td>\n",
       "      <td>0.157886</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>-0.046239</td>\n",
       "      <td>-0.323427</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.045863</td>\n",
       "      <td>-0.174155</td>\n",
       "      <td>-0.066615</td>\n",
       "      <td>0.073318</td>\n",
       "      <td>-0.234726</td>\n",
       "      <td>0.090027</td>\n",
       "      <td>0.001738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.047215</td>\n",
       "      <td>-0.183652</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>0.052284</td>\n",
       "      <td>-0.209894</td>\n",
       "      <td>-0.114622</td>\n",
       "      <td>-0.025469</td>\n",
       "      <td>0.108978</td>\n",
       "      <td>0.088887</td>\n",
       "      <td>-0.227914</td>\n",
       "      <td>-0.096427</td>\n",
       "      <td>-0.105176</td>\n",
       "      <td>0.001771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.343154</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>-0.206172</td>\n",
       "      <td>0.063638</td>\n",
       "      <td>0.073914</td>\n",
       "      <td>-0.277309</td>\n",
       "      <td>-0.199338</td>\n",
       "      <td>-0.177688</td>\n",
       "      <td>-0.133727</td>\n",
       "      <td>0.052446</td>\n",
       "      <td>0.032828</td>\n",
       "      <td>0.002109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.051031</td>\n",
       "      <td>-0.013837</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>-0.031843</td>\n",
       "      <td>0.236261</td>\n",
       "      <td>0.289248</td>\n",
       "      <td>0.050764</td>\n",
       "      <td>-0.085897</td>\n",
       "      <td>-0.199903</td>\n",
       "      <td>0.205682</td>\n",
       "      <td>0.240607</td>\n",
       "      <td>-0.106084</td>\n",
       "      <td>-0.001901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.254753</td>\n",
       "      <td>0.146438</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>0.088050</td>\n",
       "      <td>0.227607</td>\n",
       "      <td>-0.636032</td>\n",
       "      <td>0.138703</td>\n",
       "      <td>0.089914</td>\n",
       "      <td>0.488751</td>\n",
       "      <td>0.187012</td>\n",
       "      <td>0.045265</td>\n",
       "      <td>0.079132</td>\n",
       "      <td>-0.001782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.014079</td>\n",
       "      <td>-0.002627</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>0.125165</td>\n",
       "      <td>-0.294993</td>\n",
       "      <td>0.255867</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>0.030840</td>\n",
       "      <td>-0.533123</td>\n",
       "      <td>-0.081896</td>\n",
       "      <td>-0.246574</td>\n",
       "      <td>0.153809</td>\n",
       "      <td>0.001498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-0.085621</td>\n",
       "      <td>0.005721</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>-0.191101</td>\n",
       "      <td>-0.165743</td>\n",
       "      <td>0.347548</td>\n",
       "      <td>0.056753</td>\n",
       "      <td>-0.120259</td>\n",
       "      <td>0.111102</td>\n",
       "      <td>0.035421</td>\n",
       "      <td>-0.109987</td>\n",
       "      <td>-0.256573</td>\n",
       "      <td>0.001804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.156254</td>\n",
       "      <td>-0.059153</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>-0.263981</td>\n",
       "      <td>0.064397</td>\n",
       "      <td>0.205089</td>\n",
       "      <td>-0.150436</td>\n",
       "      <td>-0.263119</td>\n",
       "      <td>0.222175</td>\n",
       "      <td>-0.112548</td>\n",
       "      <td>0.110990</td>\n",
       "      <td>-0.200508</td>\n",
       "      <td>0.001411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.133253</td>\n",
       "      <td>-0.061776</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>0.069830</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.136915</td>\n",
       "      <td>-0.058140</td>\n",
       "      <td>0.131337</td>\n",
       "      <td>0.111102</td>\n",
       "      <td>0.044059</td>\n",
       "      <td>0.236970</td>\n",
       "      <td>0.050759</td>\n",
       "      <td>-0.001912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.314049</td>\n",
       "      <td>0.085381</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>0.241234</td>\n",
       "      <td>0.041914</td>\n",
       "      <td>0.174528</td>\n",
       "      <td>0.199962</td>\n",
       "      <td>0.193941</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.365634</td>\n",
       "      <td>-0.084564</td>\n",
       "      <td>0.243693</td>\n",
       "      <td>-0.003209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sensor_00  sensor_01  sensor_02  sensor_03  sensor_04  \\\n",
       "subject step                                                          \n",
       "4       0     -0.055973  -0.022184   0.275540   0.156882   0.146349   \n",
       "        1      0.030065   0.005959   0.384887   0.101771   0.288743   \n",
       "        2     -0.166427   0.014784   0.384887  -0.092577  -0.175286   \n",
       "        3      0.100989  -0.039357   0.384887  -0.059736  -0.203118   \n",
       "        4      0.120464   0.057476   0.384887   0.018993   0.035463   \n",
       "        5     -0.211190   0.021223   0.384887  -0.011374   0.074745   \n",
       "        6     -0.001908  -0.100891   0.384887  -0.075931   0.075476   \n",
       "        7      0.044308   0.059623   0.384887   0.088725   0.118581   \n",
       "        8      0.051575   0.070117   0.384887   0.127414  -0.196704   \n",
       "        9      0.311433  -0.023615   0.384887  -0.099100  -0.078496   \n",
       "        10    -0.303623   0.025039   0.384887  -0.051638   0.019515   \n",
       "        11    -0.052775  -0.083480   0.384887   0.003247   0.053725   \n",
       "        12     0.285564   0.068686   0.384887   0.038563   0.183993   \n",
       "        13    -0.044346   0.068924   0.347956   0.053184  -0.013223   \n",
       "        14    -0.219038  -0.127365   0.240057  -0.051638  -0.362672   \n",
       "        15     0.005068   0.019554   0.240057  -0.036567  -0.041639   \n",
       "        16    -0.065855   0.004290   0.240057  -0.119345   0.153920   \n",
       "        17    -0.023708  -0.051997   0.240057   0.039913   0.387180   \n",
       "        18     0.439909   0.108993   0.240057   0.369225  -0.101155   \n",
       "        19    -0.326876  -0.054621   0.240057  -0.282427  -0.376066   \n",
       "        20     0.234115  -0.004058   0.240057   0.060157  -0.114706   \n",
       "        21    -0.332690  -0.087773   0.204464   0.116392   0.501879   \n",
       "        22    -0.048706   0.015261   0.094780  -0.051413   0.214732   \n",
       "        23     0.333815   0.172435   0.094780  -0.061310  -0.726922   \n",
       "        24    -0.185321  -0.166957   0.059297  -0.143638  -0.301046   \n",
       "        25     0.089362   0.044120  -0.050050   0.234711   0.399425   \n",
       "        26    -0.026615   0.037442  -0.050050   0.052284   0.557859   \n",
       "        27    -0.240838  -0.057245  -0.050050  -0.194250  -0.204886   \n",
       "        28     0.248067   0.031002  -0.050050  -0.024645  -0.496597   \n",
       "        29    -0.201017  -0.068931  -0.050050   0.080402   0.010500   \n",
       "        30     0.309980   0.156932  -0.050050   0.008421   0.485071   \n",
       "        31     0.013788  -0.008351  -0.050050   0.111894   0.351590   \n",
       "        32    -0.246361  -0.084911  -0.050050  -0.148587  -0.470319   \n",
       "        33    -0.158579  -0.074417  -0.050050  -0.062435  -0.442755   \n",
       "        34     0.093431   0.102792  -0.050050   0.016294   0.037943   \n",
       "        35     0.242835   0.042689  -0.050050  -0.082680   0.431358   \n",
       "        36     0.022799  -0.051282  -0.050050   0.067805   0.180041   \n",
       "        37    -0.160032  -0.107807  -0.050050   0.129214  -0.316485   \n",
       "        38    -0.049578   0.075125  -0.050050  -0.128792  -0.123453   \n",
       "        39     0.171621  -0.011452  -0.050050   0.102671   0.235030   \n",
       "        40    -0.088818   0.157409  -0.050050   0.131013   0.183604   \n",
       "        41    -0.038242  -0.134997  -0.013843  -0.256558  -0.277083   \n",
       "        42     0.166099   0.058907   0.094780   0.123365  -0.148184   \n",
       "        43    -0.128059  -0.055575   0.094780   0.121116   0.113407   \n",
       "        44    -0.015570   0.058192   0.094780  -0.111022   0.237057   \n",
       "        45     0.087618  -0.055814   0.094780  -0.079306  -0.091890   \n",
       "        46     0.000708  -0.041265   0.131826  -0.094152  -0.243197   \n",
       "        47    -0.203051  -0.004535   0.240057   0.149683  -0.008984   \n",
       "        48     0.091687   0.053898   0.240057  -0.128343   0.210706   \n",
       "        49     0.248939  -0.055098   0.240057   0.279474   0.201829   \n",
       "        50    -0.073994   0.157886   0.240057  -0.046239  -0.323427   \n",
       "        51     0.047215  -0.183652   0.240057   0.052284  -0.209894   \n",
       "        52    -0.343154   0.005482   0.240057  -0.206172   0.063638   \n",
       "        53    -0.051031  -0.013837   0.240057  -0.031843   0.236261   \n",
       "        54     0.254753   0.146438   0.240057   0.088050   0.227607   \n",
       "        55     0.014079  -0.002627   0.240057   0.125165  -0.294993   \n",
       "        56    -0.085621   0.005721   0.240057  -0.191101  -0.165743   \n",
       "        57    -0.156254  -0.059153   0.240057  -0.263981   0.064397   \n",
       "        58     0.133253  -0.061776   0.240057   0.069830   0.306931   \n",
       "        59     0.314049   0.085381   0.240057   0.241234   0.041914   \n",
       "\n",
       "              sensor_05  sensor_06  sensor_07  sensor_08  sensor_09  \\\n",
       "subject step                                                          \n",
       "4       0      0.104004  -0.043711   0.219830   0.177746  -0.076044   \n",
       "        1      0.081906  -0.009678   0.087796   0.288819  -0.474808   \n",
       "        2     -0.169161  -0.106330  -0.144736  -0.666411   0.005047   \n",
       "        3     -0.049269   0.099498  -0.018350   0.288819   0.328015   \n",
       "        4     -0.029522   0.118284   0.021190   0.155531   0.046288   \n",
       "        5      0.017964  -0.128383  -0.070599  -0.310976  -0.329904   \n",
       "        6     -0.030463  -0.061680  -0.019762   0.466537   0.018980   \n",
       "        7      0.069682   0.045863   0.072968  -0.222118  -0.105303   \n",
       "        8     -0.110860   0.040145   0.070379   0.111102   0.324671   \n",
       "        9      0.059339   0.134347  -0.029882   0.088887   0.462886   \n",
       "        10     0.364474  -0.153976  -0.115787  -0.310976  -0.231537   \n",
       "        11     0.408669  -0.057868   0.084030   0.555395  -0.430500   \n",
       "        12     0.497530   0.216025   0.013188  -0.488694   0.604725   \n",
       "        13     0.122810   0.005841  -0.005170   0.444322  -0.264976   \n",
       "        14     0.122810  -0.146625   0.002362  -0.288762  -0.365015   \n",
       "        15    -0.097696  -0.026558  -0.058360  -0.133259   0.030683   \n",
       "        16    -0.202542  -0.041805  -0.171566   0.266605   0.117347   \n",
       "        17    -0.384495  -0.008045   0.096033   0.199961  -0.101123   \n",
       "        18    -0.440915   0.331463   0.448361  -0.244332   0.178652   \n",
       "        19    -0.188437  -0.328222  -0.313015   0.222175  -0.100009   \n",
       "        20    -0.109450   0.282729   0.088266   0.088887   0.273397   \n",
       "        21    -0.057262  -0.350002   0.122158  -0.444264  -0.469792   \n",
       "        22    -0.023410   0.018909  -0.173449   0.288819   0.148278   \n",
       "        23    -0.003663   0.280278  -0.130850  -0.044400   0.462050   \n",
       "        24     0.043353  -0.217140  -0.048240   0.466537  -0.219554   \n",
       "        25     0.140677   0.080440   0.322916  -0.488694  -0.147660   \n",
       "        26     0.107295   0.031161   0.011776  -0.066615  -0.056259   \n",
       "        27    -0.129197  -0.131650  -0.273475   0.799757  -0.072700   \n",
       "        28     0.115288   0.150138   0.040960  -0.266547   0.235778   \n",
       "        29    -0.122614  -0.151798   0.023544  -0.310976   0.049911   \n",
       "        30    -0.010246   0.151772   0.016483   0.022244   0.243580   \n",
       "        31     0.145848   0.027077   0.192764   0.044458  -0.303988   \n",
       "        32    -0.128727  -0.089722  -0.252528   0.288819  -0.264419   \n",
       "        33    -0.426810  -0.138185  -0.036472  -0.088830  -0.171903   \n",
       "        34     0.279375   0.042051   0.078146   0.199961   0.134624   \n",
       "        35     0.440641   0.150955  -0.127790   0.155531   0.507472   \n",
       "        36     0.381870   0.019454   0.158638  -0.777484  -0.030622   \n",
       "        37     0.306644  -0.067125   0.119804   0.000029  -0.374768   \n",
       "        38     0.037711  -0.088089  -0.273710   0.510966   0.113167   \n",
       "        39    -0.022470   0.075539   0.140280   0.155531   0.084744   \n",
       "        40     0.334854   0.098137   0.030840  -0.666411  -0.100009   \n",
       "        41    -0.037515  -0.117765  -0.100724   0.222175   0.068581   \n",
       "        42    -0.317262   0.092964   0.188293  -0.177688   0.032355   \n",
       "        43    -0.478998  -0.062224   0.048256   0.311034  -0.147660   \n",
       "        44    -0.402832   0.018909  -0.143559  -0.088830   0.178095   \n",
       "        45    -0.305978   0.010469  -0.050593   0.066673   0.050468   \n",
       "        46    -0.024350  -0.036632  -0.093428  -0.088830   0.014521   \n",
       "        47    -0.060083  -0.057051   0.130630   0.044458  -0.502395   \n",
       "        48     0.158073   0.074178  -0.142617   0.066673   0.298198   \n",
       "        49     0.103534   0.088608   0.418000  -0.066615   0.142705   \n",
       "        50     0.093660   0.045863  -0.174155  -0.066615   0.073318   \n",
       "        51    -0.114622  -0.025469   0.108978   0.088887  -0.227914   \n",
       "        52     0.073914  -0.277309  -0.199338  -0.177688  -0.133727   \n",
       "        53     0.289248   0.050764  -0.085897  -0.199903   0.205682   \n",
       "        54    -0.636032   0.138703   0.089914   0.488751   0.187012   \n",
       "        55     0.255867   0.008019   0.030840  -0.533123  -0.081896   \n",
       "        56     0.347548   0.056753  -0.120259   0.111102   0.035421   \n",
       "        57     0.205089  -0.150436  -0.263119   0.222175  -0.112548   \n",
       "        58     0.136915  -0.058140   0.131337   0.111102   0.044059   \n",
       "        59     0.174528   0.199962   0.193941   0.000029   0.365634   \n",
       "\n",
       "              sensor_10  sensor_11  sensor_12  \n",
       "subject step                                   \n",
       "4       0      0.054113   0.127479  -0.002021  \n",
       "        1      0.135277   0.071642  -0.000517  \n",
       "        2     -0.123275  -0.023690  -0.004865  \n",
       "        3     -0.057277  -0.092693   0.010510  \n",
       "        4      0.018720   0.019209  -0.002937  \n",
       "        5      0.023114   0.025792  -0.001520  \n",
       "        6      0.084279  -0.103134   0.000921  \n",
       "        7      0.089839   0.100695  -0.000125  \n",
       "        8     -0.191485   0.127252  -0.002708  \n",
       "        9     -0.091246  -0.124016   0.006347  \n",
       "        10     0.029326  -0.029365  -0.002860  \n",
       "        11     0.086400   0.040999   0.001466  \n",
       "        12     0.148989  -0.015746  -0.000201  \n",
       "        13    -0.023551   0.111363  -0.003961  \n",
       "        14    -0.243528  -0.129917   0.008472  \n",
       "        15    -0.037172   0.023068  -0.002991  \n",
       "        16     0.180972  -0.079982  -0.000071  \n",
       "        17     0.201441  -0.030273  -0.000735  \n",
       "        18    -0.170864   0.189445  -0.002348  \n",
       "        19    -0.242907  -0.121746   0.003155  \n",
       "        20     0.020069   0.066875   0.005421  \n",
       "        21     0.210229   0.102511  -0.004440  \n",
       "        22    -0.007627  -0.027776  -0.004876  \n",
       "        23    -0.170955  -0.101545   0.003700  \n",
       "        24     0.033690  -0.131052   0.010183  \n",
       "        25     0.244167   0.254588  -0.007382  \n",
       "        26     0.137777   0.032601  -0.001999  \n",
       "        27    -0.303572  -0.150573   0.000387  \n",
       "        28    -0.212211  -0.069767   0.002991  \n",
       "        29     0.081112   0.084807   0.003275  \n",
       "        30     0.277560   0.055299  -0.003122  \n",
       "        31     0.015827   0.074366  -0.004669  \n",
       "        32    -0.262406  -0.173952   0.003275  \n",
       "        33    -0.038959  -0.048431   0.002828  \n",
       "        34     0.102839   0.067102   0.003340  \n",
       "        35     0.225516  -0.057283  -0.004222  \n",
       "        36    -0.106700   0.017393  -0.005258  \n",
       "        37    -0.164001   0.069826   0.007121  \n",
       "        38    -0.031475  -0.030954   0.000823  \n",
       "        39     0.186730   0.013081   0.001640  \n",
       "        40     0.037189   0.203064  -0.006609  \n",
       "        41    -0.182349  -0.282449   0.003264  \n",
       "        42    -0.050368   0.163115   0.002959  \n",
       "        43     0.138383   0.037368   0.001335  \n",
       "        44     0.188093  -0.060688  -0.002686  \n",
       "        45    -0.187636  -0.129009  -0.002969  \n",
       "        46    -0.165319  -0.069086   0.004669  \n",
       "        47     0.024978   0.113633   0.002021  \n",
       "        48     0.214820  -0.028911  -0.000725  \n",
       "        49     0.077537   0.103646  -0.003220  \n",
       "        50    -0.234726   0.090027   0.001738  \n",
       "        51    -0.096427  -0.105176   0.001771  \n",
       "        52     0.052446   0.032828   0.002109  \n",
       "        53     0.240607  -0.106084  -0.001901  \n",
       "        54     0.045265   0.079132  -0.001782  \n",
       "        55    -0.246574   0.153809   0.001498  \n",
       "        56    -0.109987  -0.256573   0.001804  \n",
       "        57     0.110990  -0.200508   0.001411  \n",
       "        58     0.236970   0.050759  -0.001912  \n",
       "        59    -0.084564   0.243693  -0.003209  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_series.loc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67e653b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.5973e-02,  3.0065e-02, -1.6643e-01,  1.0099e-01,  1.2046e-01,\n",
       "          -2.1119e-01, -1.9082e-03,  4.4308e-02,  5.1575e-02,  3.1143e-01,\n",
       "          -3.0362e-01, -5.2775e-02,  2.8556e-01, -4.4346e-02, -2.1904e-01,\n",
       "           5.0678e-03, -6.5855e-02, -2.3708e-02,  4.3991e-01, -3.2688e-01,\n",
       "           2.3412e-01, -3.3269e-01, -4.8706e-02,  3.3381e-01, -1.8532e-01,\n",
       "           8.9362e-02, -2.6615e-02, -2.4084e-01,  2.4807e-01, -2.0102e-01,\n",
       "           3.0998e-01,  1.3788e-02, -2.4636e-01, -1.5858e-01,  9.3431e-02,\n",
       "           2.4284e-01,  2.2799e-02, -1.6003e-01, -4.9578e-02,  1.7162e-01,\n",
       "          -8.8818e-02, -3.8242e-02,  1.6610e-01, -1.2806e-01, -1.5570e-02,\n",
       "           8.7618e-02,  7.0781e-04, -2.0305e-01,  9.1687e-02,  2.4894e-01,\n",
       "          -7.3994e-02,  4.7215e-02, -3.4315e-01, -5.1031e-02,  2.5475e-01,\n",
       "           1.4079e-02, -8.5621e-02, -1.5625e-01,  1.3325e-01,  3.1405e-01],\n",
       "         [-2.2184e-02,  5.9591e-03,  1.4784e-02, -3.9357e-02,  5.7476e-02,\n",
       "           2.1223e-02, -1.0089e-01,  5.9623e-02,  7.0117e-02, -2.3615e-02,\n",
       "           2.5039e-02, -8.3480e-02,  6.8686e-02,  6.8924e-02, -1.2736e-01,\n",
       "           1.9554e-02,  4.2896e-03, -5.1997e-02,  1.0899e-01, -5.4621e-02,\n",
       "          -4.0581e-03, -8.7773e-02,  1.5261e-02,  1.7244e-01, -1.6696e-01,\n",
       "           4.4120e-02,  3.7442e-02, -5.7245e-02,  3.1002e-02, -6.8931e-02,\n",
       "           1.5693e-01, -8.3512e-03, -8.4911e-02, -7.4417e-02,  1.0279e-01,\n",
       "           4.2689e-02, -5.1282e-02, -1.0781e-01,  7.5125e-02, -1.1452e-02,\n",
       "           1.5741e-01, -1.3500e-01,  5.8907e-02, -5.5575e-02,  5.8192e-02,\n",
       "          -5.5814e-02, -4.1265e-02, -4.5351e-03,  5.3898e-02, -5.5098e-02,\n",
       "           1.5789e-01, -1.8365e-01,  5.4821e-03, -1.3837e-02,  1.4644e-01,\n",
       "          -2.6271e-03,  5.7206e-03, -5.9153e-02, -6.1776e-02,  8.5381e-02],\n",
       "         [ 2.7554e-01,  3.8489e-01,  3.8489e-01,  3.8489e-01,  3.8489e-01,\n",
       "           3.8489e-01,  3.8489e-01,  3.8489e-01,  3.8489e-01,  3.8489e-01,\n",
       "           3.8489e-01,  3.8489e-01,  3.8489e-01,  3.4796e-01,  2.4006e-01,\n",
       "           2.4006e-01,  2.4006e-01,  2.4006e-01,  2.4006e-01,  2.4006e-01,\n",
       "           2.4006e-01,  2.0446e-01,  9.4780e-02,  9.4780e-02,  5.9297e-02,\n",
       "          -5.0050e-02, -5.0050e-02, -5.0050e-02, -5.0050e-02, -5.0050e-02,\n",
       "          -5.0050e-02, -5.0050e-02, -5.0050e-02, -5.0050e-02, -5.0050e-02,\n",
       "          -5.0050e-02, -5.0050e-02, -5.0050e-02, -5.0050e-02, -5.0050e-02,\n",
       "          -5.0050e-02, -1.3843e-02,  9.4780e-02,  9.4780e-02,  9.4780e-02,\n",
       "           9.4780e-02,  1.3183e-01,  2.4006e-01,  2.4006e-01,  2.4006e-01,\n",
       "           2.4006e-01,  2.4006e-01,  2.4006e-01,  2.4006e-01,  2.4006e-01,\n",
       "           2.4006e-01,  2.4006e-01,  2.4006e-01,  2.4006e-01,  2.4006e-01],\n",
       "         [ 1.5688e-01,  1.0177e-01, -9.2577e-02, -5.9736e-02,  1.8993e-02,\n",
       "          -1.1374e-02, -7.5931e-02,  8.8725e-02,  1.2741e-01, -9.9100e-02,\n",
       "          -5.1638e-02,  3.2474e-03,  3.8563e-02,  5.3184e-02, -5.1638e-02,\n",
       "          -3.6567e-02, -1.1934e-01,  3.9913e-02,  3.6923e-01, -2.8243e-01,\n",
       "           6.0157e-02,  1.1639e-01, -5.1413e-02, -6.1310e-02, -1.4364e-01,\n",
       "           2.3471e-01,  5.2284e-02, -1.9425e-01, -2.4645e-02,  8.0402e-02,\n",
       "           8.4211e-03,  1.1189e-01, -1.4859e-01, -6.2435e-02,  1.6294e-02,\n",
       "          -8.2680e-02,  6.7805e-02,  1.2921e-01, -1.2879e-01,  1.0267e-01,\n",
       "           1.3101e-01, -2.5656e-01,  1.2337e-01,  1.2112e-01, -1.1102e-01,\n",
       "          -7.9306e-02, -9.4152e-02,  1.4968e-01, -1.2834e-01,  2.7947e-01,\n",
       "          -4.6239e-02,  5.2284e-02, -2.0617e-01, -3.1843e-02,  8.8050e-02,\n",
       "           1.2517e-01, -1.9110e-01, -2.6398e-01,  6.9830e-02,  2.4123e-01],\n",
       "         [ 1.4635e-01,  2.8874e-01, -1.7529e-01, -2.0312e-01,  3.5463e-02,\n",
       "           7.4745e-02,  7.5476e-02,  1.1858e-01, -1.9670e-01, -7.8496e-02,\n",
       "           1.9515e-02,  5.3725e-02,  1.8399e-01, -1.3223e-02, -3.6267e-01,\n",
       "          -4.1639e-02,  1.5392e-01,  3.8718e-01, -1.0115e-01, -3.7607e-01,\n",
       "          -1.1471e-01,  5.0188e-01,  2.1473e-01, -7.2692e-01, -3.0105e-01,\n",
       "           3.9943e-01,  5.5786e-01, -2.0489e-01, -4.9660e-01,  1.0500e-02,\n",
       "           4.8507e-01,  3.5159e-01, -4.7032e-01, -4.4275e-01,  3.7943e-02,\n",
       "           4.3136e-01,  1.8004e-01, -3.1649e-01, -1.2345e-01,  2.3503e-01,\n",
       "           1.8360e-01, -2.7708e-01, -1.4818e-01,  1.1341e-01,  2.3706e-01,\n",
       "          -9.1890e-02, -2.4320e-01, -8.9842e-03,  2.1071e-01,  2.0183e-01,\n",
       "          -3.2343e-01, -2.0989e-01,  6.3638e-02,  2.3626e-01,  2.2761e-01,\n",
       "          -2.9499e-01, -1.6574e-01,  6.4397e-02,  3.0693e-01,  4.1914e-02],\n",
       "         [ 1.0400e-01,  8.1906e-02, -1.6916e-01, -4.9269e-02, -2.9522e-02,\n",
       "           1.7964e-02, -3.0463e-02,  6.9682e-02, -1.1086e-01,  5.9339e-02,\n",
       "           3.6447e-01,  4.0867e-01,  4.9753e-01,  1.2281e-01,  1.2281e-01,\n",
       "          -9.7696e-02, -2.0254e-01, -3.8450e-01, -4.4091e-01, -1.8844e-01,\n",
       "          -1.0945e-01, -5.7262e-02, -2.3410e-02, -3.6633e-03,  4.3353e-02,\n",
       "           1.4068e-01,  1.0730e-01, -1.2920e-01,  1.1529e-01, -1.2261e-01,\n",
       "          -1.0246e-02,  1.4585e-01, -1.2873e-01, -4.2681e-01,  2.7937e-01,\n",
       "           4.4064e-01,  3.8187e-01,  3.0664e-01,  3.7711e-02, -2.2470e-02,\n",
       "           3.3485e-01, -3.7515e-02, -3.1726e-01, -4.7900e-01, -4.0283e-01,\n",
       "          -3.0598e-01, -2.4350e-02, -6.0083e-02,  1.5807e-01,  1.0353e-01,\n",
       "           9.3660e-02, -1.1462e-01,  7.3914e-02,  2.8925e-01, -6.3603e-01,\n",
       "           2.5587e-01,  3.4755e-01,  2.0509e-01,  1.3692e-01,  1.7453e-01],\n",
       "         [-4.3711e-02, -9.6781e-03, -1.0633e-01,  9.9498e-02,  1.1828e-01,\n",
       "          -1.2838e-01, -6.1680e-02,  4.5863e-02,  4.0145e-02,  1.3435e-01,\n",
       "          -1.5398e-01, -5.7868e-02,  2.1603e-01,  5.8407e-03, -1.4662e-01,\n",
       "          -2.6558e-02, -4.1805e-02, -8.0445e-03,  3.3146e-01, -3.2822e-01,\n",
       "           2.8273e-01, -3.5000e-01,  1.8909e-02,  2.8028e-01, -2.1714e-01,\n",
       "           8.0440e-02,  3.1161e-02, -1.3165e-01,  1.5014e-01, -1.5180e-01,\n",
       "           1.5177e-01,  2.7077e-02, -8.9722e-02, -1.3818e-01,  4.2051e-02,\n",
       "           1.5096e-01,  1.9454e-02, -6.7125e-02, -8.8089e-02,  7.5539e-02,\n",
       "           9.8137e-02, -1.1777e-01,  9.2964e-02, -6.2224e-02,  1.8909e-02,\n",
       "           1.0469e-02, -3.6632e-02, -5.7051e-02,  7.4178e-02,  8.8608e-02,\n",
       "           4.5863e-02, -2.5469e-02, -2.7731e-01,  5.0764e-02,  1.3870e-01,\n",
       "           8.0188e-03,  5.6753e-02, -1.5044e-01, -5.8140e-02,  1.9996e-01],\n",
       "         [ 2.1983e-01,  8.7796e-02, -1.4474e-01, -1.8350e-02,  2.1190e-02,\n",
       "          -7.0599e-02, -1.9762e-02,  7.2968e-02,  7.0379e-02, -2.9882e-02,\n",
       "          -1.1579e-01,  8.4030e-02,  1.3188e-02, -5.1697e-03,  2.3616e-03,\n",
       "          -5.8360e-02, -1.7157e-01,  9.6033e-02,  4.4836e-01, -3.1301e-01,\n",
       "           8.8266e-02,  1.2216e-01, -1.7345e-01, -1.3085e-01, -4.8240e-02,\n",
       "           3.2292e-01,  1.1776e-02, -2.7348e-01,  4.0960e-02,  2.3544e-02,\n",
       "           1.6483e-02,  1.9276e-01, -2.5253e-01, -3.6472e-02,  7.8146e-02,\n",
       "          -1.2779e-01,  1.5864e-01,  1.1980e-01, -2.7371e-01,  1.4028e-01,\n",
       "           3.0840e-02, -1.0072e-01,  1.8829e-01,  4.8256e-02, -1.4356e-01,\n",
       "          -5.0593e-02, -9.3428e-02,  1.3063e-01, -1.4262e-01,  4.1800e-01,\n",
       "          -1.7416e-01,  1.0898e-01, -1.9934e-01, -8.5897e-02,  8.9914e-02,\n",
       "           3.0840e-02, -1.2026e-01, -2.6312e-01,  1.3134e-01,  1.9394e-01],\n",
       "         [ 1.7775e-01,  2.8882e-01, -6.6641e-01,  2.8882e-01,  1.5553e-01,\n",
       "          -3.1098e-01,  4.6654e-01, -2.2212e-01,  1.1110e-01,  8.8887e-02,\n",
       "          -3.1098e-01,  5.5540e-01, -4.8869e-01,  4.4432e-01, -2.8876e-01,\n",
       "          -1.3326e-01,  2.6660e-01,  1.9996e-01, -2.4433e-01,  2.2218e-01,\n",
       "           8.8887e-02, -4.4426e-01,  2.8882e-01, -4.4400e-02,  4.6654e-01,\n",
       "          -4.8869e-01, -6.6615e-02,  7.9976e-01, -2.6655e-01, -3.1098e-01,\n",
       "           2.2243e-02,  4.4458e-02,  2.8882e-01, -8.8830e-02,  1.9996e-01,\n",
       "           1.5553e-01, -7.7748e-01,  2.8843e-05,  5.1097e-01,  1.5553e-01,\n",
       "          -6.6641e-01,  2.2218e-01, -1.7769e-01,  3.1103e-01, -8.8830e-02,\n",
       "           6.6673e-02, -8.8830e-02,  4.4458e-02,  6.6673e-02, -6.6615e-02,\n",
       "          -6.6615e-02,  8.8887e-02, -1.7769e-01, -1.9990e-01,  4.8875e-01,\n",
       "          -5.3312e-01,  1.1110e-01,  2.2218e-01,  1.1110e-01,  2.8843e-05],\n",
       "         [-7.6044e-02, -4.7481e-01,  5.0466e-03,  3.2801e-01,  4.6288e-02,\n",
       "          -3.2990e-01,  1.8980e-02, -1.0530e-01,  3.2467e-01,  4.6289e-01,\n",
       "          -2.3154e-01, -4.3050e-01,  6.0472e-01, -2.6498e-01, -3.6502e-01,\n",
       "           3.0683e-02,  1.1735e-01, -1.0112e-01,  1.7865e-01, -1.0001e-01,\n",
       "           2.7340e-01, -4.6979e-01,  1.4828e-01,  4.6205e-01, -2.1955e-01,\n",
       "          -1.4766e-01, -5.6259e-02, -7.2700e-02,  2.3578e-01,  4.9911e-02,\n",
       "           2.4358e-01, -3.0399e-01, -2.6442e-01, -1.7190e-01,  1.3462e-01,\n",
       "           5.0747e-01, -3.0622e-02, -3.7477e-01,  1.1317e-01,  8.4744e-02,\n",
       "          -1.0001e-01,  6.8581e-02,  3.2355e-02, -1.4766e-01,  1.7809e-01,\n",
       "           5.0468e-02,  1.4521e-02, -5.0239e-01,  2.9820e-01,  1.4271e-01,\n",
       "           7.3318e-02, -2.2791e-01, -1.3373e-01,  2.0568e-01,  1.8701e-01,\n",
       "          -8.1896e-02,  3.5421e-02, -1.1255e-01,  4.4059e-02,  3.6563e-01],\n",
       "         [ 5.4113e-02,  1.3528e-01, -1.2327e-01, -5.7277e-02,  1.8720e-02,\n",
       "           2.3114e-02,  8.4279e-02,  8.9839e-02, -1.9148e-01, -9.1246e-02,\n",
       "           2.9326e-02,  8.6400e-02,  1.4899e-01, -2.3551e-02, -2.4353e-01,\n",
       "          -3.7172e-02,  1.8097e-01,  2.0144e-01, -1.7086e-01, -2.4291e-01,\n",
       "           2.0069e-02,  2.1023e-01, -7.6272e-03, -1.7095e-01,  3.3690e-02,\n",
       "           2.4417e-01,  1.3778e-01, -3.0357e-01, -2.1221e-01,  8.1112e-02,\n",
       "           2.7756e-01,  1.5827e-02, -2.6241e-01, -3.8959e-02,  1.0284e-01,\n",
       "           2.2552e-01, -1.0670e-01, -1.6400e-01, -3.1475e-02,  1.8673e-01,\n",
       "           3.7189e-02, -1.8235e-01, -5.0368e-02,  1.3838e-01,  1.8809e-01,\n",
       "          -1.8764e-01, -1.6532e-01,  2.4978e-02,  2.1482e-01,  7.7537e-02,\n",
       "          -2.3473e-01, -9.6427e-02,  5.2446e-02,  2.4061e-01,  4.5265e-02,\n",
       "          -2.4657e-01, -1.0999e-01,  1.1099e-01,  2.3697e-01, -8.4564e-02],\n",
       "         [ 1.2748e-01,  7.1642e-02, -2.3690e-02, -9.2693e-02,  1.9209e-02,\n",
       "           2.5792e-02, -1.0313e-01,  1.0070e-01,  1.2725e-01, -1.2402e-01,\n",
       "          -2.9365e-02,  4.0999e-02, -1.5746e-02,  1.1136e-01, -1.2992e-01,\n",
       "           2.3068e-02, -7.9982e-02, -3.0273e-02,  1.8944e-01, -1.2175e-01,\n",
       "           6.6875e-02,  1.0251e-01, -2.7776e-02, -1.0154e-01, -1.3105e-01,\n",
       "           2.5459e-01,  3.2601e-02, -1.5057e-01, -6.9767e-02,  8.4807e-02,\n",
       "           5.5299e-02,  7.4366e-02, -1.7395e-01, -4.8431e-02,  6.7102e-02,\n",
       "          -5.7283e-02,  1.7393e-02,  6.9826e-02, -3.0954e-02,  1.3081e-02,\n",
       "           2.0306e-01, -2.8245e-01,  1.6312e-01,  3.7368e-02, -6.0688e-02,\n",
       "          -1.2901e-01, -6.9086e-02,  1.1363e-01, -2.8911e-02,  1.0365e-01,\n",
       "           9.0027e-02, -1.0518e-01,  3.2828e-02, -1.0608e-01,  7.9132e-02,\n",
       "           1.5381e-01, -2.5657e-01, -2.0051e-01,  5.0759e-02,  2.4369e-01],\n",
       "         [-2.0213e-03, -5.1750e-04, -4.8653e-03,  1.0510e-02, -2.9366e-03,\n",
       "          -1.5200e-03,  9.2088e-04, -1.2521e-04, -2.7078e-03,  6.3475e-03,\n",
       "          -2.8603e-03,  1.4657e-03, -2.0149e-04, -3.9609e-03,  8.4724e-03,\n",
       "          -2.9911e-03, -7.0728e-05, -7.3543e-04, -2.3482e-03,  3.1547e-03,\n",
       "           5.4213e-03, -4.4403e-03, -4.8762e-03,  3.6996e-03,  1.0183e-02,\n",
       "          -7.3825e-03, -1.9995e-03,  3.8694e-04,  2.9913e-03,  3.2746e-03,\n",
       "          -3.1218e-03, -4.6692e-03,  3.2746e-03,  2.8278e-03,  3.3400e-03,\n",
       "          -4.2224e-03, -5.2576e-03,  7.1212e-03,  8.2281e-04,  1.6401e-03,\n",
       "          -6.6088e-03,  3.2637e-03,  2.9586e-03,  1.3350e-03, -2.6860e-03,\n",
       "          -2.9693e-03,  4.6694e-03,  2.0215e-03, -7.2454e-04, -3.2199e-03,\n",
       "           1.7381e-03,  1.7708e-03,  2.1086e-03, -1.9014e-03, -1.7815e-03,\n",
       "           1.4984e-03,  1.8035e-03,  1.4112e-03, -1.9123e-03, -3.2090e-03]]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "340f60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    X, y = next(iter(test_loader))\n",
    "    output = model(X)\n",
    "    score = roc_auc_score(y.detach().numpy(), output.detach().numpy()[:,1])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e99a515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epoches=100):\n",
    "    torch.manual_seed(123)\n",
    "    N = len(dataset)\n",
    "    n1 = np.int32(N*0.9)\n",
    "    n2 = np.int32(N - n1)\n",
    "    print(n1,n2)\n",
    "    train_dataset, test_dataset = random_split(dataset, [n1, n2])\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, drop_last=False, batch_size=16)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False, batch_size=2597)\n",
    "    \n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            output = model(X)\n",
    "            loss = criteria(output, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += loss.item() * len(y)\n",
    "        \n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        score = evaluate(test_loader)\n",
    "        print(f\"{epoch}: {curr_loss}; roc={score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7011a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, C_mid=13):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1d_1 = nn.Conv1d(13, C_mid, 5, groups=13, padding='valid')\n",
    "        self.conv1d_2 = nn.Conv1d(13, C_mid, 60, groups=13, padding='valid')\n",
    "        self.maxpool_1 = nn.MaxPool1d(56)\n",
    "        self.fc1 = nn.Linear(2*C_mid, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ''' input shape (N,C,L) '''\n",
    "        X1 = F.relu(self.conv1d_1(X))\n",
    "        X1 = self.maxpool_1(X1)\n",
    "        X1 = torch.squeeze(X1, dim=2)\n",
    "        \n",
    "        X2 = F.relu(self.conv1d_2(X))\n",
    "        X2 = torch.squeeze(X2, dim=2)\n",
    "        \n",
    "        X = torch.concat([X1,X2], dim=1)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        output = F.softmax(self.fc2(X), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2d5255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90e6dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(mydataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4cb970e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtmp, ytmp = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64d3e3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6200, 0.3800],\n",
       "        [0.6867, 0.3133],\n",
       "        [0.6448, 0.3552],\n",
       "        [0.6581, 0.3419],\n",
       "        [0.6308, 0.3692],\n",
       "        [0.6101, 0.3899],\n",
       "        [0.5837, 0.4163],\n",
       "        [0.6191, 0.3809],\n",
       "        [0.5974, 0.4026],\n",
       "        [0.5973, 0.4027],\n",
       "        [0.5852, 0.4148],\n",
       "        [0.6288, 0.3712],\n",
       "        [0.5990, 0.4010],\n",
       "        [0.6016, 0.3984],\n",
       "        [0.6018, 0.3982],\n",
       "        [0.6026, 0.3974]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(xtmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab0e07e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 1, 5])\n",
      "torch.Size([13])\n",
      "torch.Size([13, 1, 60])\n",
      "torch.Size([13])\n",
      "torch.Size([10, 26])\n",
      "torch.Size([10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for para in model.parameters():\n",
    "    print(para.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84ef1577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23371 2597\n",
      "0: 0.6616648170354404; roc=0.6707823872424821\n",
      "1: 0.6298053853140783; roc=0.6891902952145074\n",
      "2: 0.6141629361442208; roc=0.7022106095975555\n",
      "3: 0.6046295400292407; roc=0.7119143868439282\n",
      "4: 0.5985858738766694; roc=0.7154797235744583\n",
      "5: 0.5946827273375772; roc=0.718379914310929\n",
      "6: 0.5912462856159412; roc=0.7209304808267145\n",
      "7: 0.5881369958884459; roc=0.7213577993186628\n",
      "8: 0.5848629309618347; roc=0.7231483260739602\n",
      "9: 0.5828149065290529; roc=0.7268998918211064\n",
      "10: 0.5795258213199459; roc=0.7284199713420826\n",
      "11: 0.5765498526839725; roc=0.7314580545828945\n",
      "12: 0.5749314935383546; roc=0.7328301591368463\n",
      "13: 0.5724534204037718; roc=0.7329977059431967\n",
      "14: 0.5708203465696488; roc=0.7341168593010126\n",
      "15: 0.5693776226960303; roc=0.7287805676545108\n",
      "16: 0.5678354449543719; roc=0.7345791698693312\n",
      "17: 0.5674268871763848; roc=0.7372222578073846\n",
      "18: 0.5651608175197159; roc=0.7345269782977956\n",
      "19: 0.5641707781700527; roc=0.73817297472979\n",
      "20: 0.5636276195948647; roc=0.7371816314136325\n",
      "21: 0.5623268441177446; roc=0.7373260478644159\n",
      "22: 0.5610187908385166; roc=0.7369767201867511\n",
      "23: 0.5601532722112685; roc=0.7372928350461658\n",
      "24: 0.5595468329983403; roc=0.7366101930139209\n",
      "25: 0.5589294966977663; roc=0.7358000374830377\n",
      "26: 0.5590409885770442; roc=0.7353267548229757\n",
      "27: 0.5575522865549671; roc=0.7343007159734676\n",
      "28: 0.5574358960670275; roc=0.7355497551740826\n",
      "29: 0.5569921307287118; roc=0.7318308091591463\n",
      "30: 0.5559876699662577; roc=0.732955893377364\n",
      "31: 0.5550373903487298; roc=0.7323651796813466\n",
      "32: 0.5558802071362792; roc=0.732119345517693\n",
      "33: 0.5554317697754496; roc=0.7343935339387556\n",
      "34: 0.5549188648089483; roc=0.7323521317884628\n",
      "35: 0.5539372847779134; roc=0.7322252113758647\n",
      "36: 0.5535744475790944; roc=0.731592981657035\n",
      "37: 0.5530332974809966; roc=0.732919122042873\n",
      "38: 0.5531103191456936; roc=0.7326964182347862\n",
      "39: 0.5517367536849946; roc=0.732699383664987\n",
      "40: 0.5523625623633551; roc=0.7324043233599986\n",
      "41: 0.5519532935721675; roc=0.7290569457492337\n",
      "42: 0.5504086695662166; roc=0.7320544025962935\n",
      "43: 0.5500290858455343; roc=0.7326388888888888\n",
      "44: 0.5495598467314669; roc=0.73197670832503\n",
      "45: 0.5499207300718715; roc=0.7334472651616515\n",
      "46: 0.5500776117590965; roc=0.7333606745997856\n",
      "47: 0.5493259282086822; roc=0.7294560926542735\n",
      "48: 0.5489033669058622; roc=0.730750206393942\n",
      "49: 0.5481276941602422; roc=0.7315324868809367\n",
      "50: 0.5474243947803262; roc=0.7301559341816837\n",
      "51: 0.547773138144515; roc=0.7333330960989173\n",
      "52: 0.547484378729821; roc=0.7317394739089589\n",
      "53: 0.5473297154940138; roc=0.7324615561628757\n",
      "54: 0.5463736348814032; roc=0.731939640447519\n",
      "55: 0.5465759772538455; roc=0.7335993917309571\n",
      "56: 0.5461229408888062; roc=0.7331667354646474\n",
      "57: 0.5460709443391657; roc=0.7328375727123485\n",
      "58: 0.5456955024833005; roc=0.7314909708581243\n",
      "59: 0.5451897957079024; roc=0.7308270110361449\n",
      "60: 0.5449917046097785; roc=0.7325591188164848\n",
      "61: 0.5453926843920709; roc=0.7311446086106603\n",
      "62: 0.5452555433750296; roc=0.7296980717586661\n",
      "63: 0.5448835301647127; roc=0.7309073741945891\n",
      "64: 0.5445515123086321; roc=0.7328900608269044\n",
      "65: 0.5446456607269703; roc=0.7312863561742629\n",
      "66: 0.5447261541193029; roc=0.733185714217933\n",
      "67: 0.5430344931363988; roc=0.7323746690579896\n",
      "68: 0.5432304777263968; roc=0.7315387142843586\n",
      "69: 0.5432582083302251; roc=0.7313773948814302\n",
      "70: 0.5435744461773102; roc=0.7313080038147294\n",
      "71: 0.5429332792940018; roc=0.7324998102124671\n",
      "72: 0.5420716818814039; roc=0.7320205966920033\n",
      "73: 0.5433663553078303; roc=0.7273702090509674\n",
      "74: 0.5431417466959513; roc=0.7320407616173694\n",
      "75: 0.5433493429636864; roc=0.7300316826562664\n",
      "76: 0.5415969719823939; roc=0.7304029545174178\n",
      "77: 0.5420095407658285; roc=0.732486169233543\n",
      "78: 0.5420734781564203; roc=0.73427521327374\n",
      "79: 0.5413723320864289; roc=0.7310280672037655\n",
      "80: 0.5417000876378505; roc=0.7321742059764094\n",
      "81: 0.5415386229270199; roc=0.7323699243696683\n",
      "82: 0.5406070229387024; roc=0.7335810060637117\n",
      "83: 0.5407881672894163; roc=0.7321682751160077\n",
      "84: 0.5406669937142725; roc=0.7336874650079237\n",
      "85: 0.5408826483509455; roc=0.732622282479764\n",
      "86: 0.5397722557901821; roc=0.7327201416763933\n",
      "87: 0.540763698751316; roc=0.7335869369241134\n",
      "88: 0.5400013548893982; roc=0.7351823383721923\n",
      "89: 0.5401420341540422; roc=0.7329517417750828\n",
      "90: 0.540474415758874; roc=0.7349299802620965\n",
      "91: 0.5391900248413984; roc=0.7324938793520653\n",
      "92: 0.5393749396927799; roc=0.7333609711428057\n",
      "93: 0.5392886991270506; roc=0.7320140727455614\n",
      "94: 0.5390027282347264; roc=0.7304415051100293\n",
      "95: 0.5389785034786287; roc=0.7317970032548562\n",
      "96: 0.5384060473574901; roc=0.7315912023989143\n",
      "97: 0.5386497342440126; roc=0.7342615722948159\n",
      "98: 0.5389923398600102; roc=0.7351254021123352\n",
      "99: 0.538041214671879; roc=0.7344056922025792\n"
     ]
    }
   ],
   "source": [
    "train(mydataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d0fb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c21d8093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23371 2597\n",
      "0: 0.6242367610304108; roc=0.7567146236038755\n",
      "1: 0.5553748617989991; roc=0.7835980276330649\n",
      "2: 0.5299405658088494; roc=0.7820726103377269\n",
      "3: 0.5158164243092969; roc=0.7920355661836573\n",
      "4: 0.5052800488525032; roc=0.7952133211869311\n",
      "5: 0.4972211042125226; roc=0.7966663819853673\n",
      "6: 0.49168347535153356; roc=0.8023318363841678\n",
      "7: 0.48299952962371906; roc=0.8004203200766742\n",
      "8: 0.48072193018799747; roc=0.7999763951756009\n",
      "9: 0.475662295833172; roc=0.8034634445488276\n",
      "10: 0.47263668879664483; roc=0.8057880452833054\n",
      "11: 0.47040971430748346; roc=0.8048269493551968\n",
      "12: 0.4663178967608857; roc=0.807362392176958\n",
      "13: 0.46132279199714515; roc=0.804267372676289\n",
      "14: 0.4626153702174004; roc=0.8047723854395005\n",
      "15: 0.4573960980617782; roc=0.804760523718697\n",
      "16: 0.45635547326437537; roc=0.8075569243981363\n",
      "17: 0.4523508092322759; roc=0.8086912014499767\n",
      "18: 0.45211357779198047; roc=0.8073724746396409\n",
      "19: 0.44765373151158805; roc=0.8148984399464799\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmydataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epoches)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criteria(output, y)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m curr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(mydataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe3c366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, C_mid=13):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1d_1 = nn.Conv1d(13, C_mid, 5, groups=13, padding='valid')\n",
    "        self.conv1d_2 = nn.Conv1d(13, C_mid, 60, groups=13, padding='valid')\n",
    "        self.maxpool_1 = nn.MaxPool1d(56)\n",
    "        self.fc1 = nn.Linear(2*C_mid, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ''' input shape (N,C,L) '''\n",
    "        X1 = F.relu(self.conv1d_1(X))\n",
    "        X1 = self.maxpool_1(X1)\n",
    "        X1 = torch.squeeze(X1, dim=2)\n",
    "        X1 = self.dropout(X1)\n",
    "        \n",
    "        X2 = F.relu(self.conv1d_2(X))\n",
    "        X2 = torch.squeeze(X2, dim=2)\n",
    "        X2 = self.dropout(X2)\n",
    "        \n",
    "        X = torch.concat([X1,X2], dim=1)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        output = F.softmax(self.fc2(X), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6cd0b52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23371 2597\n",
      "0: 0.631463116717009; roc=0.7525256569020979\n",
      "1: 0.5782157972057805; roc=0.7781336294018845\n",
      "2: 0.553774187877627; roc=0.787520105616762\n",
      "3: 0.5402953006541702; roc=0.7982241224698949\n",
      "4: 0.5292369814903713; roc=0.8046588094628063\n",
      "5: 0.5247702295435066; roc=0.8072918149381768\n",
      "6: 0.5147991832191905; roc=0.8078214407720558\n",
      "7: 0.5127435361516272; roc=0.8088344317286796\n",
      "8: 0.5086478089116656; roc=0.8099601090329375\n",
      "9: 0.5047048686325594; roc=0.8134596132130081\n",
      "10: 0.503396777527552; roc=0.8105896698645866\n",
      "11: 0.49841461525404696; roc=0.8129670552566401\n",
      "12: 0.4943892804313688; roc=0.8102957957316783\n",
      "13: 0.4958615191652239; roc=0.8094571720708665\n",
      "14: 0.49481227174021913; roc=0.8139652190622597\n",
      "15: 0.49195614996405695; roc=0.8169184909993263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m130\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmydataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epoches)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criteria(output, y)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m curr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MyModel(130)\n",
    "train(mydataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3dd31f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, C_mid=13, fc_mid=100):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1d_1 = nn.Conv1d(13, C_mid, 5, groups=13, padding='valid')\n",
    "        self.conv1d_2 = nn.Conv1d(13, C_mid, 60, groups=13, padding='valid')\n",
    "        self.maxpool_1 = nn.MaxPool1d(56)\n",
    "        self.fc1 = nn.Linear(2*C_mid, fc_mid)\n",
    "        self.fc2 = nn.Linear(fc_mid, 2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ''' input shape (N,C,L) '''\n",
    "        X1 = F.relu(self.conv1d_1(X))\n",
    "        X1 = self.maxpool_1(X1)\n",
    "        X1 = torch.squeeze(X1, dim=2)\n",
    "        X1 = self.dropout(X1)\n",
    "        \n",
    "        X2 = F.relu(self.conv1d_2(X))\n",
    "        X2 = torch.squeeze(X2, dim=2)\n",
    "        X2 = self.dropout(X2)\n",
    "        \n",
    "        X = torch.concat([X1,X2], dim=1)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.dropout(X)\n",
    "        output = F.softmax(self.fc2(X), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "700e8fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23371 2597\n",
      "0: 0.6128469232297169; roc=0.7786935026238127\n",
      "1: 0.5513315293830328; roc=0.8008102741480911\n",
      "2: 0.5263290014238302; roc=0.8196523211015269\n",
      "3: 0.5133557498141352; roc=0.8186073034987331\n",
      "4: 0.5048488486749636; roc=0.8193015107087617\n",
      "5: 0.49790862422103493; roc=0.8206564157675483\n",
      "6: 0.4922636246200214; roc=0.8267738017289644\n",
      "7: 0.48545004852898116; roc=0.8311661969425229\n",
      "8: 0.4870107556768133; roc=0.8258340568983022\n",
      "9: 0.4765271395527757; roc=0.826722499786489\n",
      "10: 0.47416655749693376; roc=0.8315279794270315\n",
      "11: 0.471131141390249; roc=0.8251270983384101\n",
      "12: 0.4661748827965114; roc=0.8358681830690542\n",
      "13: 0.46382250175040063; roc=0.8318660384699329\n",
      "14: 0.45999197719961304; roc=0.830034292234843\n",
      "15: 0.4609842584041437; roc=0.8372316878754235\n",
      "16: 0.45593217865431507; roc=0.8338422011558061\n",
      "17: 0.4540470343902927; roc=0.8375741950636264\n",
      "18: 0.45314162076705483; roc=0.8375703400043653\n",
      "19: 0.44877962729631055; roc=0.8399115471479678\n",
      "20: 0.4472937310472909; roc=0.834951865136979\n",
      "21: 0.44423144009992577; roc=0.8475327027642554\n",
      "22: 0.44279226224743085; roc=0.8484048357863372\n",
      "23: 0.44143649055080364; roc=0.8490483341399304\n",
      "24: 0.4420485682646835; roc=0.840414780653059\n",
      "25: 0.441489759198376; roc=0.8429392513830767\n",
      "26: 0.44161579120751343; roc=0.8401090447993471\n",
      "27: 0.4385869729634167; roc=0.8415469819037589\n",
      "28: 0.43890816432220536; roc=0.8435252203907726\n",
      "29: 0.43830183034510173; roc=0.8311326875812528\n",
      "30: 0.43678458275683174; roc=0.8339806867461876\n",
      "31: 0.4423952496922484; roc=0.8383291935927729\n",
      "32: 0.43673636717201786; roc=0.8321450854518366\n",
      "33: 0.43961583145045996; roc=0.8329226212505101\n",
      "34: 0.43131585152218266; roc=0.8355043247834051\n",
      "35: 0.4366258196543884; roc=0.8357934542279917\n",
      "36: 0.4353697501665861; roc=0.832252434025109\n",
      "37: 0.43291287117462235; roc=0.8382639541283532\n",
      "38: 0.42996250941936914; roc=0.8398237704140215\n",
      "39: 0.430455206624759; roc=0.8418281046868032\n",
      "40: 0.42861588322801586; roc=0.8417643479374839\n",
      "41: 0.42324149448959036; roc=0.8332749143583758\n",
      "42: 0.428700131585451; roc=0.8458652413622949\n",
      "43: 0.4298987951885062; roc=0.8338528767045292\n",
      "44: 0.42847871943874083; roc=0.8383579582657215\n",
      "45: 0.4255987247335381; roc=0.8406339259449047\n",
      "46: 0.422166954106082; roc=0.8389157556865089\n",
      "47: 0.4241033274975103; roc=0.8425134156062288\n",
      "48: 0.42115044353566394; roc=0.8422761811901576\n",
      "49: 0.4263768408870387; roc=0.8380445122934874\n",
      "50: 0.42262977899551635; roc=0.837256894032131\n",
      "51: 0.4226509076204631; roc=0.8435278892779533\n",
      "52: 0.42244164424108643; roc=0.8297146188591873\n",
      "53: 0.4221364803343487; roc=0.824756716106319\n",
      "54: 0.4271659494838851; roc=0.8339545909604198\n",
      "55: 0.4219481958848083; roc=0.836090590334121\n",
      "56: 0.4247790262377883; roc=0.8422370375115059\n",
      "57: 0.42365005893168844; roc=0.8406537943272506\n",
      "58: 0.4225402580957746; roc=0.8377070463366262\n",
      "59: 0.4218811438129657; roc=0.8388184895759199\n",
      "60: 0.4183240389467882; roc=0.8338825310065382\n",
      "61: 0.41837687871941837; roc=0.8432989580664446\n",
      "62: 0.41629357148663365; roc=0.8403643683396438\n",
      "63: 0.41825859160586704; roc=0.8463340758770558\n",
      "64: 0.41822977462571215; roc=0.8318841275941584\n",
      "65: 0.4186342645250508; roc=0.8364663103405737\n",
      "66: 0.4200913488678722; roc=0.8366741869976563\n",
      "67: 0.4183151854496958; roc=0.834704844801245\n",
      "68: 0.4180276241952222; roc=0.8325649903682828\n",
      "69: 0.41742107580870147; roc=0.8354476850665679\n",
      "70: 0.4140750527812909; roc=0.8306608876362913\n",
      "71: 0.41893803507407135; roc=0.8364497039314487\n",
      "72: 0.41679368985157805; roc=0.8355710469629251\n",
      "73: 0.41934155417719965; roc=0.8351158534270885\n",
      "74: 0.420603352844279; roc=0.8317014570937835\n",
      "75: 0.4185391416213952; roc=0.8264221017071389\n",
      "76: 0.41603224890528234; roc=0.83668693834752\n",
      "77: 0.41737268470757877; roc=0.8304986786043024\n",
      "78: 0.4164903778076723; roc=0.836184297928469\n",
      "79: 0.42079425851333985; roc=0.8362510201079891\n",
      "80: 0.42168791267778105; roc=0.837216860724419\n",
      "81: 0.4135648678954904; roc=0.8387781597251875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1300\u001b[39m,\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmydataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epoches)\u001b[0m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     curr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m     28\u001b[0m curr_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/optim/adam.py:133\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 133\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/optim/_functional.py:86\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     83\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m     87\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MyModel(1300,50)\n",
    "train(mydataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "164b90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, C_mid=13, fc_mid=100):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1d_1 = nn.Conv1d(13, C_mid, 5, groups=13, padding='valid')\n",
    "        self.conv1d_2 = nn.Conv1d(13, C_mid, 60, groups=13, padding='valid')\n",
    "        self.maxpool_1 = nn.MaxPool1d(56)\n",
    "        self.fc1 = nn.Linear(2*C_mid, fc_mid)\n",
    "        self.fc2 = nn.Linear(fc_mid, 50)\n",
    "        self.fc3 = nn.Linear(50,2)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ''' input shape (N,C,L) '''\n",
    "        X1 = F.relu(self.conv1d_1(X))\n",
    "        X1 = self.maxpool_1(X1)\n",
    "        X1 = torch.squeeze(X1, dim=2)\n",
    "        X1 = self.dropout(X1)\n",
    "        \n",
    "        X2 = F.relu(self.conv1d_2(X))\n",
    "        X2 = torch.squeeze(X2, dim=2)\n",
    "        X2 = self.dropout(X2)\n",
    "        \n",
    "        X = torch.concat([X1,X2], dim=1)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.dropout(X)\n",
    "        output = F.softmax(self.fc3(X), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6613331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23371 2597\n",
      "0: 0.6114721737901174; roc=0.7723282066976019\n",
      "1: 0.5516143989962886; roc=0.7840256426680331\n",
      "2: 0.5295649510673797; roc=0.8039963323559275\n",
      "3: 0.5208065045909752; roc=0.8050277089797971\n",
      "4: 0.5081275405272109; roc=0.8091232646302464\n",
      "5: 0.4995332149466992; roc=0.8182114185669143\n",
      "6: 0.4897642134765744; roc=0.8263452970649358\n",
      "7: 0.48605749444474733; roc=0.8329273659388314\n",
      "8: 0.48195167483212703; roc=0.8324712827739346\n",
      "9: 0.4749200105422279; roc=0.836670628481415\n",
      "10: 0.4703779295472604; roc=0.8391630725652632\n",
      "11: 0.4668513895124216; roc=0.8463910121369128\n",
      "12: 0.463060362632006; roc=0.8380142649054384\n",
      "13: 0.46106864177422213; roc=0.8491951229348744\n",
      "14: 0.45758084224127443; roc=0.8468005380476556\n",
      "15: 0.45925494868077654; roc=0.8487120543551494\n",
      "16: 0.45498906655844307; roc=0.8291473320617568\n",
      "17: 0.4525845047228501; roc=0.8368607125572921\n",
      "18: 0.45241003136651964; roc=0.836803479754415\n",
      "19: 0.4497100186385707; roc=0.8331130018694073\n",
      "20: 0.45398390445123254; roc=0.8357531243772596\n",
      "21: 0.45212374642956144; roc=0.8378585798198916\n",
      "22: 0.4476779788193126; roc=0.8400138544898985\n",
      "23: 0.4416739139980858; roc=0.8476640713221548\n",
      "24: 0.44392440627502755; roc=0.8450613132348337\n",
      "25: 0.4468312580601496; roc=0.8384442522845673\n",
      "26: 0.4389254869071098; roc=0.8318838310511384\n",
      "27: 0.443719655900185; roc=0.8359206711836099\n",
      "28: 0.43932972268780546; roc=0.8312459670149267\n",
      "29: 0.4397521830343179; roc=0.8238851761702775\n",
      "30: 0.4417675217002047; roc=0.8354859391161595\n",
      "31: 0.43786618763351465; roc=0.8423162144978696\n",
      "32: 0.4330195558338415; roc=0.8415247411772522\n",
      "33: 0.43180633963983; roc=0.8389786228067678\n",
      "34: 0.4349157004588264; roc=0.840364961425684\n",
      "35: 0.43919054430310794; roc=0.8348996735654435\n",
      "36: 0.43962034616374035; roc=0.8348258343534413\n",
      "37: 0.4363397197580874; roc=0.8353284747724923\n",
      "38: 0.43350291773941474; roc=0.841074885415777\n",
      "39: 0.4330341448287471; roc=0.8422824085935796\n",
      "40: 0.43518503345837617; roc=0.8417299489471536\n",
      "41: 0.4343714264981539; roc=0.8357901922547708\n",
      "42: 0.4380396592882783; roc=0.8364870683519798\n",
      "43: 0.4353180653573225; roc=0.8288682850798531\n",
      "44: 0.43244693870033546; roc=0.8420261954242225\n",
      "45: 0.4367770538378637; roc=0.8336444069614067\n",
      "46: 0.43837315469099086; roc=0.8301582472172404\n",
      "47: 0.43874928551212744; roc=0.8302566994999099\n",
      "48: 0.44241047190521177; roc=0.8256410073922243\n",
      "49: 0.44237265808582; roc=0.8208992845010011\n",
      "50: 0.44517898659799765; roc=0.8134717714768317\n",
      "51: 0.44180241021972877; roc=0.8245779006652052\n",
      "52: 0.4368944079133993; roc=0.8088412522181418\n",
      "53: 0.439745970647064; roc=0.823947153661476\n",
      "54: 0.44072465171460384; roc=0.8219001171938016\n",
      "55: 0.44437792712322766; roc=0.8202878127935777\n",
      "56: 0.4545940910495703; roc=0.8116859894098556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1300\u001b[39m,\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmydataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epoches)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criteria(output, y)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m curr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MyModel(1300,200)\n",
    "train(mydataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e10de8",
   "metadata": {},
   "source": [
    "#### so looks like simple model gets to 0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22d6dd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 13, 60])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "72fabdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 26, 56])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtmp1 = nn.Conv1d(13,26,5,padding='valid')(xtmp)\n",
    "xtmp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57f6b84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 26, 11])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MaxPool1d(5)(xtmp1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5307196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1d_1 = nn.Conv1d(13, 26, 5, padding='valid')\n",
    "        self.maxpool_1 = nn.MaxPool1d(5)\n",
    "        self.conv1d_2 = nn.Conv1d(26, 52, 5, padding='valid')\n",
    "        self.maxpool_2 = nn.MaxPool1d(7)\n",
    "        self.fc1 = nn.Linear(52, 26)\n",
    "        self.fc2 = nn.Linear(26, 13)\n",
    "        self.fc3 = nn.Linear(13,2)\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ''' input shape (N,C,L) '''\n",
    "        X = F.relu(self.conv1d_1(X))\n",
    "        X = self.maxpool_1(X)\n",
    "        X = F.relu(self.conv1d_2(X))\n",
    "        X = self.maxpool_2(X)\n",
    "        \n",
    "        X = torch.squeeze(X, dim=2)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        output = F.softmax(self.fc3(X), dim=1)\n",
    "#         X1 = torch.squeeze(X1, dim=2)\n",
    "#         X1 = self.dropout(X1)\n",
    "        \n",
    "#         X2 = F.relu(self.conv1d_2(X))\n",
    "#         X2 = torch.squeeze(X2, dim=2)\n",
    "#         X2 = self.dropout(X2)\n",
    "        \n",
    "#         X = torch.concat([X1,X2], dim=1)\n",
    "#         X = F.relu(self.fc1(X))\n",
    "#         X = self.dropout(X)\n",
    "#         X = F.relu(self.fc2(X))\n",
    "#         X = self.dropout(X)\n",
    "#         output = F.softmax(self.fc3(X), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a46754e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a107ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epoches=100, batch_size=16):\n",
    "    torch.manual_seed(123)\n",
    "    N = len(dataset)\n",
    "    n1 = int(N*0.9)\n",
    "    n2 = int(N - n1)\n",
    "    print(n1,n2)\n",
    "    train_dataset, test_dataset = random_split(dataset, [n1, n2])\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, drop_last=False, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False, batch_size=n2)\n",
    "    \n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            output = model(X)\n",
    "            loss = criteria(output, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += loss.item() * len(y)\n",
    "        \n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        score = evaluate(test_loader)\n",
    "        print(f\"{epoch}: {curr_loss}; roc={score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1fd4f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23371 2597\n",
      "0: 0.41561480979548915; roc=0.8761310150786195\n",
      "1: 0.41006502277798773; roc=0.8769972172402993\n",
      "2: 0.4095316994244284; roc=0.8736361986506106\n",
      "3: 0.4077707007033218; roc=0.8751945322211784\n",
      "4: 0.40844460438765917; roc=0.8763946418234785\n",
      "5: 0.4061176417832462; roc=0.8736812731896642\n",
      "6: 0.40543619783335466; roc=0.8747805581651342\n",
      "7: 0.4125679976382849; roc=0.8772525407805961\n",
      "8: 0.40541054623448636; roc=0.8749516634877255\n",
      "9: 0.4040663159198099; roc=0.8736065443486017\n",
      "10: 0.402344284777688; roc=0.8726235042370067\n",
      "11: 0.4054757034315448; roc=0.8717116344502329\n",
      "12: 0.40610555825814537; roc=0.8713581551702869\n",
      "13: 0.40162670581985827; roc=0.8715523908484452\n",
      "14: 0.3979532440923399; roc=0.8730060447329215\n",
      "15: 0.3998954442732703; roc=0.8734185360738651\n",
      "16: 0.39698887706073255; roc=0.8754415525569125\n",
      "17: 0.4007631814603266; roc=0.8670505712604739\n",
      "18: 0.3981809124237694; roc=0.87360120657424\n",
      "19: 0.39685923791855954; roc=0.8723408987388618\n",
      "20: 0.3969563454699442; roc=0.8741468457312038\n",
      "21: 0.3976990692978987; roc=0.871661518679838\n",
      "22: 0.39813532208805696; roc=0.8736089166927625\n",
      "23: 0.3958137438506504; roc=0.8695439049733823\n",
      "24: 0.3947708370479406; roc=0.8680285701407274\n",
      "25: 0.39358234927705377; roc=0.8675333432971788\n",
      "26: 0.39577442897023607; roc=0.8688173745741641\n",
      "27: 0.39568589469683724; roc=0.8670977216006681\n",
      "28: 0.40114656169925306; roc=0.8654361910591092\n",
      "29: 0.3961199291452436; roc=0.8656292405651871\n",
      "30: 0.3933192209804998; roc=0.862583743748873\n",
      "31: 0.39605746521176904; roc=0.8670398957117507\n",
      "32: 0.39159392624625466; roc=0.8689520051052846\n",
      "33: 0.3956327517760192; roc=0.8637897842115752\n",
      "34: 0.3954973024854937; roc=0.8578390554274489\n",
      "35: 0.39440208287351497; roc=0.8687450180772626\n",
      "36: 0.3933996236537011; roc=0.862277118266101\n",
      "37: 0.3934138007616641; roc=0.8668610802706369\n",
      "38: 0.39382733305745876; roc=0.866467864225999\n",
      "39: 0.394443209176855; roc=0.8686157253205037\n",
      "40: 0.3922374195189201; roc=0.8677913357246563\n",
      "41: 0.3921728514325843; roc=0.8647416873060609\n",
      "42: 0.39711248119372894; roc=0.8665348829485391\n",
      "43: 0.39311876757363956; roc=0.8663708946584299\n",
      "44: 0.3945642253433009; roc=0.8658474962279727\n",
      "45: 0.3935485727994822; roc=0.8618251867034854\n",
      "46: 0.3920740073753267; roc=0.8672068494320609\n",
      "47: 0.38927462514295263; roc=0.8690999800723092\n",
      "48: 0.3960680148258354; roc=0.8656956662016873\n",
      "49: 0.39000935083783017; roc=0.8662149130298631\n",
      "50: 0.39091509902213556; roc=0.8649314748389179\n",
      "51: 0.39201858319390914; roc=0.8694332944268891\n",
      "52: 0.39008431457508147; roc=0.8689893695258158\n",
      "53: 0.3930152785545452; roc=0.8659364591339995\n",
      "54: 0.3905768662411851; roc=0.870346053842723\n",
      "55: 0.3926279925357943; roc=0.8660737585523007\n",
      "56: 0.3940602582706617; roc=0.866751062810184\n",
      "57: 0.3951334945376462; roc=0.8683959869426178\n",
      "58: 0.390928342191803; roc=0.8659856852753343\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmydataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, epoches, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m curr_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     20\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criteria(output, y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/dataset.py:363\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx]\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(y))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(mydataset, epoches=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "58d378b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1d_1 = nn.Conv1d(13, 26, 5, padding='valid')\n",
    "#         self.maxpool_1 = nn.MaxPool1d(5)\n",
    "        self.maxpool_1 = nn.Conv1d(26, 26, 5, stride=5, groups=26, padding='valid')\n",
    "        self.conv1d_2 = nn.Conv1d(26, 52, 5, padding='valid')\n",
    "        self.maxpool_2 = nn.MaxPool1d(7)\n",
    "        self.fc1 = nn.Linear(52, 26)\n",
    "        self.fc2 = nn.Linear(26, 13)\n",
    "        self.fc3 = nn.Linear(13,2)\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ''' input shape (N,C,L) '''\n",
    "        X = F.relu(self.conv1d_1(X))\n",
    "        X = self.maxpool_1(X)\n",
    "        X = F.relu(self.conv1d_2(X))\n",
    "        X = self.maxpool_2(X)\n",
    "        \n",
    "        X = torch.squeeze(X, dim=2)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        output = F.softmax(self.fc3(X), dim=1)\n",
    "#         X1 = torch.squeeze(X1, dim=2)\n",
    "#         X1 = self.dropout(X1)\n",
    "        \n",
    "#         X2 = F.relu(self.conv1d_2(X))\n",
    "#         X2 = torch.squeeze(X2, dim=2)\n",
    "#         X2 = self.dropout(X2)\n",
    "        \n",
    "#         X = torch.concat([X1,X2], dim=1)\n",
    "#         X = F.relu(self.fc1(X))\n",
    "#         X = self.dropout(X)\n",
    "#         X = F.relu(self.fc2(X))\n",
    "#         X = self.dropout(X)\n",
    "#         output = F.softmax(self.fc3(X), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "37ff12cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23371 2597\n",
      "0: 0.6476832419540777; roc=0.7711933365597214\n",
      "1: 0.5469525004860414; roc=0.8241366446513129\n",
      "2: 0.5217967823145861; roc=0.8464983607101849\n",
      "3: 0.506125458481631; roc=0.8567904793084142\n",
      "4: 0.4953230004409358; roc=0.8604966739734866\n",
      "5: 0.4869363435994094; roc=0.861580835254932\n",
      "6: 0.48012912609545455; roc=0.8617341479963181\n",
      "7: 0.4746139558889477; roc=0.8718854086600052\n",
      "8: 0.4690838941820401; roc=0.8662241058634857\n",
      "9: 0.46398948531933065; roc=0.86730441208567\n",
      "10: 0.4594577868574621; roc=0.8657632780102674\n",
      "11: 0.4559804725519964; roc=0.8675579563678462\n",
      "12: 0.45263074670812803; roc=0.8636723531756199\n",
      "13: 0.44651358767404753; roc=0.8620808067868021\n",
      "14: 0.4441414335132497; roc=0.869852902800315\n",
      "15: 0.44063238946919486; roc=0.863266385781118\n",
      "16: 0.43872599113656224; roc=0.8647203362086144\n",
      "17: 0.43654599569278524; roc=0.8650575056224556\n",
      "18: 0.43298718987158613; roc=0.8679135114489329\n",
      "19: 0.4304769048870954; roc=0.8643570710090052\n",
      "20: 0.428279954357481; roc=0.8700726411782009\n",
      "21: 0.4268615014424226; roc=0.8674764070373218\n",
      "22: 0.42866728829509165; roc=0.8646040913447395\n",
      "23: 0.4251581754650681; roc=0.8653353664322792\n",
      "24: 0.42364427908298; roc=0.8605909746538749\n",
      "25: 0.4220089377245381; roc=0.858565585826667\n",
      "26: 0.42792670805888533; roc=0.8585190285725132\n",
      "27: 0.42692799045937463; roc=0.8625810748616924\n",
      "28: 0.4295791529103394; roc=0.8601360776610585\n",
      "29: 0.42106743490561804; roc=0.8629028240384888\n",
      "30: 0.4184726612068336; roc=0.8573349322932976\n",
      "31: 0.41692642196117086; roc=0.8546022883631775\n",
      "32: 0.41572246225168613; roc=0.8574986240403869\n",
      "33: 0.4194003622962189; roc=0.8641785521109118\n",
      "34: 0.41781272119400736; roc=0.858728684487716\n",
      "35: 0.41217932605319024; roc=0.8567827691898919\n",
      "36: 0.41517463203016364; roc=0.8594406842789497\n",
      "37: 0.4123472109871189; roc=0.8568945659084656\n",
      "38: 0.41333974110754235; roc=0.8553326738216566\n",
      "39: 0.4120211475715472; roc=0.8600649073362371\n",
      "40: 0.41046452197233463; roc=0.8603205274195538\n",
      "41: 0.40977456218001423; roc=0.8597330756967575\n",
      "42: 0.4072295947100088; roc=0.855217318586842\n",
      "43: 0.40967606465270007; roc=0.8517439101925395\n",
      "44: 0.4109155477575056; roc=0.8597372272990389\n",
      "45: 0.40838332570826896; roc=0.8533763795181295\n",
      "46: 0.4074500162250234; roc=0.857059740370655\n",
      "47: 0.4083247396880256; roc=0.8582029137130982\n",
      "48: 0.40732118589245286; roc=0.8596808841252219\n",
      "49: 0.4062565995203271; roc=0.8480391982425675\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "train(mydataset, epoches=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "38c321cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1d_1 = nn.Conv1d(13, 26, 3, padding='valid')\n",
    "        self.maxpool_1 = nn.MaxPool1d(3) \n",
    "        self.conv1d_2 = nn.Conv1d(26, 52, 3, padding='valid')\n",
    "        self.maxpool_2 = nn.MaxPool1d(3)\n",
    "        self.conv1d_3 = nn.Conv1d(52, 104, 3, padding='valid')\n",
    "        self.maxpool_3 = nn.MaxPool1d(3)\n",
    "        \n",
    "#         self.conv1d_2 = nn.Conv1d(26, 52, 5, padding='valid')\n",
    "#         self.maxpool_2 = nn.MaxPool1d(7)\n",
    "        self.fc1 = nn.Linear(104, 52)\n",
    "        self.fc2 = nn.Linear(52, 26)\n",
    "        self.fc3 = nn.Linear(26, 2)\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        ''' input shape (N,C,L) '''\n",
    "        X = F.relu(self.conv1d_1(X))\n",
    "        X = self.maxpool_1(X)\n",
    "\n",
    "        X = F.relu(self.conv1d_2(X))\n",
    "        X = self.maxpool_2(X)\n",
    "\n",
    "        X = F.relu(self.conv1d_3(X))\n",
    "        X = self.maxpool_3(X)\n",
    "\n",
    "        \n",
    "        X = torch.squeeze(X, dim=2)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        output = F.softmax(self.fc3(X), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc90421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "419230bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23371 2597\n",
      "0: 0.6051140818764823; roc=0.7800329874455547\n",
      "1: 0.5361039293633055; roc=0.8129454076161737\n",
      "2: 0.5184349775962125; roc=0.8230483317675862\n",
      "3: 0.5131818654269821; roc=0.8344124533834372\n",
      "4: 0.5073100449205348; roc=0.8260075345650545\n",
      "5: 0.4973324881513109; roc=0.8350669238287737\n",
      "6: 0.49806519214161593; roc=0.8381506746946793\n",
      "7: 0.4944085446750035; roc=0.8390207319156204\n",
      "8: 0.4903928661465283; roc=0.8388617848568529\n",
      "9: 0.49002992789576294; roc=0.8404058843624562\n",
      "10: 0.4843610145853285; roc=0.8338872756948597\n",
      "11: 0.48295167103428255; roc=0.8411594001765024\n",
      "12: 0.47859895980295386; roc=0.8411991369411943\n",
      "13: 0.4733342497879058; roc=0.8374428265057268\n",
      "14: 0.47356076354362464; roc=0.8440735284349172\n",
      "15: 0.4831727127324254; roc=0.8262065149315342\n",
      "16: 0.47807286495799445; roc=0.828414870802137\n",
      "17: 0.4735298790912727; roc=0.8405488180981391\n",
      "18: 0.4665740102849597; roc=0.8515478952562606\n",
      "19: 0.46346645950148896; roc=0.8505221529497727\n",
      "20: 0.45923877045608474; roc=0.8508667359391162\n",
      "21: 0.45798112903509036; roc=0.8466564181398923\n",
      "22: 0.45770503872906126; roc=0.8606920958237254\n",
      "23: 0.45441755842812626; roc=0.8489498818572607\n",
      "24: 0.45334517875826; roc=0.8563124519600307\n",
      "25: 0.44888280890810284; roc=0.8584202797468234\n",
      "26: 0.4473976084644409; roc=0.8566683035841376\n",
      "27: 0.44497078773702076; roc=0.8590625919283362\n",
      "28: 0.4464777306268653; roc=0.8626160669380628\n",
      "29: 0.44141346116482405; roc=0.8570718986344787\n",
      "30: 0.4415640717527791; roc=0.8566401319972292\n",
      "31: 0.4401004580591738; roc=0.8592111599814007\n",
      "32: 0.43759198138616007; roc=0.856333506514457\n",
      "33: 0.4383176251589821; roc=0.8560719555707386\n",
      "34: 0.4337599233017924; roc=0.8589042379556087\n",
      "35: 0.43252550777261306; roc=0.8564227659635038\n",
      "36: 0.43252412854520844; roc=0.8555251302416943\n",
      "37: 0.4319551060696141; roc=0.8530970359932056\n",
      "38: 0.43434886952061885; roc=0.8593348184207779\n",
      "39: 0.43020109926459166; roc=0.8623129999715318\n",
      "40: 0.42838945495934894; roc=0.8610838291532629\n",
      "41: 0.4299065349259735; roc=0.8615158923335328\n",
      "42: 0.42776208132305216; roc=0.8645014874597888\n",
      "43: 0.4231735862707695; roc=0.8604400342566496\n",
      "44: 0.4234478140075228; roc=0.8598946916427059\n",
      "45: 0.42396363145191795; roc=0.8541301918751957\n",
      "46: 0.4226420174758612; roc=0.8582462089940311\n",
      "47: 0.425274243740821; roc=0.8567712040121084\n",
      "48: 0.42416000879982524; roc=0.861253748303774\n",
      "49: 0.41905503764389607; roc=0.8590723778479992\n",
      "50: 0.420213737314521; roc=0.8570718986344787\n",
      "51: 0.4195864553868982; roc=0.8584870019263434\n",
      "52: 0.4226798554268326; roc=0.8577613611561856\n",
      "53: 0.42242706373635514; roc=0.8598952847287461\n",
      "54: 0.41818008919892735; roc=0.8563287618261357\n",
      "55: 0.41412967586541816; roc=0.8535338438617966\n",
      "56: 0.41688138771146904; roc=0.8585130977121113\n",
      "57: 0.4169047440125136; roc=0.8528858973629023\n",
      "58: 0.4178696327052676; roc=0.858528517949156\n",
      "59: 0.41571163969216196; roc=0.854021064043803\n",
      "60: 0.4191685084848582; roc=0.8635347572142985\n",
      "61: 0.4120814498796231; roc=0.8634371945606893\n",
      "62: 0.4131728796808248; roc=0.8667329736859586\n",
      "63: 0.4085646477239765; roc=0.8608946347064461\n",
      "64: 0.41264918078395246; roc=0.8581086130327098\n",
      "65: 0.4139329111281258; roc=0.8532206944325827\n",
      "66: 0.41304372200217704; roc=0.8622142511458423\n",
      "67: 0.40998219317610707; roc=0.8576101242159403\n",
      "68: 0.4081944690594945; roc=0.8562110342471603\n",
      "69: 0.4128660078068293; roc=0.8646138772644023\n",
      "70: 0.4074381012817815; roc=0.860950977880263\n",
      "71: 0.40709875598195977; roc=0.8555535983716229\n",
      "72: 0.40322840374448804; roc=0.8567273156451354\n",
      "73: 0.4118769175918278; roc=0.8547900000948937\n",
      "74: 0.41330922170972995; roc=0.8580546422030536\n",
      "75: 0.40761037782911497; roc=0.8640560798436151\n",
      "76: 0.40013276413099014; roc=0.8635122199447717\n",
      "77: 0.4036994819555217; roc=0.8571104492270902\n",
      "78: 0.40620177387451617; roc=0.8582174443210824\n",
      "79: 0.4066744065295951; roc=0.8655153680454731\n",
      "80: 0.40303442086928015; roc=0.8525881681707328\n",
      "81: 0.4018543413446322; roc=0.8557739298355492\n",
      "82: 0.40319591016183026; roc=0.8630069106385403\n",
      "83: 0.40347863368492043; roc=0.8446322154847647\n",
      "84: 0.40993657815538515; roc=0.8563308376272762\n",
      "85: 0.404740986507382; roc=0.8594552148869339\n",
      "86: 0.3976752389038916; roc=0.859931462977197\n",
      "87: 0.3997421324084758; roc=0.8588891142615842\n",
      "88: 0.4005392502554523; roc=0.8534739421717388\n",
      "89: 0.40681486022441266; roc=0.8563246102238544\n",
      "90: 0.39942641925747496; roc=0.8552668412711968\n",
      "91: 0.395460090983904; roc=0.853454666875433\n",
      "92: 0.3960918034409932; roc=0.8535489675558213\n",
      "93: 0.4025502178116158; roc=0.8574473220979114\n",
      "94: 0.39939092549870264; roc=0.85977488826259\n",
      "95: 0.3958555408037957; roc=0.8576145723612415\n",
      "96: 0.3962123300860269; roc=0.8602125857602414\n",
      "97: 0.39162318118703693; roc=0.8539949682580352\n",
      "98: 0.3970679507857543; roc=0.8538229733063835\n",
      "99: 0.39815055435457003; roc=0.863709124510111\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "train(mydataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a0211",
   "metadata": {},
   "source": [
    "### so without any feature engineer, gets to about 0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7448554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "881e5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.Adam?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
