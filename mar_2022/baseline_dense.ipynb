{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4772c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2014aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look_back = 504\n",
    "look_back = 72 * 3\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ce6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('train.csv', index_col='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f5336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dat):\n",
    "    time_mapper = {}\n",
    "    ii = 0\n",
    "    for h in range(24):\n",
    "        for mm in ['00','20','40']:\n",
    "            hh = '{0:02d}'.format(h)\n",
    "            time_mapper[hh+':'+mm] = ii\n",
    "            ii += 1\n",
    "\n",
    "    dat['unique'] = dat['x'].astype(str) + dat['y'].astype(str) + dat['direction']\n",
    "    uniques = dat['unique'].unique()\n",
    "    dat['day'] = pd.to_datetime(dat['time']).dt.weekday\n",
    "    dat['time_stamp'] = dat['time'].apply(lambda x:time_mapper[x.split()[1][:5]])\n",
    "\n",
    "    tmp = dat.groupby(['unique','day','time_stamp']).agg({'congestion':np.median})\n",
    "    median_mapper = tmp.to_dict()['congestion']\n",
    "    dat['median'] = dat.apply(lambda x: \\\n",
    "                              median_mapper[x['unique'],x['day'],x['time_stamp']], axis=1)\n",
    "    dat['congestion-median'] = dat['congestion'] - dat['median']\n",
    "    \n",
    "    all_time = pd.DataFrame(pd.date_range('1991-04-01 00:00:00', '1991-09-30 11:40:00', freq='20Min'), columns=['time'])\n",
    "    all_time['time'] = all_time['time'].astype(str)\n",
    "    \n",
    "    return uniques, median_mapper, time_mapper, all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfbe03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, median_mapper, time_mapper, all_time = preprocess(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05652a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getseries(unique):\n",
    "    df = dat.loc[dat['unique']==unique, ['time', 'congestion-median']]\n",
    "    df = pd.merge(all_time, df, left_on='time', right_on='time', how='outer')\n",
    "    df = df.set_index('time')\n",
    "    df['congestion-median'] = df['congestion-median'].fillna(0)\n",
    "    ss = StandardScaler()\n",
    "    df['congestion-median-normalized'] = ss.fit_transform(df['congestion-median'].values.reshape(-1,1)).reshape(-1)\n",
    "    return df, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f13a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=5):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+look_back])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9095cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(dat):\n",
    "    train_loaders, test_loaders = [], []\n",
    "    for period in test_periods_with_lookback:\n",
    "        train = dat.loc[dat.index < period[0], 'congestion-median-normalized'].values\n",
    "        test = dat.loc[(dat.index >= period[0]) & (dat.index <= period[1]), 'congestion-median-normalized'].values\n",
    "        print(test[0])\n",
    "        \n",
    "        X, y = create_dataset(train, look_back=look_back)\n",
    "        train_dataset = []\n",
    "        for i in range(len(X)):\n",
    "            train_dataset.append((torch.tensor(X[i],dtype=torch.float32),\n",
    "                                  torch.tensor(y[i].reshape(-1,),dtype=torch.float32)))\n",
    "        train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "        X, y = create_dataset(test, look_back=look_back)\n",
    "        test_dataset = []\n",
    "        for i in range(len(X)):\n",
    "            test_dataset.append((torch.tensor(X[i],dtype=torch.float32),\n",
    "                                 torch.tensor(y[i].reshape(-1,),dtype=torch.float32)))\n",
    "        test_loaders.append(DataLoader(test_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44cc4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32cde569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_feature, output_feature=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_feature, 64)\n",
    "        self.fc2 = nn.Linear(64,16)\n",
    "        self.fc3 = nn.Linear(16,output_feature)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ''' X is in the shape of (N,L,input_feature) '''\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be7d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            output = model.forward(x)\n",
    "            loss += criterion(output ,y).item() * len(x)\n",
    "            n += len(x)\n",
    "        loss /= n\n",
    "    return loss\n",
    "\n",
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            output = model.forward(x)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += loss*len(x)\n",
    "            n += len(x)\n",
    "        \n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        test_loss = evaluate(test_loader)\n",
    "        if (epoch % 20 == 0):  print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "    return best_n_epoches\n",
    "\n",
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            output = model.forward(x)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ae8245a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing 00EB\n",
      "[0.95494673] [11.57954203]\n",
      "2.0765115933378677\n",
      "0.5220459717995566\n",
      "current 0 training loss=0.8658768534660339 test loss = 0.831439197063446\n",
      "updating best loss 0 training loss=0.8658768534660339 test loss = 0.831439197063446\n",
      "updating best loss 1 training loss=0.851239800453186 test loss = 0.8220176696777344\n",
      "updating best loss 2 training loss=0.8314199447631836 test loss = 0.8182315230369568\n",
      "current 20 training loss=0.7292464375495911 test loss = 0.900221586227417\n",
      "current 40 training loss=0.6794531345367432 test loss = 0.916517972946167\n",
      "current 60 training loss=0.6488896012306213 test loss = 0.9022589921951294\n",
      "current 80 training loss=0.6054600477218628 test loss = 0.8821374773979187\n",
      "current 100 training loss=0.605268657207489 test loss = 0.8817000389099121\n",
      "current 120 training loss=0.5959994196891785 test loss = 0.8917797803878784\n",
      "current 140 training loss=0.5881227850914001 test loss = 0.8972421288490295\n",
      "current 160 training loss=0.5820416808128357 test loss = 0.8873663544654846\n",
      "current 180 training loss=0.5448833703994751 test loss = 0.864193320274353\n",
      "current 200 training loss=0.5265952348709106 test loss = 0.9107092618942261\n",
      "current 220 training loss=0.5442695021629333 test loss = 0.9069991111755371\n",
      "current 240 training loss=0.5311846137046814 test loss = 0.8913905024528503\n",
      "current 260 training loss=0.5377289652824402 test loss = 0.9487172961235046\n",
      "current 280 training loss=0.5196219682693481 test loss = 0.9326378703117371\n",
      "current 300 training loss=0.5161779522895813 test loss = 0.9091992378234863\n",
      "current 320 training loss=0.5182761549949646 test loss = 0.9340983629226685\n",
      "current 340 training loss=0.5019999742507935 test loss = 0.9347830414772034\n",
      "current 360 training loss=0.5390655994415283 test loss = 0.9240071177482605\n",
      "current 380 training loss=0.4820081293582916 test loss = 0.939500629901886\n",
      "current 400 training loss=0.517329216003418 test loss = 0.9206395745277405\n",
      "current 420 training loss=0.49465101957321167 test loss = 0.9130621552467346\n",
      "current 440 training loss=0.4668709337711334 test loss = 0.9267550706863403\n",
      "current 460 training loss=0.5071536302566528 test loss = 0.935360848903656\n",
      "current 480 training loss=0.49132660031318665 test loss = 0.8978369235992432\n",
      "current 500 training loss=0.4859848916530609 test loss = 0.8901680111885071\n",
      "current 520 training loss=0.48587605357170105 test loss = 0.9146181344985962\n",
      "current 540 training loss=0.4783192276954651 test loss = 0.9156997799873352\n",
      "current 560 training loss=0.4723733067512512 test loss = 0.9157736897468567\n",
      "current 580 training loss=0.49107611179351807 test loss = 0.9150247573852539\n",
      "current 600 training loss=0.4882935881614685 test loss = 0.8727152347564697\n",
      "current 620 training loss=0.512256383895874 test loss = 0.8970963358879089\n",
      "current 640 training loss=0.47973260283470154 test loss = 0.8877514600753784\n",
      "current 660 training loss=0.4944418668746948 test loss = 0.8794867992401123\n",
      "current 680 training loss=0.49099910259246826 test loss = 0.8497812747955322\n",
      "current 700 training loss=0.486237108707428 test loss = 0.8516627550125122\n",
      "current 720 training loss=0.490342378616333 test loss = 0.8306743502616882\n",
      "current 740 training loss=0.4901717007160187 test loss = 0.8608478903770447\n",
      "current 760 training loss=0.47207435965538025 test loss = 0.8644270300865173\n",
      "current 780 training loss=0.46828317642211914 test loss = 0.8763192296028137\n",
      "current 800 training loss=0.4764852821826935 test loss = 0.8546946048736572\n",
      "current 820 training loss=0.4796043038368225 test loss = 0.845571756362915\n",
      "current 840 training loss=0.4563409090042114 test loss = 0.8703673481941223\n",
      "current 860 training loss=0.46659085154533386 test loss = 0.8813818693161011\n",
      "current 880 training loss=0.4711727499961853 test loss = 0.9027068018913269\n",
      "current 900 training loss=0.4581596255302429 test loss = 0.9211564064025879\n",
      "current 920 training loss=0.4714052677154541 test loss = 0.9050948023796082\n",
      "current 940 training loss=0.47793158888816833 test loss = 0.8858705759048462\n",
      "current 960 training loss=0.490917831659317 test loss = 0.8639406561851501\n",
      "current 980 training loss=0.470550000667572 test loss = 0.8669071793556213\n",
      "current 1000 training loss=0.4615134298801422 test loss = 0.8482682704925537\n",
      "current 1020 training loss=0.46928808093070984 test loss = 0.9083482027053833\n",
      "current 1040 training loss=0.4862331748008728 test loss = 0.8831186890602112\n",
      "current 1060 training loss=0.4689757227897644 test loss = 0.8698083162307739\n",
      "current 1080 training loss=0.4667210280895233 test loss = 0.8766778707504272\n",
      "current 1100 training loss=0.4828576445579529 test loss = 0.8843117952346802\n",
      "current 1120 training loss=0.4490252733230591 test loss = 0.8845250010490417\n",
      "current 1140 training loss=0.45368289947509766 test loss = 0.880410373210907\n",
      "current 1160 training loss=0.490107923746109 test loss = 0.8729550242424011\n",
      "current 1180 training loss=0.4832971394062042 test loss = 0.8746233582496643\n",
      "current 1200 training loss=0.4848477840423584 test loss = 0.8758460879325867\n",
      "current 1220 training loss=0.48680487275123596 test loss = 0.8737722039222717\n",
      "current 1240 training loss=0.4448971450328827 test loss = 0.8954089879989624\n",
      "current 1260 training loss=0.4646719992160797 test loss = 0.8970043659210205\n",
      "current 1280 training loss=0.4584003984928131 test loss = 0.9384305477142334\n",
      "current 1300 training loss=0.481748104095459 test loss = 0.8880208730697632\n",
      "current 1320 training loss=0.4516456723213196 test loss = 0.874964714050293\n",
      "current 1340 training loss=0.4469124376773834 test loss = 0.8880361318588257\n",
      "current 1360 training loss=0.46474310755729675 test loss = 0.8975497484207153\n",
      "current 1380 training loss=0.4381261467933655 test loss = 0.8971285820007324\n",
      "current 1400 training loss=0.474559485912323 test loss = 0.9030044674873352\n",
      "current 1420 training loss=0.47055870294570923 test loss = 0.9304542541503906\n",
      "current 1440 training loss=0.4538247287273407 test loss = 0.9284213185310364\n",
      "current 1460 training loss=0.4599570333957672 test loss = 0.9265051484107971\n",
      "current 1480 training loss=0.44886094331741333 test loss = 0.9082284569740295\n",
      "current 1500 training loss=0.464187353849411 test loss = 0.9053868055343628\n",
      "current 1520 training loss=0.4441632926464081 test loss = 0.9047678112983704\n",
      "current 1540 training loss=0.4489259123802185 test loss = 0.9253202080726624\n",
      "current 1560 training loss=0.4500797688961029 test loss = 0.904084324836731\n",
      "current 1580 training loss=0.4468851089477539 test loss = 0.9121315479278564\n",
      "current 1600 training loss=0.4918646216392517 test loss = 0.8883511424064636\n",
      "current 1620 training loss=0.4697113335132599 test loss = 0.8981377482414246\n",
      "current 1640 training loss=0.4954336881637573 test loss = 0.8967879414558411\n",
      "current 1660 training loss=0.46210572123527527 test loss = 0.9111781120300293\n",
      "current 1680 training loss=0.4527863562107086 test loss = 0.9096354842185974\n",
      "current 1700 training loss=0.42898234724998474 test loss = 0.901427149772644\n",
      "current 1720 training loss=0.4395938813686371 test loss = 0.9150994420051575\n",
      "current 1740 training loss=0.4770644009113312 test loss = 0.8834695816040039\n",
      "current 1760 training loss=0.4756890535354614 test loss = 0.8898056745529175\n",
      "current 1780 training loss=0.4721544682979584 test loss = 0.9144033193588257\n",
      "current 1800 training loss=0.4565887749195099 test loss = 0.9114059209823608\n",
      "current 1820 training loss=0.4416317939758301 test loss = 0.9435373544692993\n",
      "current 1840 training loss=0.47477248311042786 test loss = 0.9528340101242065\n",
      "current 1860 training loss=0.4423171579837799 test loss = 0.923457682132721\n",
      "current 1880 training loss=0.44559991359710693 test loss = 0.9305058121681213\n",
      "current 1900 training loss=0.4529232084751129 test loss = 0.9451214671134949\n",
      "current 1920 training loss=0.44915276765823364 test loss = 0.942843496799469\n",
      "current 1940 training loss=0.44281619787216187 test loss = 0.9275915026664734\n",
      "current 1960 training loss=0.45584556460380554 test loss = 0.9303741455078125\n",
      "current 1980 training loss=0.44607996940612793 test loss = 0.918718159198761\n",
      "refitting with {best_n_epoches}\n",
      "doing 00NB\n",
      "[0.36636225] [9.43834367]\n",
      "-0.19774256142252622\n",
      "0.6498637856960497\n",
      "current 0 training loss=0.651485025882721 test loss = 0.6107003092765808\n",
      "updating best loss 0 training loss=0.651485025882721 test loss = 0.6107003092765808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current 20 training loss=0.607338011264801 test loss = 0.6436548233032227\n",
      "current 40 training loss=0.5540330410003662 test loss = 0.6441318392753601\n",
      "current 60 training loss=0.530771017074585 test loss = 0.6253974437713623\n",
      "current 80 training loss=0.5100052952766418 test loss = 0.6370318531990051\n",
      "current 100 training loss=0.5112903118133545 test loss = 0.6440532207489014\n",
      "current 120 training loss=0.49564871191978455 test loss = 0.6448232531547546\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(look_back)\n\u001b[1;32m     20\u001b[0m train_loaders, test_loaders \u001b[38;5;241m=\u001b[39m assemble(df)\n\u001b[0;32m---> 21\u001b[0m best_n_epoches \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(look_back)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefitting with \u001b[39m\u001b[38;5;132;01m{best_n_epoches}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epoches, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     22\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     24\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:83\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m     transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 24:00:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 24:00:00']]\n",
    "\n",
    "all_ss = {}\n",
    "torch.manual_seed(123)\n",
    "for unique in uniques[:32]:\n",
    "    print(f\"doing {unique}\")\n",
    "    \n",
    "    df, ss = getseries(unique)\n",
    "    print(ss.mean_, ss.scale_)\n",
    "    all_ss[unique] = ss\n",
    "    \n",
    "    test_periods_with_lookback = []\n",
    "    for period in test_periods:\n",
    "        id1 = df.index.to_list().index(period[0])\n",
    "        test_periods_with_lookback.append([df.index[id1-look_back], period[1]])\n",
    "    \n",
    "    model = MyModel(look_back)\n",
    "    train_loaders, test_loaders = assemble(df)\n",
    "    best_n_epoches = train(2000, train_loaders[0], test_loaders[0])\n",
    "    \n",
    "    model = MyModel(look_back)\n",
    "    print('refitting with {best_n_epoches}')\n",
    "    retrain(best_n_epoches, train_loaders[1])\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model_'+unique+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e8a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ce7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test[(test['x']==0) & (test['y']==0) & (test['direction']=='EB')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83871aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for unique in uniques[60:61]:\n",
    "        print(unique)\n",
    "        df, ss = getseries(unique)\n",
    "        print(ss.mean_, ss.scale_)\n",
    "        model.load_state_dict(torch.load('model_'+unique+'.pickle'))\n",
    "        X, y = create_dataset(df['congestion-median-normalized'])\n",
    "        print(X)\n",
    "        predict = np.zeros(36)\n",
    "        for i in range(36)[0:1]:\n",
    "            X = torch.tensor(X, dtype=torch.float32).reshape(1,-1,1)\n",
    "            print(X.shape)\n",
    "            h0 = model.initHidden(1)\n",
    "            print(model.forward(X,h0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        X, y = create_dataset(df['congestion-median-normalized'], look_back=look_back)\n",
    "        print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196474a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(X[0],dtype=torch.float32).reshape(1,-1,1)\n",
    "h0 = model.initHidden(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60984239",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b33548",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = torch.tensor(y[0],dtype=torch.float32).reshape(1,-1,1)\n",
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.forward(x,h0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51565eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(y_pred, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = y_pred.detach().numpy()\n",
    "y_pred = y_pred.reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5674f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = y_target.detach().numpy().reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae36157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred.T, y_target.T, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23910c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(look_back), y_pred.T)\n",
    "plt.plot(range(look_back), y_target.T,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fae9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss += criterion(output[:,-10,:],y[:,-10,:]).item() * len(x)\n",
    "            n += len(x)\n",
    "        loss /= n\n",
    "    return loss\n",
    "\n",
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            print(output[-1,-10,:],y[-1,-10,:])\n",
    "            loss = criterion(output[:,-10,:], y[:,-10,:])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += loss*len(x)\n",
    "            n += len(x)\n",
    "        \n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        test_loss = evaluate(test_loader)\n",
    "#         if (epoch % 20 == 0):  print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "    return best_n_epoches\n",
    "\n",
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59285da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 24:00:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 24:00:00']]\n",
    "\n",
    "all_ss = {}\n",
    "torch.manual_seed(123)\n",
    "for unique in uniques[0:1]:\n",
    "    print(f\"doing {unique}\")\n",
    "    \n",
    "    df, ss = getseries(unique)\n",
    "    print(ss.mean_, ss.scale_)\n",
    "    all_ss[unique] = ss\n",
    "    \n",
    "    test_periods_with_lookback = []\n",
    "    for period in test_periods:\n",
    "        id1 = df.index.to_list().index(period[0])\n",
    "        test_periods_with_lookback.append([df.index[id1-look_back], period[1]])\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)\n",
    "    train_loaders, test_loaders = assemble(df)\n",
    "    best_n_epoches = train(200, train_loaders[0], test_loaders[0])\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)\n",
    "    print('refitting with {best_n_epoches}')\n",
    "    retrain(best_n_epoches, train_loaders[1])\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model_'+unique+'.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
