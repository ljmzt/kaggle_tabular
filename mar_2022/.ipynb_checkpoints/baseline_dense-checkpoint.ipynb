{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4772c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2014aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look_back = 504\n",
    "look_back = 72 * 3\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5ce6cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('train.csv', index_col='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76f5336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dat):\n",
    "    time_mapper = {}\n",
    "    ii = 0\n",
    "    for h in range(24):\n",
    "        for mm in ['00','20','40']:\n",
    "            hh = '{0:02d}'.format(h)\n",
    "            time_mapper[hh+':'+mm] = ii\n",
    "            ii += 1\n",
    "\n",
    "    dat['unique'] = dat['x'].astype(str) + dat['y'].astype(str) + dat['direction']\n",
    "    uniques = dat['unique'].unique()\n",
    "    dat['day'] = pd.to_datetime(dat['time']).dt.weekday\n",
    "    dat['time_stamp'] = dat['time'].apply(lambda x:time_mapper[x.split()[1][:5]])\n",
    "\n",
    "    tmp = dat.groupby(['unique','day','time_stamp']).agg({'congestion':np.median})\n",
    "    median_mapper = tmp.to_dict()['congestion']\n",
    "    dat['median'] = dat.apply(lambda x: \\\n",
    "                              median_mapper[x['unique'],x['day'],x['time_stamp']], axis=1)\n",
    "    dat['congestion-median'] = dat['congestion'] - dat['median']\n",
    "    \n",
    "    all_time = pd.DataFrame(pd.date_range('1991-04-01 00:00:00', '1991-09-30 11:40:00', freq='20Min'), columns=['time'])\n",
    "    all_time['time'] = all_time['time'].astype(str)\n",
    "    \n",
    "    return uniques, median_mapper, time_mapper, all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfbe03e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, median_mapper, time_mapper, all_time = preprocess(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05652a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getseries(unique):\n",
    "    df = dat.loc[dat['unique']==unique, ['time', 'congestion-median']]\n",
    "    df = pd.merge(all_time, df, left_on='time', right_on='time', how='outer')\n",
    "    df = df.set_index('time')\n",
    "    df['congestion-median'] = df['congestion-median'].fillna(0)\n",
    "    ss = StandardScaler()\n",
    "    df['congestion-median-normalized'] = ss.fit_transform(df['congestion-median'].values.reshape(-1,1)).reshape(-1)\n",
    "    return df, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f13a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=5):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+look_back])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9095cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(dat):\n",
    "    train_loaders, test_loaders = [], []\n",
    "    for period in test_periods_with_lookback:\n",
    "        train = dat.loc[dat.index < period[0], 'congestion-median-normalized'].values\n",
    "        test = dat.loc[(dat.index >= period[0]) & (dat.index <= period[1]), 'congestion-median-normalized'].values\n",
    "        print(test[0])\n",
    "        \n",
    "        X, y = create_dataset(train, look_back=look_back)\n",
    "        train_dataset = []\n",
    "        for i in range(len(X)):\n",
    "            train_dataset.append((torch.tensor(X[i],dtype=torch.float32),\n",
    "                                  torch.tensor(y[i].reshape(-1,),dtype=torch.float32)))\n",
    "        train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "        X, y = create_dataset(test, look_back=look_back)\n",
    "        test_dataset = []\n",
    "        for i in range(len(X)):\n",
    "            test_dataset.append((torch.tensor(X[i],dtype=torch.float32),\n",
    "                                 torch.tensor(y[i].reshape(-1,),dtype=torch.float32)))\n",
    "        test_loaders.append(DataLoader(test_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44cc4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32cde569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_feature, output_feature=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_feature, 64)\n",
    "        self.fc2 = nn.Linear(64,16)\n",
    "        self.fc3 = nn.Linear(16,output_feature)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ''' X is in the shape of (N,L,input_feature) '''\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be7d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            output = model.forward(x)\n",
    "            loss += criterion(output ,y).item() * len(x)\n",
    "            n += len(x)\n",
    "        loss /= n\n",
    "    return loss\n",
    "\n",
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            output = model.forward(x)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += loss*len(x)\n",
    "            n += len(x)\n",
    "        \n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        test_loss = evaluate(test_loader)\n",
    "        if (epoch % 20 == 0):  print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "    return best_n_epoches\n",
    "\n",
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            output = model.forward(x)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ae8245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing 00EB\n",
      "[0.95494673] [11.57954203]\n",
      "2.0765115933378677\n",
      "0.5220459717995566\n",
      "current 0 training loss=0.8658768534660339 test loss = 0.831439197063446\n",
      "updating best loss 0 training loss=0.8658768534660339 test loss = 0.831439197063446\n",
      "updating best loss 1 training loss=0.851239800453186 test loss = 0.8220176696777344\n",
      "updating best loss 2 training loss=0.8314199447631836 test loss = 0.8182315230369568\n",
      "current 20 training loss=0.7292464375495911 test loss = 0.900221586227417\n",
      "current 40 training loss=0.6794531345367432 test loss = 0.916517972946167\n",
      "current 60 training loss=0.6488896012306213 test loss = 0.9022589921951294\n",
      "current 80 training loss=0.6054600477218628 test loss = 0.8821374773979187\n",
      "current 100 training loss=0.605268657207489 test loss = 0.8817000389099121\n",
      "current 120 training loss=0.5959994196891785 test loss = 0.8917797803878784\n",
      "current 140 training loss=0.5881227850914001 test loss = 0.8972421288490295\n",
      "current 160 training loss=0.5820416808128357 test loss = 0.8873663544654846\n",
      "current 180 training loss=0.5448833703994751 test loss = 0.864193320274353\n",
      "refitting with {best_n_epoches}\n",
      "doing 00NB\n",
      "[0.36636225] [9.43834367]\n",
      "-0.19774256142252622\n",
      "0.6498637856960497\n",
      "current 0 training loss=0.6623544096946716 test loss = 0.6126546859741211\n",
      "updating best loss 0 training loss=0.6623544096946716 test loss = 0.6126546859741211\n",
      "updating best loss 1 training loss=0.6471397876739502 test loss = 0.6086345911026001\n",
      "updating best loss 2 training loss=0.6452891826629639 test loss = 0.6032648682594299\n",
      "current 20 training loss=0.5840851068496704 test loss = 0.6144702434539795\n",
      "updating best loss 28 training loss=0.5845450162887573 test loss = 0.603241503238678\n",
      "updating best loss 29 training loss=0.5661404728889465 test loss = 0.6000201106071472\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(look_back)\n\u001b[1;32m     20\u001b[0m train_loaders, test_loaders \u001b[38;5;241m=\u001b[39m assemble(df)\n\u001b[0;32m---> 21\u001b[0m best_n_epoches \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(look_back)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefitting with \u001b[39m\u001b[38;5;132;01m{best_n_epoches}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epoches, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     22\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     24\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:84\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     83\u001b[0m     transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:84\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     83\u001b[0m     transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:56\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     54\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel)\n\u001b[1;32m     55\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 24:00:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 24:00:00']]\n",
    "\n",
    "all_ss = {}\n",
    "torch.manual_seed(123)\n",
    "for unique in uniques[:32]:\n",
    "    print(f\"doing {unique}\")\n",
    "    \n",
    "    df, ss = getseries(unique)\n",
    "    print(ss.mean_, ss.scale_)\n",
    "    all_ss[unique] = ss\n",
    "    \n",
    "    test_periods_with_lookback = []\n",
    "    for period in test_periods:\n",
    "        id1 = df.index.to_list().index(period[0])\n",
    "        test_periods_with_lookback.append([df.index[id1-look_back], period[1]])\n",
    "    \n",
    "    model = MyModel(look_back)\n",
    "    train_loaders, test_loaders = assemble(df)\n",
    "    best_n_epoches = train(2000, train_loaders[0], test_loaders[0])\n",
    "    \n",
    "    model = MyModel(look_back)\n",
    "    print('refitting with {best_n_epoches}')\n",
    "    retrain(best_n_epoches, train_loaders[1])\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model_'+unique+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e8a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b60ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ce7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test[(test['x']==0) & (test['y']==0) & (test['direction']=='EB')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83871aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for unique in uniques[60:61]:\n",
    "        print(unique)\n",
    "        df, ss = getseries(unique)\n",
    "        print(ss.mean_, ss.scale_)\n",
    "        model.load_state_dict(torch.load('model_'+unique+'.pickle'))\n",
    "        X, y = create_dataset(df['congestion-median-normalized'])\n",
    "        print(X)\n",
    "        predict = np.zeros(36)\n",
    "        for i in range(36)[0:1]:\n",
    "            X = torch.tensor(X, dtype=torch.float32).reshape(1,-1,1)\n",
    "            print(X.shape)\n",
    "            h0 = model.initHidden(1)\n",
    "            print(model.forward(X,h0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        X, y = create_dataset(df['congestion-median-normalized'], look_back=look_back)\n",
    "        print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196474a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(X[0],dtype=torch.float32).reshape(1,-1,1)\n",
    "h0 = model.initHidden(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60984239",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b33548",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = torch.tensor(y[0],dtype=torch.float32).reshape(1,-1,1)\n",
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.forward(x,h0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51565eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(y_pred, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = y_pred.detach().numpy()\n",
    "y_pred = y_pred.reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5674f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = y_target.detach().numpy().reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae36157",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred.T, y_target.T, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23910c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(look_back), y_pred.T)\n",
    "plt.plot(range(look_back), y_target.T,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fae9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss += criterion(output[:,-10,:],y[:,-10,:]).item() * len(x)\n",
    "            n += len(x)\n",
    "        loss /= n\n",
    "    return loss\n",
    "\n",
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            print(output[-1,-10,:],y[-1,-10,:])\n",
    "            loss = criterion(output[:,-10,:], y[:,-10,:])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += loss*len(x)\n",
    "            n += len(x)\n",
    "        \n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        test_loss = evaluate(test_loader)\n",
    "#         if (epoch % 20 == 0):  print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "    return best_n_epoches\n",
    "\n",
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59285da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 24:00:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 24:00:00']]\n",
    "\n",
    "all_ss = {}\n",
    "torch.manual_seed(123)\n",
    "for unique in uniques[0:1]:\n",
    "    print(f\"doing {unique}\")\n",
    "    \n",
    "    df, ss = getseries(unique)\n",
    "    print(ss.mean_, ss.scale_)\n",
    "    all_ss[unique] = ss\n",
    "    \n",
    "    test_periods_with_lookback = []\n",
    "    for period in test_periods:\n",
    "        id1 = df.index.to_list().index(period[0])\n",
    "        test_periods_with_lookback.append([df.index[id1-look_back], period[1]])\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)\n",
    "    train_loaders, test_loaders = assemble(df)\n",
    "    best_n_epoches = train(200, train_loaders[0], test_loaders[0])\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)\n",
    "    print('refitting with {best_n_epoches}')\n",
    "    retrain(best_n_epoches, train_loaders[1])\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model_'+unique+'.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
