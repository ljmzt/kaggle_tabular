{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7ba8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a2db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 72\n",
    "batch_size = 512\n",
    "linear_node = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23077b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('train.csv', index_col='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff3fd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dat):\n",
    "    time_mapper = {}\n",
    "    ii = 0\n",
    "    for h in range(24):\n",
    "        for mm in ['00','20','40']:\n",
    "            hh = '{0:02d}'.format(h)\n",
    "            time_mapper[hh+':'+mm] = ii\n",
    "            ii += 1\n",
    "\n",
    "    dat['unique'] = dat['x'].astype(str) + dat['y'].astype(str) + dat['direction']\n",
    "    uniques = dat['unique'].unique()\n",
    "    dat['day'] = pd.to_datetime(dat['time']).dt.weekday\n",
    "    dat['time_stamp'] = dat['time'].apply(lambda x:time_mapper[x.split()[1][:5]])\n",
    "\n",
    "    tmp = dat.groupby(['unique','day','time_stamp']).agg({'congestion':np.median})\n",
    "    median_mapper = tmp.to_dict()['congestion']\n",
    "    dat['median'] = dat.apply(lambda x: \\\n",
    "                              median_mapper[x['unique'],x['day'],x['time_stamp']], axis=1)\n",
    "    dat['congestion-median'] = dat['congestion'] - dat['median']\n",
    "    \n",
    "    all_time = pd.DataFrame(pd.date_range('1991-04-01 00:00:00', '1991-09-30 11:40:00', freq='20Min'), columns=['time'])\n",
    "    all_time['time'] = all_time['time'].astype(str)\n",
    "    \n",
    "    return uniques, median_mapper, time_mapper, all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cffdfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, median_mapper, time_mapper, all_time = preprocess(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ea94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getseries(unique):\n",
    "    df = dat.loc[dat['unique']==unique, ['time', 'congestion-median']]\n",
    "    df = pd.merge(all_time, df, left_on='time', right_on='time', how='outer')\n",
    "    df = df.set_index('time')\n",
    "    df['congestion-median'] = df['congestion-median'].fillna(0)\n",
    "    ss = StandardScaler()\n",
    "    df['congestion-median-normalized'] = ss.fit_transform(df['congestion-median'].values.reshape(-1,1)).reshape(-1)\n",
    "    return df, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63b2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=5):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+look_back])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f57bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(dat):\n",
    "    train_loaders, test_loaders = [], []\n",
    "    for period in test_periods_with_lookback:\n",
    "        train = dat.loc[dat.index < period[0], 'congestion-median-normalized'].values\n",
    "        test = dat.loc[(dat.index >= period[0]) & (dat.index <= period[1]), 'congestion-median-normalized'].values\n",
    "        print(test[0])\n",
    "        \n",
    "        X, y = create_dataset(train, look_back=look_back)\n",
    "        train_dataset = []\n",
    "        for i in range(len(X)):\n",
    "            train_dataset.append((torch.tensor(X[i].reshape(-1,1),dtype=torch.float32),\n",
    "                                  torch.tensor(y[i].reshape(-1,),dtype=torch.float32)))\n",
    "        train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "        X, y = create_dataset(test, look_back=look_back)\n",
    "        test_dataset = []\n",
    "        for i in range(len(X)):\n",
    "            test_dataset.append((torch.tensor(X[i].reshape(-1,1),dtype=torch.float32),\n",
    "                                 torch.tensor(y[i].reshape(-1,),dtype=torch.float32)))\n",
    "        test_loaders.append(DataLoader(test_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c7b54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df9f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_size, output_feature, num_layers=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_feature, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers, dropout=0.2)\n",
    "        ''' gru input is (N,L,H_in=H_hidden), output is (N,L,H_hidden), hidden is (num_layers, h_hidden)'''\n",
    "        self.linear_out = nn.Linear(hidden_size, output_feature)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        ''' X is in the shape of (N,L,input_feature) '''\n",
    "        output = F.relu(self.linear(input))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear_out(F.relu(output))\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros((self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "833aa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss += criterion(output[:,-1,:],y).item() * len(x)\n",
    "            n += len(x)\n",
    "        loss /= n\n",
    "    return loss\n",
    "\n",
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            print(output[-1,-1,:],y[-1])\n",
    "            loss = criterion(output[:,-1,:], y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += loss*len(x)\n",
    "            n += len(x)\n",
    "        \n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        test_loss = evaluate(test_loader)\n",
    "#         if (epoch % 20 == 0):  print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "            \n",
    "        if epoch > best_n_epoches + 10:\n",
    "            print('early stop')\n",
    "            break\n",
    "    return best_n_epoches\n",
    "\n",
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss = criterion(output[:,-1,:], y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46bf4266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>congestion-median</th>\n",
       "      <th>congestion-median-normalized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1991-04-01 00:00:00</th>\n",
       "      <td>35.0</td>\n",
       "      <td>2.940104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01 00:20:00</th>\n",
       "      <td>35.0</td>\n",
       "      <td>2.940104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01 00:40:00</th>\n",
       "      <td>35.0</td>\n",
       "      <td>2.940104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01 01:00:00</th>\n",
       "      <td>35.0</td>\n",
       "      <td>2.940104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01 01:20:00</th>\n",
       "      <td>35.0</td>\n",
       "      <td>2.940104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-09-30 10:20:00</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.686983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-09-30 10:40:00</th>\n",
       "      <td>-6.0</td>\n",
       "      <td>-0.600624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-09-30 11:00:00</th>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.686983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-09-30 11:20:00</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.522046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-09-30 11:40:00</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.385638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13140 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     congestion-median  congestion-median-normalized\n",
       "time                                                                \n",
       "1991-04-01 00:00:00               35.0                      2.940104\n",
       "1991-04-01 00:20:00               35.0                      2.940104\n",
       "1991-04-01 00:40:00               35.0                      2.940104\n",
       "1991-04-01 01:00:00               35.0                      2.940104\n",
       "1991-04-01 01:20:00               35.0                      2.940104\n",
       "...                                ...                           ...\n",
       "1991-09-30 10:20:00               -7.0                     -0.686983\n",
       "1991-09-30 10:40:00               -6.0                     -0.600624\n",
       "1991-09-30 11:00:00               -7.0                     -0.686983\n",
       "1991-09-30 11:20:00                7.0                      0.522046\n",
       "1991-09-30 11:40:00               17.0                      1.385638\n",
       "\n",
       "[13140 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "537bc666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing 00EB\n",
      "[0.95494673] [11.57954203] 1.0000380539224218\n",
      "0.6947643741927023\n",
      "0.0902499658166924\n",
      "tensor([0.0362], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-0.0249], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.0502], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.0956], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.0861], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.1112], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1095], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.1554], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.1208], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.0979], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0274], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([-0.1248], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1249], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0834], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([-0.0243], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0369], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-0.0200], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.0257], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.0477], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.1658], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0772], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.0439], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([-0.0698], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([0.0043], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 0 training loss=0.6977322101593018 test loss = 0.8408336043357849\n",
      "updating best loss 0 training loss=0.6977322101593018 test loss = 0.8408336043357849\n",
      "tensor([-0.0390], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-0.4419], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.0874], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.0191], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1572], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.0006], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.0191], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2751], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0402], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([0.0431], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.5114], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([-0.0590], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([0.0109], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0675], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.1009], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1977], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1318], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2021], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2867], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.5423], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.1591], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.3504], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([-0.1401], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.1289], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 1 training loss=0.6531557440757751 test loss = 0.8812780976295471\n",
      "tensor([-0.2510], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-0.8143], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.2142], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2949], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1978], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1601], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.2005], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.4443], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2043], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.1153], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.7132], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.0448], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1496], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0791], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.2699], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.2827], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.3773], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2593], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.3484], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.3687], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.1495], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.5261], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.0853], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.1462], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 2 training loss=0.6500548124313354 test loss = 0.8657872676849365\n",
      "tensor([-0.2614], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-0.8782], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3540], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.1226], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1011], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1491], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.3307], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.5649], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2415], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.3637], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.8232], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2070], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0866], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0052], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.3455], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1613], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.3388], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.4883], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.3701], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.4919], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.1308], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.3828], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([-0.0262], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.1182], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 3 training loss=0.6520181894302368 test loss = 0.8511437177658081\n",
      "tensor([-0.2357], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-0.8034], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.1803], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2174], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1694], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.2216], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.3814], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.3172], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2410], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.2399], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.6269], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1303], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1479], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0775], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.1389], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1843], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.3375], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.3452], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2958], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.4205], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.1423], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.5647], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1517], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0218], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 4 training loss=0.6414143443107605 test loss = 0.8407588601112366\n",
      "updating best loss 4 training loss=0.6414143443107605 test loss = 0.8407588601112366\n",
      "tensor([-0.2533], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-0.9319], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3347], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2466], grad_fn=<SliceBackward0>) tensor([0.2630])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2054], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.2377], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.6393], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.5778], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.3482], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.4690], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.6384], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1907], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1660], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0822], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.2601], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.2640], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.3092], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.4411], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2868], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.3137], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0719], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.6353], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1881], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.1085], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 5 training loss=0.646555483341217 test loss = 0.8300093412399292\n",
      "updating best loss 5 training loss=0.646555483341217 test loss = 0.8300093412399292\n",
      "tensor([-0.1091], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-0.7901], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3954], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.1322], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2366], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1442], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.6853], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.5509], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.3269], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.2226], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.2380], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1297], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1309], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0712], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.1359], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.3279], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.2626], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.4764], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2979], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.4816], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0999], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2516], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1753], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0330], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 6 training loss=0.6502764821052551 test loss = 0.8277690410614014\n",
      "updating best loss 6 training loss=0.6502764821052551 test loss = 0.8277690410614014\n",
      "tensor([-0.2925], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.0683], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3151], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.1983], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2676], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1823], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.5588], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.5487], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.3048], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.3052], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.1442], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1972], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0968], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0353], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.1870], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1653], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.3041], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2581], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2473], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.3848], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0922], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1481], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1769], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0569], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 7 training loss=0.637987494468689 test loss = 0.8283444046974182\n",
      "tensor([-0.3010], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.1623], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.4130], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2304], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2249], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1285], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.3262], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.5280], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2326], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.2172], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.7480], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1625], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1681], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0608], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.1220], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.2257], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.2830], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.3151], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2594], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.5012], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0282], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.3284], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1715], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0866], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 8 training loss=0.646628201007843 test loss = 0.8304017782211304\n",
      "tensor([-0.3701], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.0793], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.2859], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2341], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1362], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1602], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.5202], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.5018], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2621], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.3031], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.1572], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1316], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0725], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0177], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.1680], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.2115], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1671], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.3593], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.1651], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.3803], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0491], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2996], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1873], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0584], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 9 training loss=0.6450971364974976 test loss = 0.8289836049079895\n",
      "tensor([-0.3316], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.0830], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3905], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.3575], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2283], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1893], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.5204], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.4597], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2372], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.2861], grad_fn=<SliceBackward0>) tensor([-1.5506])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0754], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.0930], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0737], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0163], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.1474], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.2105], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.2160], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2984], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2639], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.5324], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.1044], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.3207], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1927], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.1052], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 10 training loss=0.6413906812667847 test loss = 0.8256176114082336\n",
      "updating best loss 10 training loss=0.6413906812667847 test loss = 0.8256176114082336\n",
      "tensor([-0.2805], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.1813], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3444], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2363], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1960], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.0808], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.7055], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.3678], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2127], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.1466], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.0918], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.0247], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0979], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0123], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0961], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.2238], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1668], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.3582], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2515], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.3399], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0026], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.3203], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1956], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0327], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 11 training loss=0.6349573135375977 test loss = 0.8236303925514221\n",
      "updating best loss 11 training loss=0.6349573135375977 test loss = 0.8236303925514221\n",
      "tensor([-0.4096], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.1674], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3290], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2138], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1544], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.2148], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.4828], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.3092], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.1940], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.2609], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0870], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1391], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0311], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0094], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0746], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.2206], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1756], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.1980], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2312], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.4685], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0559], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2599], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.2159], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0832], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 12 training loss=0.6427577137947083 test loss = 0.8158844709396362\n",
      "updating best loss 12 training loss=0.6427577137947083 test loss = 0.8158844709396362\n",
      "tensor([-0.3432], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.2818], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3822], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2727], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2231], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1723], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.3973], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.3208], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2459], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.2988], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.0079], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1799], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0847], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0100], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0969], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1884], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.0915], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2233], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2030], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.4491], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([0.0027], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1528], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1503], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0657], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 13 training loss=0.6329721212387085 test loss = 0.8185831904411316\n",
      "tensor([-0.2786], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.3844], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3689], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2560], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2788], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1161], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.6714], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.3981], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.1738], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.3089], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0443], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1801], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0708], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0779], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0300], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1383], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1428], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.3482], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.1953], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.3185], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0065], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1595], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.0781], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.1004], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 14 training loss=0.627749502658844 test loss = 0.8115652799606323\n",
      "updating best loss 14 training loss=0.627749502658844 test loss = 0.8115652799606323\n",
      "tensor([-0.3766], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.3229], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3712], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.3044], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2688], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1389], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.3044], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.3409], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2702], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.3025], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0973], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1190], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0995], grad_fn=<SliceBackward0>) tensor([1.0402])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0182], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0960], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.2649], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.0954], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2923], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2620], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.3118], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0321], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1049], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1310], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0895], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 15 training loss=0.6367776393890381 test loss = 0.8063948750495911\n",
      "updating best loss 15 training loss=0.6367776393890381 test loss = 0.8063948750495911\n",
      "tensor([-0.3515], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.3609], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3129], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2393], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.3100], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1502], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.6006], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2802], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2642], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.3827], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0681], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1161], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1651], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0234], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0378], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1776], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.0952], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.3377], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2762], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.3198], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0021], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1530], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1051], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0894], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 16 training loss=0.6331640481948853 test loss = 0.8092992305755615\n",
      "tensor([-0.3566], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.4021], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.4155], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2595], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1670], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1314], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.2442], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.1978], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2077], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.3461], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.0163], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2154], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1322], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0093], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0732], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1723], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1523], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2345], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2698], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.2103], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([0.0351], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2045], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1176], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0714], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 17 training loss=0.6290116906166077 test loss = 0.815617561340332\n",
      "tensor([-0.3846], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.4391], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3373], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2338], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2410], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.0903], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.4275], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2272], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.1882], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.4338], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0800], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.3634], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0553], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0449], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0673], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1486], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.0835], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2921], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2231], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.2112], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0307], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1269], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1040], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0606], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 18 training loss=0.6190903186798096 test loss = 0.813067615032196\n",
      "tensor([-0.3101], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.3176], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3168], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.1825], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2983], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1080], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.2162], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2448], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.1525], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.4766], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.0950], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1920], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0825], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0172], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.1161], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1764], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.0863], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.3271], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2282], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.2425], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.1576], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1766], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.0389], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0755], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 19 training loss=0.6217769980430603 test loss = 0.8110575675964355\n",
      "tensor([-0.4896], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.4480], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.2097], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.1181], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1816], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.1885], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.2624], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2812], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.1319], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.4432], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0940], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2382], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0872], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0134], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0635], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1630], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1132], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2758], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2068], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.2572], grad_fn=<SliceBackward0>) tensor([-1.3347])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0747], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1178], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1139], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.1027], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 20 training loss=0.6299672722816467 test loss = 0.8099434971809387\n",
      "tensor([-0.3796], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.5306], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.2981], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.1910], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2545], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.0903], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.2116], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2228], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.1572], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.5220], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0359], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1163], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1295], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0337], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0465], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1319], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1025], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.3534], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2000], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.3304], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0511], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1386], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.0821], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0652], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 21 training loss=0.6209834218025208 test loss = 0.8181188106536865\n",
      "tensor([-0.4155], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.0679], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.2738], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.1469], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1742], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.0974], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.2011], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2324], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.1252], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.3949], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.0377], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2412], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1302], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0457], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.1476], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1135], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.0529], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2264], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2275], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.3003], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0838], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1792], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.1366], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0508], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 22 training loss=0.6269393563270569 test loss = 0.8177710771560669\n",
      "tensor([-0.4933], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.0909], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.2167], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.1941], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.3088], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.0644], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.2542], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2253], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.1558], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.3713], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.0564], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1343], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0698], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([-0.0778], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([-0.0098], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0938], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.0922], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.1973], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.1839], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.2303], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.1173], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1383], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.0552], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0776], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 23 training loss=0.6174033880233765 test loss = 0.8132534623146057\n",
      "tensor([-0.5117], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.4708], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3222], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.2263], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1748], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.0236], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.2727], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2241], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.1985], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.4719], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1217], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2950], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.1007], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0713], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0147], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1281], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1325], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.3290], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.1397], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.2767], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.1340], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.0473], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.0380], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0884], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 24 training loss=0.6271370053291321 test loss = 0.8154208064079285\n",
      "tensor([-0.4906], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.1866], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.2323], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.1248], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2586], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.0407], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.1387], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2592], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2479], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.4574], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0873], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1774], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0932], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0004], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.1077], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1327], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1306], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.3181], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2358], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.0957], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.1588], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.0976], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.0337], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0822], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 25 training loss=0.6228607892990112 test loss = 0.8138887882232666\n",
      "tensor([-0.5834], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-1.2370], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.3054], grad_fn=<SliceBackward0>) tensor([3.0265])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1594], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2753], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.0714], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.2164], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2250], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.1911], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.8039], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.0043], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1607], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0326], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0119], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0825], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.0306], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.2091], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.1586], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.3219], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.1322], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.1457], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1326], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.0811], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0535], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 26 training loss=0.6207947731018066 test loss = 0.8134244084358215\n",
      "tensor([-0.5394], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([-0.8174], grad_fn=<SliceBackward0>) tensor([0.3925])\n",
      "tensor([-0.2908], grad_fn=<SliceBackward0>) tensor([3.0265])\n",
      "tensor([-0.1796], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2381], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.0704], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.1653], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.2585], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.2643], grad_fn=<SliceBackward0>) tensor([0.4357])\n",
      "tensor([-0.6795], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([0.0168], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1621], grad_fn=<SliceBackward0>) tensor([-0.7302])\n",
      "tensor([-0.0768], grad_fn=<SliceBackward0>) tensor([1.0402])\n",
      "tensor([0.0134], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([0.0290], grad_fn=<SliceBackward0>) tensor([-1.5506])\n",
      "tensor([-0.1250], grad_fn=<SliceBackward0>) tensor([-0.6870])\n",
      "tensor([0.1114], grad_fn=<SliceBackward0>) tensor([-1.2051])\n",
      "tensor([0.2114], grad_fn=<SliceBackward0>) tensor([0.7379])\n",
      "tensor([-0.2098], grad_fn=<SliceBackward0>) tensor([-0.2120])\n",
      "tensor([-0.0861], grad_fn=<SliceBackward0>) tensor([-1.3347])\n",
      "tensor([-0.0759], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1673], grad_fn=<SliceBackward0>) tensor([-0.3415])\n",
      "tensor([0.0702], grad_fn=<SliceBackward0>) tensor([0.1334])\n",
      "tensor([-0.0499], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "current 27 training loss=0.6290488839149475 test loss = 0.8133100867271423\n",
      "early stop\n",
      "refitting with {best_n_epoches}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([512, 1])) that is different to the input size (torch.Size([512, 72, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (72) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefitting with \u001b[39m\u001b[38;5;132;01m{best_n_epoches}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mretrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_n_epoches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39munique\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mretrain\u001b[0;34m(n_epoches, train_loader)\u001b[0m\n\u001b[1;32m     57\u001b[0m h0 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minitHidden(\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[1;32m     58\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(x, h0)\n\u001b[0;32m---> 59\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     62\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/nn/modules/loss.py:96\u001b[0m, in \u001b[0;36mL1Loss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/nn/functional.py:3080\u001b[0m, in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3078\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3080\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39ml1_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/functional.py:72\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (72) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 24:00:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 24:00:00']]\n",
    "\n",
    "all_ss = {}\n",
    "torch.manual_seed(123)\n",
    "for unique in uniques[0:1]:\n",
    "    print(f\"doing {unique}\")\n",
    "    \n",
    "    df, ss = getseries(unique)\n",
    "    print(ss.mean_, ss.scale_, df['congestion-median-normalized'].std())\n",
    "    all_ss[unique] = ss\n",
    "    \n",
    "    test_periods_with_lookback = []\n",
    "    for period in test_periods:\n",
    "        id1 = df.index.to_list().index(period[0])\n",
    "        test_periods_with_lookback.append([df.index[id1-look_back], period[1]])\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)\n",
    "    train_loaders, test_loaders = assemble(df)\n",
    "    best_n_epoches = train(200, train_loaders[0], test_loaders[0])\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)\n",
    "    print('refitting with {best_n_epoches}')\n",
    "    retrain(best_n_epoches, train_loaders[1])\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model_'+unique+'.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
