{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9c4143",
   "metadata": {},
   "source": [
    "## now when make evaluation, try includes future points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e298c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7764c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 72*3\n",
    "look_future = 36\n",
    "batch_size = 512\n",
    "# batch_size = 64\n",
    "linear_node = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c79162",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('train.csv', index_col='row_id')\n",
    "def preprocess(dat):\n",
    "    time_mapper = {}\n",
    "    ii = 0\n",
    "    for h in range(24):\n",
    "        for mm in ['00','20','40']:\n",
    "            hh = '{0:02d}'.format(h)\n",
    "            time_mapper[hh+':'+mm] = ii\n",
    "            ii += 1\n",
    "\n",
    "    dat['unique'] = dat['x'].astype(str) + dat['y'].astype(str) + dat['direction']\n",
    "    uniques = dat['unique'].unique()\n",
    "    dat['day'] = pd.to_datetime(dat['time']).dt.weekday\n",
    "    dat['time_stamp'] = dat['time'].apply(lambda x:time_mapper[x.split()[1][:5]])\n",
    "\n",
    "    tmp = dat.groupby(['unique','day','time_stamp']).agg({'congestion':np.median})\n",
    "    median_mapper = tmp.to_dict()['congestion']\n",
    "    dat['median'] = dat.apply(lambda x: \\\n",
    "                              median_mapper[x['unique'],x['day'],x['time_stamp']], axis=1)\n",
    "    dat['congestion-median'] = dat['congestion'] - dat['median']\n",
    "    \n",
    "    all_time = pd.DataFrame(pd.date_range('1991-04-01 00:00:00', '1991-09-30 11:40:00', freq='20Min'), columns=['time'])\n",
    "    all_time['time'] = all_time['time'].astype(str)\n",
    "    \n",
    "    return uniques, median_mapper, time_mapper, all_time\n",
    "uniques, median_mapper, time_mapper, all_time = preprocess(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb05e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getseries(unique):\n",
    "    df = dat.loc[dat['unique']==unique, ['time', 'congestion-median']]\n",
    "    df = pd.merge(all_time, df, left_on='time', right_on='time', how='outer')\n",
    "    df = df.set_index('time')\n",
    "    df['congestion-median'] = df['congestion-median'].fillna(0)\n",
    "    ss = StandardScaler()\n",
    "    df['congestion-median-normalized'] = ss.fit_transform(df['congestion-median'].values.reshape(-1,1)).reshape(-1)\n",
    "    return df, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b3d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df,_ = getseries('00EB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4046893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>congestion-median</th>\n",
       "      <th>congestion-median-normalized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1991-04-01 00:00:00</th>\n",
       "      <td>-8.0</td>\n",
       "      <td>-0.778084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01 00:20:00</th>\n",
       "      <td>23.0</td>\n",
       "      <td>2.153930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01 00:40:00</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.546051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01 01:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.021436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-04-01 01:20:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.021436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-09-30 10:20:00</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-09-30 10:40:00</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.546051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-09-30 11:00:00</th>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.967247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-09-30 11:20:00</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.735213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-09-30 11:40:00</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>-1.250990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13140 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     congestion-median  congestion-median-normalized\n",
       "time                                                                \n",
       "1991-04-01 00:00:00               -8.0                     -0.778084\n",
       "1991-04-01 00:20:00               23.0                      2.153930\n",
       "1991-04-01 00:40:00                6.0                      0.546051\n",
       "1991-04-01 01:00:00                0.0                     -0.021436\n",
       "1991-04-01 01:20:00                0.0                     -0.021436\n",
       "...                                ...                           ...\n",
       "1991-09-30 10:20:00               14.0                      1.302700\n",
       "1991-09-30 10:40:00                6.0                      0.546051\n",
       "1991-09-30 11:00:00              -10.0                     -0.967247\n",
       "1991-09-30 11:20:00                8.0                      0.735213\n",
       "1991-09-30 11:40:00              -13.0                     -1.250990\n",
       "\n",
       "[13140 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "651b08f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12060 12167\n"
     ]
    }
   ],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 23:40:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 23:40:00']]\n",
    "test_periods_with_lookback = []\n",
    "for period in test_periods[:-1]:\n",
    "    id1 = df.index.to_list().index(period[0])\n",
    "    id1 = id1 - look_back\n",
    "    id2 = df.index.to_list().index(period[1])\n",
    "    print(id1, id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a24378",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = []\n",
    "for unique in uniques:\n",
    "    df, ss = getseries(unique)\n",
    "    data_all.append(df['congestion-median-normalized'].values)\n",
    "data_all = np.array(data_all).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5bd696e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13140, 65)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a85de2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "data_all_pca = pca.fit_transform(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "946eded4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13aaccac0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12f9842d",
   "metadata": {},
   "source": [
    "0 1 2        look_back = 3\n",
    "      3 4    look_future = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70fc05ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=72, look_future=36):\n",
    "    ''' return shape (N,look_back), (N,look_future)'''\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-look_future+1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+look_back:i+look_back+look_future])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31452286",
   "metadata": {},
   "source": [
    "[12060 12131] inclusive on both ends train\n",
    "[12132 12167] inclusive on both end test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e9cc7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 72), (1, 36))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = create_dataset(data_all[12060:12168,0])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c82ed4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(dat,icol):\n",
    "    train_loaders, test_loaders = [], []\n",
    "    \n",
    "    # for train/test\n",
    "    train = dat[:id1,icol]\n",
    "#     test = dat[id1:id1+look_back+look_future,icol]\n",
    "    test = dat[id1:,icol]\n",
    "    print(train.shape, test.shape)\n",
    "\n",
    "    X, y = create_dataset(train, look_back=look_back, look_future=look_future)\n",
    "    train_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        train_dataset.append((torch.tensor(X[i].reshape(-1,1),dtype=torch.float32),\n",
    "                              torch.tensor(y[i].reshape(-1,1),dtype=torch.float32)))\n",
    "    train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True))\n",
    "\n",
    "    X, y = create_dataset(test, look_back=look_back, look_future=look_future)\n",
    "    test_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        test_dataset.append((torch.tensor(X[i].reshape(-1,1),dtype=torch.float32),\n",
    "                             torch.tensor(y[i].reshape(-1,1),dtype=torch.float32)))\n",
    "    test_loaders.append(DataLoader(test_dataset, batch_size=batch_size, drop_last=False, shuffle=True))\n",
    "    \n",
    "    train = dat[:,icol]\n",
    "    X, y = create_dataset(train, look_back=look_back, look_future=look_future)\n",
    "    train_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        train_dataset.append((torch.tensor(X[i].reshape(-1,1),dtype=torch.float32),\n",
    "                              torch.tensor(y[i].reshape(-1,1),dtype=torch.float32)))\n",
    "    train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True))\n",
    "        \n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dd4cb9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12060,) (1080,)\n"
     ]
    }
   ],
   "source": [
    "train_loaders, test_loaders = assemble(data_all, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f0dd280a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 72, 1]), torch.Size([1, 36, 1]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(test_loaders[0]))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7f20980",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_size, output_feature, num_layers=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_feature, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, dropout=0.2)\n",
    "        ''' \n",
    "            gru input is (L,N,H_in=H_hidden), hidden is (num_layers, h_hidden)\n",
    "            output is (L,N,H_hidden), hidden is (num_layers, h_hidden)\n",
    "        '''\n",
    "        self.linear_out = nn.Linear(hidden_size, output_feature)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        ''' \n",
    "            input is in the shape of (L,N,input_feature) \n",
    "            output is (1,N,out_feature), the last time step\n",
    "            hidden is (num_layers, h_hidden)\n",
    "        '''\n",
    "        output = F.relu(self.linear(input))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear_out(F.relu(output[[-1]]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros((self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0a6e3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [3, 4]]),\n",
       " tensor([[1, 3],\n",
       "         [2, 4]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, a.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "085f0e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((a,a), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0e5a47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        curr_loss = 0\n",
    "        \n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            \n",
    "            N = len(x)\n",
    "            x = x.transpose(0,1)\n",
    "            y = y.transpose(0,1)\n",
    "            \n",
    "            hidden = model.initHidden(N)\n",
    "            loss = 0\n",
    "            output, hidden = model.forward(x, hidden)\n",
    "            for idx in range(look_future):\n",
    "                loss += criterion(output, y[[idx]])\n",
    "                output, hidden = model.forward(output, hidden)\n",
    "            loss /= look_future\n",
    "\n",
    "            curr_loss += loss.item() * N\n",
    "            \n",
    "        curr_loss /= len(test_loader.dataset)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "495d13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "\n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        teacher = 0.05\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            \n",
    "            # turn into (L,N,H)\n",
    "            N = len(x)\n",
    "            x = x.transpose(0,1)\n",
    "            y = y.transpose(0,1)\n",
    "            \n",
    "            hidden = model.initHidden(N)\n",
    "            loss = 0\n",
    "            output, hidden = model.forward(x, hidden)\n",
    "            \n",
    "            if np.random.random() > teacher:\n",
    "                for idx in range(look_future):\n",
    "                    loss += criterion(output, y[[idx]])\n",
    "                    output, hidden = model.forward(output, hidden)\n",
    "            else:\n",
    "                for idx in range(look_future):\n",
    "                    loss += criterion(output, y[[idx]])\n",
    "                    output, hidden = model.forward(y[[idx]], hidden)\n",
    "                    \n",
    "            #print(output[-1,-1,:],y[-1])\n",
    "            loss /= look_future\n",
    "#             print(f\"{batch} {loss}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss += loss.item()*N\n",
    "\n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        test_loss = evaluate(test_loader)\n",
    "        if (epoch % 1 == 0):  print(f'current {epoch} training loss={curr_loss} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "\n",
    "        if epoch > best_n_epoches + 50:\n",
    "            print('early stop')\n",
    "            break\n",
    "    return best_n_epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "067b24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        teacher = 0.05\n",
    "        \n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            N = len(x)\n",
    "            x = x.transpose(0,1)\n",
    "            y = y.transpose(0,1)\n",
    "            \n",
    "            hidden = model.initHidden(N)\n",
    "            loss = 0\n",
    "            output, hidden = model.forward(x, hidden)\n",
    "            \n",
    "            if np.random.random() > teacher:\n",
    "                for idx in range(look_future):\n",
    "                    loss += criterion(output, y[[idx]])\n",
    "                    output, hidden = model.forward(output, hidden)\n",
    "            else:\n",
    "                for idx in range(look_future):\n",
    "                    loss += criterion(output, y[[idx]])\n",
    "                    output, hidden = model.forward(y[[idx]], hidden)\n",
    "                    \n",
    "            #print(output[-1,-1,:],y[-1])\n",
    "            loss /= look_future\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss += loss.item()*N\n",
    "\n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "    return curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e292613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.94010361,  2.94010361,  2.94010361, ..., -0.68698284,\n",
       "        0.52204597,  1.38563798])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a8f94a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45036474, 0.73587087, 0.43689814, ..., 1.89473286, 1.33331841,\n",
       "       0.86773921])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all_pca[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ac83997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12060,) (252,)\n",
      "0 1.4033322602406981\n",
      "1 1.3117928372096506\n",
      "2 1.301082363196499\n",
      "3 1.2736462611572255\n",
      "4 1.2480871277746233\n",
      "5 1.255521957794953\n",
      "6 1.2611059462330627\n",
      "7 1.3117865071852712\n",
      "8 1.2665795635456296\n",
      "9 1.242636215707997\n",
      "10 1.1436186196368154\n",
      "11 1.235162328337218\n",
      "12 1.2339836691849035\n",
      "13 1.2227175812766073\n",
      "14 1.223032568116648\n",
      "15 1.1493650094874677\n",
      "16 1.2314268619627158\n",
      "17 1.1670391963997828\n",
      "18 1.1799956392662443\n",
      "19 1.1880108127041609\n",
      "20 1.1295786481554133\n",
      "21 1.2126874125410634\n",
      "22 1.2096940163421694\n",
      "23 1.201860698356244\n",
      "24 1.2045902372749162\n",
      "25 1.1391960489769088\n",
      "26 1.174048693176652\n",
      "27 1.1667660131049868\n",
      "28 1.1450861421787464\n",
      "29 1.1135069704124432\n",
      "30 1.175145611259069\n",
      "31 1.0928127508844245\n",
      "32 1.107656738862871\n",
      "33 1.2284538908406128\n",
      "34 1.1609371935789263\n",
      "35 1.1560323983569665\n",
      "36 1.17965499617242\n",
      "37 1.1527094995958334\n",
      "38 1.1856134340819422\n",
      "39 1.1428223874488384\n",
      "40 1.1802066323092733\n",
      "41 1.2308635916594013\n",
      "42 1.125423322675674\n",
      "43 1.0416088277272624\n",
      "44 1.1646939814570232\n",
      "45 1.2854357263823653\n",
      "46 1.104436520789598\n",
      "47 1.2342993361912167\n",
      "48 1.1679695132968084\n",
      "49 1.1459144347809707\n",
      "50 1.3495062025548925\n",
      "51 1.3165481837357682\n",
      "52 1.3286735600849364\n",
      "53 1.2562838482184437\n",
      "54 1.3634098300542967\n",
      "55 1.2590979729268883\n",
      "56 1.3117328590982924\n",
      "57 1.305357725737975\n",
      "58 1.3011936900657888\n",
      "59 1.3165258266790743\n",
      "60 1.316988991806737\n",
      "61 1.3012903256948922\n",
      "62 1.300833384727622\n",
      "63 1.3588130545359742\n",
      "64 1.2763586301324692\n",
      "65 1.2899643744731073\n",
      "66 1.3180058533861287\n",
      "67 1.3417067557651554\n",
      "68 1.3165672858710897\n",
      "69 1.33983852568746\n",
      "70 1.3539049295903018\n",
      "71 1.3234459059296377\n",
      "72 1.3373154205435651\n",
      "73 1.2511908201452981\n",
      "74 1.2935456155529372\n",
      "75 1.3138542096553663\n",
      "76 1.326780388605404\n",
      "77 1.3308415332750256\n",
      "78 1.2988717582851879\n",
      "79 1.2977520989689297\n",
      "80 1.2828811638562603\n",
      "81 1.3046450688621114\n",
      "82 1.2798487572981305\n",
      "83 1.3106543719995656\n",
      "84 1.3116461878778247\n",
      "85 1.2795483837665536\n",
      "86 1.246424052836237\n",
      "87 1.2798113824350281\n",
      "88 1.3330969367829617\n",
      "89 1.297912222897535\n",
      "90 1.3179203981428125\n",
      "91 1.3470379772431784\n",
      "92 1.3150283849171713\n",
      "93 1.2640600660348162\n",
      "94 1.2940384095140514\n",
      "95 1.3132375601799908\n",
      "96 1.3073721045068403\n",
      "97 1.321627013981519\n",
      "98 1.3131725061897783\n",
      "99 1.344546147250475\n",
      "100 1.2418392039504218\n",
      "101 1.2917851535802933\n",
      "102 1.254379087120726\n",
      "103 1.3105291627345577\n",
      "104 1.345072186460583\n",
      "105 1.2605751815945785\n",
      "106 1.2604443540176598\n",
      "107 1.3169985601263816\n",
      "108 1.3116134784195201\n",
      "109 1.304231728371985\n",
      "110 1.270629767812735\n",
      "111 1.2677997704312989\n",
      "112 1.2730335020294177\n",
      "113 1.2937102240484022\n",
      "114 1.2801394109268054\n",
      "115 1.312215673316796\n",
      "116 1.271399650013268\n",
      "117 1.3352370652006507\n",
      "118 1.289468632734481\n",
      "119 1.3083099785702266\n",
      "120 1.2205796920129655\n",
      "121 1.262160223983408\n",
      "122 1.2644222371008265\n",
      "123 1.3136124356916146\n",
      "124 1.3004473889244532\n",
      "125 1.304331486105404\n",
      "126 1.3116065486943815\n",
      "127 1.271029052105268\n",
      "128 1.2849612083793625\n",
      "129 1.3274401196659\n",
      "130 1.2725612227060075\n",
      "131 1.324520318491391\n",
      "132 1.2798824301015534\n",
      "133 1.2889634744815535\n",
      "134 1.3013842762632044\n",
      "135 1.282034070882055\n",
      "136 1.3245889832642572\n",
      "137 1.310811851660397\n",
      "138 1.2515167281471993\n",
      "139 1.3212581793090332\n",
      "140 1.2863229984493743\n",
      "141 1.2752136258451487\n",
      "142 1.2686228898308685\n",
      "143 1.2687655973519245\n",
      "144 1.2612665576185378\n",
      "145 1.2744627230071892\n",
      "146 1.28451453213042\n",
      "147 1.2435126895373152\n",
      "148 1.3020950889837752\n",
      "149 1.2847234028564176\n",
      "150 1.2596859686541633\n",
      "151 1.2733634537186365\n",
      "152 1.2495706197278487\n",
      "153 1.252299126142046\n",
      "154 1.2732549345415587\n",
      "155 1.277463560282442\n",
      "156 1.2093548810757613\n",
      "157 1.2708757367543135\n",
      "158 1.2448733742488047\n",
      "159 1.2966267116912304\n",
      "160 1.2783393802121124\n",
      "161 1.2682121831099422\n",
      "162 1.2922608207027013\n",
      "163 1.2626382293502507\n",
      "164 1.2203133512445747\n",
      "165 1.257538797581001\n",
      "166 1.2747996161314947\n",
      "167 1.2215135784554576\n",
      "168 1.243989078911145\n",
      "169 1.2399764537044076\n",
      "170 1.2690554801631775\n",
      "171 1.2877207514756983\n",
      "172 1.299861043078377\n",
      "173 1.271650778569924\n",
      "174 1.2782536815331098\n",
      "175 1.2594865580620234\n",
      "176 1.2764117156472998\n",
      "177 1.2815030556607785\n",
      "178 1.253634088993759\n",
      "179 1.2081757924106302\n",
      "180 1.2293435871697005\n",
      "181 1.2395270465643604\n",
      "182 1.2271964250405158\n",
      "183 1.2467223653866826\n",
      "184 1.2493440275905356\n",
      "185 1.2336724463562243\n",
      "186 1.2580407500741484\n",
      "187 1.2483978672449145\n",
      "188 1.2375384107230831\n",
      "189 1.2399328738105257\n",
      "190 1.2493717566969151\n",
      "191 1.2352309733848787\n",
      "192 1.2374904418599344\n",
      "193 1.2237102430370925\n",
      "194 1.292989425770626\n",
      "195 1.250252718979584\n",
      "196 1.2294721916129925\n",
      "197 1.2166267585689787\n",
      "198 1.2132022873555204\n",
      "199 1.2265841917398468\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'best_n_epoches' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [96]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_loaders, test_loaders \u001b[38;5;241m=\u001b[39m assemble(data_all_pca, icol)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)    \n\u001b[0;32m----> 5\u001b[0m best_n_epoches \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_epoches\u001b[39m\u001b[38;5;124m'\u001b[39m: best_n_epoches,\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict()},\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_pca_withforward_train_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(icol)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)    \n",
      "Input \u001b[0;32mIn [91]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epoches, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly stop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbest_n_epoches\u001b[49m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'best_n_epoches' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for icol in range(1):\n",
    "    train_loaders, test_loaders = assemble(data_all_pca, icol)\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)    \n",
    "    best_n_epoches = train(500, train_loaders[0], test_loaders[0])\n",
    "    torch.save({'best_epoches': best_n_epoches,\n",
    "                'model': model.state_dict()},\n",
    "                'model_pca_withforward_train_'+str(icol)+'.pickle')\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)    \n",
    "    loss = retrain(best_n_epoches, train_loaders[1])\n",
    "    torch.save({'loss': loss,\n",
    "                'best_epoches': best_n_epoches,\n",
    "                'model': model.state_dict()},\n",
    "                'model_pca_withforward_all_'+str(icol)+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c993fc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12060,) (1080,)\n",
      "current 0 training loss=1.4073950786253007 test loss = 1.8003478050231934\n",
      "updating best loss 0 training loss=1.4745192527770996 test loss = 1.8003478050231934\n",
      "current 1 training loss=1.3763398504325848 test loss = 1.8493226766586304\n",
      "current 2 training loss=1.361021642277511 test loss = 1.839813470840454\n",
      "current 3 training loss=1.3227369691023567 test loss = 1.8499170541763306\n",
      "current 4 training loss=1.314649047844698 test loss = 1.774214267730713\n",
      "updating best loss 4 training loss=1.5293854475021362 test loss = 1.774214267730713\n",
      "current 5 training loss=1.3170794776437122 test loss = 1.778043270111084\n",
      "current 6 training loss=1.314476665760825 test loss = 1.792319416999817\n",
      "current 7 training loss=1.3507308773005562 test loss = 1.7048888206481934\n",
      "updating best loss 7 training loss=1.0121159553527832 test loss = 1.7048888206481934\n",
      "current 8 training loss=1.3066719456382827 test loss = 1.8311917781829834\n",
      "current 9 training loss=1.36187530879254 test loss = 1.738755702972412\n",
      "current 10 training loss=1.3481949517472307 test loss = 1.839376449584961\n",
      "current 11 training loss=1.3571551433414315 test loss = 1.8026989698410034\n",
      "current 12 training loss=1.3414964570715684 test loss = 1.632348656654358\n",
      "updating best loss 12 training loss=1.2756450176239014 test loss = 1.632348656654358\n",
      "current 13 training loss=1.3248718273065285 test loss = 1.7516324520111084\n",
      "current 14 training loss=1.339938037461497 test loss = 1.7265448570251465\n",
      "current 15 training loss=1.3365294046391478 test loss = 1.8278971910476685\n",
      "current 16 training loss=1.2770977773940153 test loss = 1.7354016304016113\n",
      "current 17 training loss=1.2767575483276112 test loss = 1.7293564081192017\n",
      "current 18 training loss=1.333965660423578 test loss = 1.7986397743225098\n",
      "current 19 training loss=1.3312778722544367 test loss = 1.8082430362701416\n",
      "current 20 training loss=1.316142001102776 test loss = 1.6783592700958252\n",
      "current 21 training loss=1.3445819467015911 test loss = 1.6664371490478516\n",
      "current 22 training loss=1.315077517708772 test loss = 1.7816625833511353\n",
      "current 23 training loss=1.3352621540670744 test loss = 1.6970349550247192\n",
      "current 24 training loss=1.298429420279229 test loss = 1.7761293649673462\n",
      "current 25 training loss=1.2808917124574841 test loss = 1.6991286277770996\n",
      "current 26 training loss=1.3260096622993263 test loss = 1.6644203662872314\n",
      "current 27 training loss=1.3234617301476719 test loss = 1.739249348640442\n",
      "current 28 training loss=1.2999371854000525 test loss = 1.7652150392532349\n",
      "current 29 training loss=1.2907699353608706 test loss = 1.7608642578125\n",
      "current 30 training loss=1.310192458372595 test loss = 1.7841440439224243\n",
      "current 31 training loss=1.2805899379719565 test loss = 1.7399955987930298\n",
      "current 32 training loss=1.2362978866839451 test loss = 1.843536376953125\n",
      "current 33 training loss=1.234262630511263 test loss = 1.7117432355880737\n",
      "current 34 training loss=1.2897800784862479 test loss = 1.742699146270752\n",
      "current 35 training loss=1.2424638936981762 test loss = 1.7436165809631348\n",
      "current 36 training loss=1.2867179773170625 test loss = 1.7853412628173828\n",
      "current 37 training loss=1.2699525098371025 test loss = 1.7367534637451172\n",
      "current 38 training loss=1.3047785423724432 test loss = 1.6920826435089111\n",
      "current 39 training loss=1.3128041390167768 test loss = 1.7195160388946533\n",
      "current 40 training loss=1.2710786446031803 test loss = 1.672519326210022\n",
      "current 41 training loss=1.2956094440996764 test loss = 1.7739540338516235\n",
      "current 42 training loss=1.2775356395550066 test loss = 1.772712230682373\n",
      "current 43 training loss=1.2356410918525298 test loss = 1.673882007598877\n",
      "current 44 training loss=1.2700813921998584 test loss = 1.7602181434631348\n",
      "current 45 training loss=1.2845381137246412 test loss = 1.8177781105041504\n",
      "current 46 training loss=1.269038827779261 test loss = 1.8033193349838257\n",
      "current 47 training loss=1.2256317377211592 test loss = 1.7864973545074463\n",
      "current 48 training loss=1.2545212261742809 test loss = 1.7571635246276855\n",
      "current 49 training loss=1.2383953172091111 test loss = 1.7962654829025269\n",
      "current 50 training loss=1.278898738989005 test loss = 1.757291316986084\n",
      "current 51 training loss=1.251300871488313 test loss = 1.7858061790466309\n",
      "current 52 training loss=1.261181618743549 test loss = 1.724197506904602\n",
      "current 53 training loss=1.2198579640662663 test loss = 1.8476691246032715\n",
      "current 54 training loss=1.2561814133284488 test loss = 1.7990169525146484\n",
      "current 55 training loss=1.281887634237207 test loss = 1.7966641187667847\n",
      "current 56 training loss=1.2553567881507044 test loss = 1.759488582611084\n",
      "current 57 training loss=1.260766454025231 test loss = 1.84656822681427\n",
      "current 58 training loss=1.225531713838793 test loss = 1.7841922044754028\n",
      "current 59 training loss=1.247211836448157 test loss = 1.8854459524154663\n",
      "current 60 training loss=1.2575271847751 test loss = 1.804396152496338\n",
      "current 61 training loss=1.212666209842967 test loss = 1.8048136234283447\n",
      "current 62 training loss=1.2232664500297161 test loss = 1.8390299081802368\n",
      "current 63 training loss=1.2474849995634822 test loss = 1.7566407918930054\n",
      "current 64 training loss=1.2325020643806506 test loss = 1.803670883178711\n",
      "current 65 training loss=1.2194037772383857 test loss = 1.865814208984375\n",
      "current 66 training loss=1.2341319753229492 test loss = 1.8604979515075684\n",
      "current 67 training loss=1.2189444022675717 test loss = 1.8754057884216309\n",
      "current 68 training loss=1.2110900215779972 test loss = 1.791283130645752\n",
      "current 69 training loss=1.2161803085744023 test loss = 1.7712537050247192\n",
      "current 70 training loss=1.206753338026308 test loss = 1.8529431819915771\n",
      "current 71 training loss=1.2213747048521337 test loss = 1.8848868608474731\n",
      "current 72 training loss=1.2222676587070476 test loss = 1.9070173501968384\n",
      "current 73 training loss=1.2132601459561967 test loss = 1.9335986375808716\n",
      "current 74 training loss=1.1801357340758172 test loss = 1.8608981370925903\n",
      "current 75 training loss=1.170802893328903 test loss = 1.863951563835144\n",
      "current 76 training loss=1.2167763310874262 test loss = 1.8251620531082153\n",
      "current 77 training loss=1.1842912226620088 test loss = 1.9021714925765991\n",
      "current 78 training loss=1.2019645970906863 test loss = 1.8348002433776855\n",
      "current 79 training loss=1.1767823759430762 test loss = 1.9472918510437012\n",
      "current 80 training loss=1.1505728159319064 test loss = 1.9710949659347534\n",
      "current 81 training loss=1.190560753677204 test loss = 2.0051510334014893\n",
      "current 82 training loss=1.2180299328873387 test loss = 2.0169880390167236\n",
      "current 83 training loss=1.1499828798561538 test loss = 2.0008795261383057\n",
      "current 84 training loss=1.1608976982865167 test loss = 1.9052246809005737\n",
      "current 85 training loss=1.1788022861582468 test loss = 1.9262574911117554\n",
      "current 86 training loss=1.1643264095611017 test loss = 1.9483634233474731\n",
      "current 87 training loss=1.1534319165619293 test loss = 1.9044537544250488\n",
      "current 88 training loss=1.1603214943153568 test loss = 2.0193090438842773\n",
      "current 89 training loss=1.1695177752437231 test loss = 1.9545400142669678\n",
      "current 90 training loss=1.1516847240600792 test loss = 1.94729483127594\n",
      "current 91 training loss=1.153143210640829 test loss = 1.8305710554122925\n",
      "current 92 training loss=1.150801619578704 test loss = 1.9674791097640991\n",
      "current 93 training loss=1.137238388491965 test loss = 1.9650893211364746\n",
      "current 94 training loss=1.127057047729741 test loss = 1.9056353569030762\n",
      "current 95 training loss=1.1672054650024613 test loss = 2.0381367206573486\n",
      "current 96 training loss=1.1453688915319553 test loss = 1.9654181003570557\n",
      "current 97 training loss=1.143496274534234 test loss = 1.9627516269683838\n",
      "current 98 training loss=1.1525057964597913 test loss = 1.9424421787261963\n",
      "current 99 training loss=1.134611693604833 test loss = 1.8962366580963135\n",
      "current 100 training loss=1.1250009043310183 test loss = 1.9499062299728394\n",
      "current 101 training loss=1.122280413169281 test loss = 1.933249831199646\n",
      "current 102 training loss=1.1122912018090545 test loss = 1.9955124855041504\n",
      "current 103 training loss=1.118441387966814 test loss = 2.0723140239715576\n",
      "current 104 training loss=1.1685217810238393 test loss = 1.8704427480697632\n",
      "current 105 training loss=1.1377633345077642 test loss = 1.9117439985275269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current 106 training loss=1.1290097359890598 test loss = 1.8473987579345703\n",
      "current 107 training loss=1.1069704207490527 test loss = 1.879019856452942\n",
      "current 108 training loss=1.1135867046193393 test loss = 2.0409231185913086\n",
      "current 109 training loss=1.1035201097889449 test loss = 2.0579771995544434\n",
      "current 110 training loss=1.101445940096278 test loss = 1.9454100131988525\n",
      "current 111 training loss=1.0995785000261864 test loss = 2.076508045196533\n",
      "current 112 training loss=1.1132578638710573 test loss = 2.076627254486084\n",
      "current 113 training loss=1.0875485788727 test loss = 1.9711384773254395\n",
      "current 114 training loss=1.068948440441119 test loss = 2.1081786155700684\n",
      "current 115 training loss=1.0572292512070265 test loss = 2.0249013900756836\n",
      "current 116 training loss=1.083929096999303 test loss = 1.9714311361312866\n",
      "current 117 training loss=1.0679507290280252 test loss = 2.0030570030212402\n",
      "current 118 training loss=1.0882913227034459 test loss = 1.9410661458969116\n",
      "current 119 training loss=1.094736247707134 test loss = 1.8370816707611084\n",
      "current 120 training loss=1.059146440345111 test loss = 2.042206048965454\n",
      "current 121 training loss=1.076669122143294 test loss = 1.9165104627609253\n",
      "current 122 training loss=1.0704667473113934 test loss = 2.067305564880371\n",
      "current 123 training loss=1.0877731560913753 test loss = 1.9103741645812988\n",
      "current 124 training loss=1.0952763719606766 test loss = 1.8832656145095825\n",
      "current 125 training loss=1.0638841672153232 test loss = 1.9378443956375122\n",
      "current 126 training loss=1.0796645766885813 test loss = 1.9953670501708984\n",
      "current 127 training loss=1.077706212193488 test loss = 1.955962896347046\n",
      "current 128 training loss=1.069987570394538 test loss = 1.9187687635421753\n",
      "current 129 training loss=1.0732201600026312 test loss = 1.96611487865448\n",
      "current 130 training loss=1.0754863112894866 test loss = 1.9444631338119507\n",
      "current 131 training loss=1.0600826878563416 test loss = 1.9403940439224243\n",
      "current 132 training loss=1.0530797559993517 test loss = 2.0276896953582764\n",
      "current 133 training loss=1.0455843895757033 test loss = 1.931044101715088\n",
      "current 134 training loss=1.0579915677615817 test loss = 1.9742251634597778\n",
      "current 135 training loss=1.0478307915254776 test loss = 1.924304723739624\n",
      "current 136 training loss=1.075700772586932 test loss = 1.9779232740402222\n",
      "current 137 training loss=1.0589289473421295 test loss = 1.9005303382873535\n",
      "current 138 training loss=1.0497412280160554 test loss = 2.008904457092285\n",
      "current 139 training loss=1.06469558235026 test loss = 2.0011746883392334\n",
      "current 140 training loss=1.4587853877729542 test loss = 1.9666190147399902\n",
      "current 141 training loss=1.398747426264877 test loss = 1.9076733589172363\n",
      "current 142 training loss=1.383619887437755 test loss = 1.8853929042816162\n",
      "current 143 training loss=1.3521087519532387 test loss = 1.8134454488754272\n",
      "current 144 training loss=1.3475372706595057 test loss = 1.923658847808838\n",
      "current 145 training loss=1.336601838339535 test loss = 1.8191723823547363\n",
      "current 146 training loss=1.3092003737207452 test loss = 1.8389317989349365\n",
      "current 147 training loss=1.2989364488935056 test loss = 1.8303923606872559\n",
      "current 148 training loss=1.2657964250378992 test loss = 1.7720168828964233\n",
      "current 149 training loss=1.314205023464901 test loss = 1.8128899335861206\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [125]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_loaders, test_loaders \u001b[38;5;241m=\u001b[39m assemble(data_all_pca, icol)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)    \n\u001b[0;32m----> 5\u001b[0m best_n_epoches \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_epoches\u001b[39m\u001b[38;5;124m'\u001b[39m: best_n_epoches,\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict()},\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_pca_withforward_train_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(icol)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)    \n",
      "Input \u001b[0;32mIn [124]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epoches, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#             print(f\"{batch} {loss}\")\u001b[39;00m\n\u001b[1;32m     36\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m             \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m             optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m             curr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mN\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for icol in range(1):\n",
    "    train_loaders, test_loaders = assemble(data_all_pca, icol)\n",
    "    \n",
    "#     model = MyModel(1, linear_node, 1, num_layers=3)    \n",
    "#     best_n_epoches = train(2000, train_loaders[0], test_loaders[0])\n",
    "#     torch.save({'best_epoches': best_n_epoches,\n",
    "#                 'model': model.state_dict()},\n",
    "#                 'model_pca_withforward_train_'+str(icol)+'.pickle')\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)    \n",
    "    loss = retrain(best_n_epoches, train_loaders[1])\n",
    "    torch.save({'loss': loss,\n",
    "                'best_epoches': best_n_epoches,\n",
    "                'model': model.state_dict()},\n",
    "                'model_pca_withforward_all_'+str(icol)+'.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
