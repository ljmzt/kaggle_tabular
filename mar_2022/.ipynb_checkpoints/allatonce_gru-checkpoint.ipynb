{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e298c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7764c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 72*3\n",
    "n_features = 65\n",
    "linear_node = 32\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c79162",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('train.csv', index_col='row_id')\n",
    "def preprocess(dat):\n",
    "    time_mapper = {}\n",
    "    ii = 0\n",
    "    for h in range(24):\n",
    "        for mm in ['00','20','40']:\n",
    "            hh = '{0:02d}'.format(h)\n",
    "            time_mapper[hh+':'+mm] = ii\n",
    "            ii += 1\n",
    "\n",
    "    dat['unique'] = dat['x'].astype(str) + dat['y'].astype(str) + dat['direction']\n",
    "    uniques = dat['unique'].unique()\n",
    "    dat['day'] = pd.to_datetime(dat['time']).dt.weekday\n",
    "    dat['time_stamp'] = dat['time'].apply(lambda x:time_mapper[x.split()[1][:5]])\n",
    "\n",
    "    tmp = dat.groupby(['unique','day','time_stamp']).agg({'congestion':np.median})\n",
    "    median_mapper = tmp.to_dict()['congestion']\n",
    "    dat['median'] = dat.apply(lambda x: \\\n",
    "                              median_mapper[x['unique'],x['day'],x['time_stamp']], axis=1)\n",
    "    dat['congestion-median'] = dat['congestion'] - dat['median']\n",
    "    \n",
    "    all_time = pd.DataFrame(pd.date_range('1991-04-01 00:00:00', '1991-09-30 11:40:00', freq='20Min'), columns=['time'])\n",
    "    all_time['time'] = all_time['time'].astype(str)\n",
    "    \n",
    "    return uniques, median_mapper, time_mapper, all_time\n",
    "uniques, median_mapper, time_mapper, all_time = preprocess(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb05e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getseries(unique):\n",
    "    df = dat.loc[dat['unique']==unique, ['time', 'congestion-median']]\n",
    "    df = pd.merge(all_time, df, left_on='time', right_on='time', how='outer')\n",
    "    df = df.set_index('time')\n",
    "    df['congestion-median'] = df['congestion-median'].fillna(0)\n",
    "    ss = StandardScaler()\n",
    "    df['congestion-median-normalized'] = ss.fit_transform(df['congestion-median'].values.reshape(-1,1)).reshape(-1)\n",
    "    return df, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b3d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df,_ = getseries('00EB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651b08f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12636, 13140)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 24:00:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 24:00:00']]\n",
    "id1 = df.index.to_list().index(test_periods[1][0])\n",
    "id1, len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a24378",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = []\n",
    "for unique in uniques:\n",
    "    df, ss = getseries(unique)\n",
    "    data_all.append(df['congestion-median-normalized'].values)\n",
    "data_all = np.array(data_all).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5bd696e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13140, 65)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88baa81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=5):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back),:]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+1:i+look_back+1,:])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4289f2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 5, 65), (4, 5, 65))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = create_dataset(data_all[:10])\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c82ed4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(dat):\n",
    "    train_loaders, test_loaders = [], []\n",
    "    \n",
    "    # for train/test\n",
    "    train = dat[:12636]\n",
    "    test = dat[12636:]\n",
    "\n",
    "    X, y = create_dataset(train, look_back=look_back)\n",
    "    train_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        train_dataset.append((torch.tensor(X[i],dtype=torch.float32),\n",
    "                              torch.tensor(y[i],dtype=torch.float32)))\n",
    "    train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False))\n",
    "\n",
    "    X, y = create_dataset(test, look_back=look_back)\n",
    "    test_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        test_dataset.append((torch.tensor(X[i],dtype=torch.float32),\n",
    "                             torch.tensor(y[i],dtype=torch.float32)))\n",
    "    test_loaders.append(DataLoader(test_dataset, batch_size=batch_size, drop_last=False))\n",
    "    \n",
    "    train = dat[:]\n",
    "    X, y = create_dataset(train, look_back=look_back)\n",
    "    train_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        train_dataset.append((torch.tensor(X[i],dtype=torch.float32),\n",
    "                              torch.tensor(y[i],dtype=torch.float32)))\n",
    "    train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1101b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders, test_loaders = assemble(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84cb3f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 72, 65]), torch.Size([32, 65]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = next(iter(train_loaders[0]))\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1151b815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 72, 65]), torch.Size([32, 65]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = next(iter(test_loaders[0]))\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7f20980",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_size, output_feature, num_layers=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_feature, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers, dropout=0.2)\n",
    "        ''' gru input is (N,L,H_in=H_hidden), output is (N,L,H_hidden), hidden is (num_layers, h_hidden)'''\n",
    "        self.linear_out = nn.Linear(hidden_size, output_feature)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        ''' X is in the shape of (N,L,input_feature) '''\n",
    "        output = F.relu(self.linear(input))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear_out(F.relu(output))\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros((self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "495d13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "#             loss += criterion(output[:,-1,:],y).item() * len(x)\n",
    "            loss += criterion(output,y).item() * len(x)\n",
    "            n += len(x)\n",
    "        loss /= n\n",
    "    return loss\n",
    "\n",
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "\n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            #print(output[-1,-1,:],y[-1])\n",
    "#             loss = criterion(output[:,-1,:], y)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss += loss.item()*len(x)\n",
    "#             print(f\"{batch} {loss.item()}\")\n",
    "\n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        test_loss = evaluate(test_loader)\n",
    "#         if (epoch % 20 == 0):  print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "\n",
    "        if epoch > best_n_epoches + 50:\n",
    "            print('early stop')\n",
    "            break\n",
    "    return best_n_epoches\n",
    "\n",
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "\n",
    "        curr_loss = 0.0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "#             loss = criterion(output[:,-1,:], y)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss += loss.item()*len(x)\n",
    "\n",
    "    curr_loss /= len(train_loader.dataset)\n",
    "    return curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ac83997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current 0 training loss=0.7243251800537109 test loss = 0.6830596132787363\n",
      "updating best loss 0 training loss=0.7243251800537109 test loss = 0.6830596132787363\n",
      "current 1 training loss=0.720717191696167 test loss = 0.6782243807741771\n",
      "updating best loss 1 training loss=0.720717191696167 test loss = 0.6782243807741771\n",
      "current 2 training loss=0.7203051447868347 test loss = 0.6775425138716908\n",
      "updating best loss 2 training loss=0.7203051447868347 test loss = 0.6775425138716908\n",
      "current 3 training loss=0.7191995978355408 test loss = 0.6772174434152944\n",
      "updating best loss 3 training loss=0.7191995978355408 test loss = 0.6772174434152944\n",
      "current 4 training loss=0.7187342047691345 test loss = 0.6769855314230421\n",
      "updating best loss 4 training loss=0.7187342047691345 test loss = 0.6769855314230421\n",
      "current 5 training loss=0.7179566025733948 test loss = 0.6768030849244367\n",
      "updating best loss 5 training loss=0.7179566025733948 test loss = 0.6768030849244367\n",
      "current 6 training loss=0.7167661786079407 test loss = 0.676826351479975\n",
      "current 7 training loss=0.7161142230033875 test loss = 0.6768225760581599\n",
      "current 8 training loss=0.7154437303543091 test loss = 0.676933804133652\n",
      "current 9 training loss=0.7151679396629333 test loss = 0.6768073518425572\n",
      "current 10 training loss=0.7145895957946777 test loss = 0.6767866698605832\n",
      "updating best loss 10 training loss=0.7145895957946777 test loss = 0.6767866698605832\n",
      "current 11 training loss=0.7141861915588379 test loss = 0.676876726255616\n",
      "current 12 training loss=0.7140217423439026 test loss = 0.6768443923262045\n",
      "current 13 training loss=0.713566780090332 test loss = 0.6768066358953507\n",
      "current 14 training loss=0.7135668992996216 test loss = 0.6767018866096461\n",
      "updating best loss 14 training loss=0.7135668992996216 test loss = 0.6767018866096461\n",
      "current 15 training loss=0.7135617136955261 test loss = 0.6768221503897223\n",
      "current 16 training loss=0.712986409664154 test loss = 0.6768807764672901\n",
      "current 17 training loss=0.7136054039001465 test loss = 0.676823226039205\n",
      "current 18 training loss=0.7130746245384216 test loss = 0.6768725301439413\n",
      "current 19 training loss=0.7133011817932129 test loss = 0.6768562580759176\n",
      "current 20 training loss=0.7127634286880493 test loss = 0.6768633029023898\n",
      "current 21 training loss=0.7126941084861755 test loss = 0.6770189217790926\n",
      "current 22 training loss=0.7126232385635376 test loss = 0.677062228909778\n",
      "current 23 training loss=0.7134144306182861 test loss = 0.6770735784085886\n",
      "current 24 training loss=0.7121991515159607 test loss = 0.6776337907098562\n",
      "current 25 training loss=0.7138153314590454 test loss = 0.6773765857424371\n",
      "current 26 training loss=0.7126598358154297 test loss = 0.6778661774648591\n",
      "current 27 training loss=0.7125642895698547 test loss = 0.6774050600290852\n",
      "current 28 training loss=0.7124329805374146 test loss = 0.6779389557041867\n",
      "current 29 training loss=0.7121497988700867 test loss = 0.6776460505970117\n",
      "current 30 training loss=0.7114534378051758 test loss = 0.677985633747207\n",
      "current 31 training loss=0.7121458649635315 test loss = 0.6778309917781856\n",
      "current 32 training loss=0.7118373513221741 test loss = 0.6782726081230801\n",
      "current 33 training loss=0.7120687961578369 test loss = 0.6778560511750578\n",
      "current 34 training loss=0.7113605737686157 test loss = 0.6782558261656706\n",
      "current 35 training loss=0.7117902040481567 test loss = 0.678132626683972\n",
      "current 36 training loss=0.7109593152999878 test loss = 0.6781935070894157\n",
      "current 37 training loss=0.7119755744934082 test loss = 0.6781200235515072\n",
      "current 38 training loss=0.7104530334472656 test loss = 0.6781428659598402\n",
      "current 39 training loss=0.7120023369789124 test loss = 0.678192873980219\n",
      "current 40 training loss=0.710374116897583 test loss = 0.6782157358879832\n",
      "current 41 training loss=0.7117560505867004 test loss = 0.6781863805312844\n",
      "current 42 training loss=0.7098773717880249 test loss = 0.6784383145673093\n",
      "current 43 training loss=0.7118720412254333 test loss = 0.6781557044684196\n",
      "current 44 training loss=0.7102819085121155 test loss = 0.6785066863500325\n",
      "current 45 training loss=0.7125228643417358 test loss = 0.6783402902344263\n",
      "current 46 training loss=0.7098629474639893 test loss = 0.6783813304524964\n",
      "current 47 training loss=0.711158812046051 test loss = 0.6782122925096764\n",
      "current 48 training loss=0.7107102870941162 test loss = 0.6781483665973017\n",
      "current 49 training loss=0.7102430462837219 test loss = 0.6781422227551932\n",
      "current 50 training loss=0.7104186415672302 test loss = 0.6787054921800741\n",
      "current 51 training loss=0.710783064365387 test loss = 0.678315170818028\n",
      "current 52 training loss=0.7105062007904053 test loss = 0.6788858031452117\n",
      "current 53 training loss=0.7108005881309509 test loss = 0.6785785544776032\n",
      "current 54 training loss=0.7102832198143005 test loss = 0.6790376105054072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_loaders, test_loaders \u001b[38;5;241m=\u001b[39m assemble(data_all)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(n_features, linear_node, n_features, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)    \n\u001b[0;32m----> 4\u001b[0m best_n_epoches \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_epoches\u001b[39m\u001b[38;5;124m'\u001b[39m: best_n_epoches,\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict()},\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_allgru_train.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(n_features, linear_node, n_features, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)    \n",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epoches, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y)\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m curr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loaders, test_loaders = assemble(data_all)\n",
    "\n",
    "model = MyModel(n_features, linear_node, n_features, num_layers=3)    \n",
    "best_n_epoches = train(300, train_loaders[0], test_loaders[0])\n",
    "torch.save({'best_epoches': best_n_epoches,\n",
    "            'model': model.state_dict()},\n",
    "            'model_allgru_train.pickle')\n",
    "\n",
    "model = MyModel(n_features, linear_node, n_features, num_layers=3)    \n",
    "loss = retrain(best_n_epoches, train_loaders[1])\n",
    "torch.save({'loss': loss,\n",
    "            'best_epoches': best_n_epoches,\n",
    "            'model': model.state_dict()},\n",
    "            'model_allgru.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09b07baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (linear): Linear(in_features=65, out_features=16, bias=True)\n",
       "  (gru): GRU(16, 16, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (linear_out): Linear(in_features=16, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
