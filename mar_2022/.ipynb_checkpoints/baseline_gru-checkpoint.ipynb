{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf89012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d73822d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look_back = 504\n",
    "look_back = 72 * 3\n",
    "batch_size = 512\n",
    "linear_node = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4224d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('train.csv', index_col='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "693ffb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dat):\n",
    "    time_mapper = {}\n",
    "    ii = 0\n",
    "    for h in range(24):\n",
    "        for mm in ['00','20','40']:\n",
    "            hh = '{0:02d}'.format(h)\n",
    "            time_mapper[hh+':'+mm] = ii\n",
    "            ii += 1\n",
    "\n",
    "    dat['unique'] = dat['x'].astype(str) + dat['y'].astype(str) + dat['direction']\n",
    "    uniques = dat['unique'].unique()\n",
    "    dat['day'] = pd.to_datetime(dat['time']).dt.weekday\n",
    "    dat['time_stamp'] = dat['time'].apply(lambda x:time_mapper[x.split()[1][:5]])\n",
    "\n",
    "    tmp = dat.groupby(['unique','day','time_stamp']).agg({'congestion':np.median})\n",
    "    median_mapper = tmp.to_dict()['congestion']\n",
    "    dat['median'] = dat.apply(lambda x: \\\n",
    "                              median_mapper[x['unique'],x['day'],x['time_stamp']], axis=1)\n",
    "    dat['congestion-median'] = dat['congestion'] - dat['median']\n",
    "    \n",
    "    all_time = pd.DataFrame(pd.date_range('1991-04-01 00:00:00', '1991-09-30 11:40:00', freq='20Min'), columns=['time'])\n",
    "    all_time['time'] = all_time['time'].astype(str)\n",
    "    \n",
    "    return uniques, median_mapper, time_mapper, all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed87c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, median_mapper, time_mapper, all_time = preprocess(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6881fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getseries(unique):\n",
    "    df = dat.loc[dat['unique']==unique, ['time', 'congestion-median']]\n",
    "    df = pd.merge(all_time, df, left_on='time', right_on='time', how='outer')\n",
    "    df = df.set_index('time')\n",
    "    df['congestion-median'] = df['congestion-median'].fillna(0)\n",
    "    ss = StandardScaler()\n",
    "    df['congestion-median-normalized'] = ss.fit_transform(df['congestion-median'].values.reshape(-1,1)).reshape(-1)\n",
    "    return df, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43d11fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=5):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+1:i+look_back+1])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f46910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(dat):\n",
    "    train_loaders, test_loaders = [], []\n",
    "    for period in test_periods_with_lookback:\n",
    "        train = dat.loc[dat.index < period[0], 'congestion-median-normalized'].values\n",
    "        test = dat.loc[(dat.index >= period[0]) & (dat.index <= period[1]), 'congestion-median-normalized'].values\n",
    "        print(test[0])\n",
    "        \n",
    "        X, y = create_dataset(train, look_back=look_back)\n",
    "        train_dataset = []\n",
    "        for i in range(len(X)):\n",
    "            train_dataset.append((torch.tensor(X[i].reshape(-1,1),dtype=torch.float32),\n",
    "                                  torch.tensor(y[i].reshape(-1,1),dtype=torch.float32)))\n",
    "        train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "        X, y = create_dataset(test, look_back=look_back)\n",
    "        test_dataset = []\n",
    "        for i in range(len(X)):\n",
    "            test_dataset.append((torch.tensor(X[i].reshape(-1,1),dtype=torch.float32),\n",
    "                                 torch.tensor(y[i].reshape(-1,1),dtype=torch.float32)))\n",
    "        test_loaders.append(DataLoader(test_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d24f03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3729acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_size, output_feature, num_layers=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_feature, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers, dropout=0.2)\n",
    "        ''' gru input is (N,L,H_in=H_hidden), output is (N,L,H_hidden), hidden is (num_layers, h_hidden)'''\n",
    "        self.linear_out = nn.Linear(hidden_size, output_feature)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        ''' X is in the shape of (N,L,input_feature) '''\n",
    "        output = F.relu(self.linear(input))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear_out(F.relu(output))\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros((self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "565c2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss += criterion(output ,y).item() * len(x)\n",
    "            n += len(x)\n",
    "        loss /= n\n",
    "    return loss\n",
    "\n",
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += loss*len(x)\n",
    "            n += len(x)\n",
    "        \n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        test_loss = evaluate(test_loader)\n",
    "        if (epoch % 20 == 0):  print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "    return best_n_epoches\n",
    "\n",
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a1ac41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f861750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing 00EB\n",
      "[0.95494673] [11.57954203]\n",
      "2.0765115933378677\n",
      "0.5220459717995566\n",
      "current 0 training loss=0.8927696943283081 test loss = 0.6937204599380493\n",
      "updating best loss 0 training loss=0.8927696943283081 test loss = 0.6937204599380493\n",
      "updating best loss 1 training loss=0.8868663907051086 test loss = 0.6935110092163086\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     20\u001b[0m train_loaders, test_loaders \u001b[38;5;241m=\u001b[39m assemble(df)\n\u001b[0;32m---> 21\u001b[0m best_n_epoches \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefitting with \u001b[39m\u001b[38;5;132;01m{best_n_epoches}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epoches, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, y)\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m curr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 24:00:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 24:00:00']]\n",
    "\n",
    "all_ss = {}\n",
    "torch.manual_seed(123)\n",
    "for unique in uniques[:32]:\n",
    "    print(f\"doing {unique}\")\n",
    "    \n",
    "    df, ss = getseries(unique)\n",
    "    print(ss.mean_, ss.scale_)\n",
    "    all_ss[unique] = ss\n",
    "    \n",
    "    test_periods_with_lookback = []\n",
    "    for period in test_periods:\n",
    "        id1 = df.index.to_list().index(period[0])\n",
    "        test_periods_with_lookback.append([df.index[id1-look_back], period[1]])\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)\n",
    "    train_loaders, test_loaders = assemble(df)\n",
    "    best_n_epoches = train(200, train_loaders[0], test_loaders[0])\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)\n",
    "    print('refitting with {best_n_epoches}')\n",
    "    retrain(best_n_epoches, train_loaders[1])\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model_'+unique+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfe34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f9804",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test[(test['x']==0) & (test['y']==0) & (test['direction']=='EB')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d48e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6242b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f79606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for unique in uniques[60:61]:\n",
    "        print(unique)\n",
    "        df, ss = getseries(unique)\n",
    "        print(ss.mean_, ss.scale_)\n",
    "        model.load_state_dict(torch.load('model_'+unique+'.pickle'))\n",
    "        X, y = create_dataset(df['congestion-median-normalized'])\n",
    "        print(X)\n",
    "        predict = np.zeros(36)\n",
    "        for i in range(36)[0:1]:\n",
    "            X = torch.tensor(X, dtype=torch.float32).reshape(1,-1,1)\n",
    "            print(X.shape)\n",
    "            h0 = model.initHidden(1)\n",
    "            print(model.forward(X,h0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f103ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "        X, y = create_dataset(df['congestion-median-normalized'], look_back=look_back)\n",
    "        print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd7a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(X[0],dtype=torch.float32).reshape(1,-1,1)\n",
    "h0 = model.initHidden(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = torch.tensor(y[0],dtype=torch.float32).reshape(1,-1,1)\n",
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be911348",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.forward(x,h0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(y_pred, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dafc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = y_pred.detach().numpy()\n",
    "y_pred = y_pred.reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347f70c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = y_target.detach().numpy().reshape((1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred.T, y_target.T, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c04db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(look_back), y_pred.T)\n",
    "plt.plot(range(look_back), y_target.T,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb508261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss += criterion(output[:,-1,:],y[:,-1,:]).item() * len(x)\n",
    "            n += len(x)\n",
    "        loss /= n\n",
    "    return loss\n",
    "\n",
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "        \n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            print(output[-1,-1,:],y[-1,-1,:])\n",
    "            loss = criterion(output[:,-1,:], y[:,-1,:])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            curr_loss += loss*len(x)\n",
    "            n += len(x)\n",
    "        \n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        test_loss = evaluate(test_loader)\n",
    "#         if (epoch % 20 == 0):  print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "            \n",
    "        if epoch > best_n_epoches + 10:\n",
    "            print('early stop')\n",
    "            break\n",
    "    return best_n_epoches\n",
    "\n",
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b99480f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing 00EB\n",
      "[0.95494673] [11.57954203]\n",
      "2.0765115933378677\n",
      "0.5220459717995566\n",
      "tensor([0.0686], grad_fn=<SliceBackward0>) tensor([0.4789])\n",
      "tensor([-0.0465], grad_fn=<SliceBackward0>) tensor([-0.2984])\n",
      "tensor([-0.0559], grad_fn=<SliceBackward0>) tensor([-0.5143])\n",
      "tensor([-0.0905], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.0965], grad_fn=<SliceBackward0>) tensor([0.3061])\n",
      "tensor([-0.1417], grad_fn=<SliceBackward0>) tensor([-0.1256])\n",
      "tensor([-0.1241], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([-0.1341], grad_fn=<SliceBackward0>) tensor([-0.2552])\n",
      "tensor([-0.0786], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([-0.0677], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([-0.0790], grad_fn=<SliceBackward0>) tensor([-1.8097])\n",
      "tensor([-0.1258], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1022], grad_fn=<SliceBackward0>) tensor([0.3493])\n",
      "tensor([-0.0638], grad_fn=<SliceBackward0>) tensor([-1.0324])\n",
      "tensor([-0.0211], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([-0.0620], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "tensor([-0.1368], grad_fn=<SliceBackward0>) tensor([0.5220])\n",
      "tensor([-0.0316], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([-0.0897], grad_fn=<SliceBackward0>) tensor([-0.2552])\n",
      "tensor([-0.0605], grad_fn=<SliceBackward0>) tensor([-0.4279])\n",
      "tensor([0.2172], grad_fn=<SliceBackward0>) tensor([1.1266])\n",
      "tensor([0.1421], grad_fn=<SliceBackward0>) tensor([1.3856])\n",
      "tensor([0.1397], grad_fn=<SliceBackward0>) tensor([0.5220])\n",
      "current 0 training loss=0.8391989469528198 test loss = 0.8377510905265808\n",
      "updating best loss 0 training loss=0.8391989469528198 test loss = 0.8377510905265808\n",
      "tensor([-0.1526], grad_fn=<SliceBackward0>) tensor([0.4789])\n",
      "tensor([0.1980], grad_fn=<SliceBackward0>) tensor([-0.2984])\n",
      "tensor([-0.1710], grad_fn=<SliceBackward0>) tensor([-0.5143])\n",
      "tensor([-0.2170], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.0354], grad_fn=<SliceBackward0>) tensor([0.3061])\n",
      "tensor([-0.3485], grad_fn=<SliceBackward0>) tensor([-0.1256])\n",
      "tensor([-0.2225], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([-0.1469], grad_fn=<SliceBackward0>) tensor([-0.2552])\n",
      "tensor([0.0926], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.4327], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.5689], grad_fn=<SliceBackward0>) tensor([-1.8097])\n",
      "tensor([-0.1021], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.0400], grad_fn=<SliceBackward0>) tensor([0.3493])\n",
      "tensor([0.0255], grad_fn=<SliceBackward0>) tensor([-1.0324])\n",
      "tensor([0.2728], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([-0.1951], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "tensor([-0.3959], grad_fn=<SliceBackward0>) tensor([0.5220])\n",
      "tensor([-0.0451], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([-0.2066], grad_fn=<SliceBackward0>) tensor([-0.2552])\n",
      "tensor([-0.3288], grad_fn=<SliceBackward0>) tensor([-0.4279])\n",
      "tensor([0.4905], grad_fn=<SliceBackward0>) tensor([1.1266])\n",
      "tensor([0.3986], grad_fn=<SliceBackward0>) tensor([1.3856])\n",
      "tensor([0.3520], grad_fn=<SliceBackward0>) tensor([0.5220])\n",
      "current 1 training loss=0.8152946829795837 test loss = 0.8851481676101685\n",
      "tensor([-0.5065], grad_fn=<SliceBackward0>) tensor([0.4789])\n",
      "tensor([0.5470], grad_fn=<SliceBackward0>) tensor([-0.2984])\n",
      "tensor([-0.4120], grad_fn=<SliceBackward0>) tensor([-0.5143])\n",
      "tensor([-0.3425], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.0761], grad_fn=<SliceBackward0>) tensor([0.3061])\n",
      "tensor([-0.4262], grad_fn=<SliceBackward0>) tensor([-0.1256])\n",
      "tensor([-0.1931], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.1378], grad_fn=<SliceBackward0>) tensor([-0.2552])\n",
      "tensor([0.3054], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.5706], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([0.7508], grad_fn=<SliceBackward0>) tensor([-1.8097])\n",
      "tensor([-0.1742], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.1336], grad_fn=<SliceBackward0>) tensor([0.3493])\n",
      "tensor([-0.1205], grad_fn=<SliceBackward0>) tensor([-1.0324])\n",
      "tensor([0.4974], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([-0.2167], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "tensor([-0.6362], grad_fn=<SliceBackward0>) tensor([0.5220])\n",
      "tensor([0.0922], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([-0.2699], grad_fn=<SliceBackward0>) tensor([-0.2552])\n",
      "tensor([-0.4026], grad_fn=<SliceBackward0>) tensor([-0.4279])\n",
      "tensor([0.5337], grad_fn=<SliceBackward0>) tensor([1.1266])\n",
      "tensor([0.4589], grad_fn=<SliceBackward0>) tensor([1.3856])\n",
      "tensor([0.2735], grad_fn=<SliceBackward0>) tensor([0.5220])\n",
      "current 2 training loss=0.816525936126709 test loss = 0.8712764382362366\n",
      "tensor([-0.2992], grad_fn=<SliceBackward0>) tensor([0.4789])\n",
      "tensor([0.5959], grad_fn=<SliceBackward0>) tensor([-0.2984])\n",
      "tensor([-0.3999], grad_fn=<SliceBackward0>) tensor([-0.5143])\n",
      "tensor([-0.5764], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.2814], grad_fn=<SliceBackward0>) tensor([0.3061])\n",
      "tensor([-0.3399], grad_fn=<SliceBackward0>) tensor([-0.1256])\n",
      "tensor([-0.2839], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2794], grad_fn=<SliceBackward0>) tensor([-0.2552])\n",
      "tensor([0.2549], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.7108], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([1.1229], grad_fn=<SliceBackward0>) tensor([-1.8097])\n",
      "tensor([-0.1672], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.0484], grad_fn=<SliceBackward0>) tensor([0.3493])\n",
      "tensor([-0.0218], grad_fn=<SliceBackward0>) tensor([-1.0324])\n",
      "tensor([0.7196], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([-0.3258], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "tensor([-0.6283], grad_fn=<SliceBackward0>) tensor([0.5220])\n",
      "tensor([0.0119], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([-0.3159], grad_fn=<SliceBackward0>) tensor([-0.2552])\n",
      "tensor([-0.3872], grad_fn=<SliceBackward0>) tensor([-0.4279])\n",
      "tensor([0.5403], grad_fn=<SliceBackward0>) tensor([1.1266])\n",
      "tensor([0.6026], grad_fn=<SliceBackward0>) tensor([1.3856])\n",
      "tensor([0.2825], grad_fn=<SliceBackward0>) tensor([0.5220])\n",
      "current 3 training loss=0.8035104870796204 test loss = 0.8537973761558533\n",
      "tensor([-0.1246], grad_fn=<SliceBackward0>) tensor([0.4789])\n",
      "tensor([0.4597], grad_fn=<SliceBackward0>) tensor([-0.2984])\n",
      "tensor([-0.3377], grad_fn=<SliceBackward0>) tensor([-0.5143])\n",
      "tensor([-0.3730], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([-0.1174], grad_fn=<SliceBackward0>) tensor([0.3061])\n",
      "tensor([-0.4629], grad_fn=<SliceBackward0>) tensor([-0.1256])\n",
      "tensor([-0.2769], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.2026], grad_fn=<SliceBackward0>) tensor([-0.2552])\n",
      "tensor([0.1439], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([0.9286], grad_fn=<SliceBackward0>) tensor([0.7811])\n",
      "tensor([1.2269], grad_fn=<SliceBackward0>) tensor([-1.8097])\n",
      "tensor([-0.1357], grad_fn=<SliceBackward0>) tensor([0.2630])\n",
      "tensor([0.0422], grad_fn=<SliceBackward0>) tensor([0.3493])\n",
      "tensor([-0.1381], grad_fn=<SliceBackward0>) tensor([-1.0324])\n",
      "tensor([0.8606], grad_fn=<SliceBackward0>) tensor([-0.0825])\n",
      "tensor([-0.4368], grad_fn=<SliceBackward0>) tensor([-0.9461])\n",
      "tensor([-0.7675], grad_fn=<SliceBackward0>) tensor([0.5220])\n",
      "tensor([0.0490], grad_fn=<SliceBackward0>) tensor([0.1766])\n",
      "tensor([-0.2714], grad_fn=<SliceBackward0>) tensor([-0.2552])\n",
      "tensor([-0.3669], grad_fn=<SliceBackward0>) tensor([-0.4279])\n",
      "tensor([0.5379], grad_fn=<SliceBackward0>) tensor([1.1266])\n",
      "tensor([0.5953], grad_fn=<SliceBackward0>) tensor([1.3856])\n",
      "tensor([0.2889], grad_fn=<SliceBackward0>) tensor([0.5220])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     22\u001b[0m train_loaders, test_loaders \u001b[38;5;241m=\u001b[39m assemble(df)\n\u001b[0;32m---> 23\u001b[0m best_n_epoches \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefitting with \u001b[39m\u001b[38;5;132;01m{best_n_epoches}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epoches, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:], y[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:])\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m curr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(x)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cs2020/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 24:00:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 24:00:00']]\n",
    "\n",
    "\n",
    "linear_node = 32\n",
    "look_back = 72\n",
    "all_ss = {}\n",
    "torch.manual_seed(123)\n",
    "for unique in uniques[0:1]:\n",
    "    print(f\"doing {unique}\")\n",
    "    \n",
    "    df, ss = getseries(unique)\n",
    "    print(ss.mean_, ss.scale_)\n",
    "    all_ss[unique] = ss\n",
    "    \n",
    "    test_periods_with_lookback = []\n",
    "    for period in test_periods:\n",
    "        id1 = df.index.to_list().index(period[0])\n",
    "        test_periods_with_lookback.append([df.index[id1-look_back], period[1]])\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)\n",
    "    train_loaders, test_loaders = assemble(df)\n",
    "    best_n_epoches = train(200, train_loaders[0], test_loaders[0])\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)\n",
    "    print('refitting with {best_n_epoches}')\n",
    "    retrain(best_n_epoches, train_loaders[1])\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model_'+unique+'.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
