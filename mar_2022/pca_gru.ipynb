{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e298c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7764c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 72\n",
    "batch_size = 512\n",
    "linear_node = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c79162",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('train.csv', index_col='row_id')\n",
    "def preprocess(dat):\n",
    "    time_mapper = {}\n",
    "    ii = 0\n",
    "    for h in range(24):\n",
    "        for mm in ['00','20','40']:\n",
    "            hh = '{0:02d}'.format(h)\n",
    "            time_mapper[hh+':'+mm] = ii\n",
    "            ii += 1\n",
    "\n",
    "    dat['unique'] = dat['x'].astype(str) + dat['y'].astype(str) + dat['direction']\n",
    "    uniques = dat['unique'].unique()\n",
    "    dat['day'] = pd.to_datetime(dat['time']).dt.weekday\n",
    "    dat['time_stamp'] = dat['time'].apply(lambda x:time_mapper[x.split()[1][:5]])\n",
    "\n",
    "    tmp = dat.groupby(['unique','day','time_stamp']).agg({'congestion':np.median})\n",
    "    median_mapper = tmp.to_dict()['congestion']\n",
    "    dat['median'] = dat.apply(lambda x: \\\n",
    "                              median_mapper[x['unique'],x['day'],x['time_stamp']], axis=1)\n",
    "    dat['congestion-median'] = dat['congestion'] - dat['median']\n",
    "    \n",
    "    all_time = pd.DataFrame(pd.date_range('1991-04-01 00:00:00', '1991-09-30 11:40:00', freq='20Min'), columns=['time'])\n",
    "    all_time['time'] = all_time['time'].astype(str)\n",
    "    \n",
    "    return uniques, median_mapper, time_mapper, all_time\n",
    "uniques, median_mapper, time_mapper, all_time = preprocess(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb05e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getseries(unique):\n",
    "    df = dat.loc[dat['unique']==unique, ['time', 'congestion-median']]\n",
    "    df = pd.merge(all_time, df, left_on='time', right_on='time', how='outer')\n",
    "    df = df.set_index('time')\n",
    "    df['congestion-median'] = df['congestion-median'].fillna(0)\n",
    "    ss = StandardScaler()\n",
    "    df['congestion-median-normalized'] = ss.fit_transform(df['congestion-median'].values.reshape(-1,1)).reshape(-1)\n",
    "    return df, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55b3d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df,_ = getseries('00EB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "651b08f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12636, 13140)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 24:00:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 24:00:00']]\n",
    "id1 = df.index.to_list().index(period[0])\n",
    "id1, len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a24378",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = []\n",
    "for unique in uniques:\n",
    "    df, ss = getseries(unique)\n",
    "    data_all.append(df['congestion-median-normalized'].values)\n",
    "data_all = np.array(data_all).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5bd696e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13140, 65)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85de2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "data_all_pca = pca.fit_transform(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "946eded4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x141ff0dc0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c82ed4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=5):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "def assemble(dat,icol):\n",
    "    train_loaders, test_loaders = [], []\n",
    "    \n",
    "    # for train/test\n",
    "    train = dat[:12636,icol]\n",
    "    test = dat[12636:,icol]\n",
    "\n",
    "    X, y = create_dataset(train, look_back=look_back)\n",
    "    train_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        train_dataset.append((torch.tensor(X[i].reshape(-1,1),dtype=torch.float32),\n",
    "                              torch.tensor(y[i].reshape(-1,),dtype=torch.float32)))\n",
    "    train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False))\n",
    "\n",
    "    X, y = create_dataset(test, look_back=look_back)\n",
    "    test_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        test_dataset.append((torch.tensor(X[i].reshape(-1,1),dtype=torch.float32),\n",
    "                             torch.tensor(y[i].reshape(-1,),dtype=torch.float32)))\n",
    "    test_loaders.append(DataLoader(test_dataset, batch_size=batch_size, drop_last=False))\n",
    "    \n",
    "    train = dat[:,icol]\n",
    "    X, y = create_dataset(train, look_back=look_back)\n",
    "    train_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        train_dataset.append((torch.tensor(X[i].reshape(-1,1),dtype=torch.float32),\n",
    "                              torch.tensor(y[i].reshape(-1,),dtype=torch.float32)))\n",
    "    train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7f20980",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_size, output_feature, num_layers=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_feature, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers, dropout=0.2)\n",
    "        ''' gru input is (N,L,H_in=H_hidden), output is (N,L,H_hidden), hidden is (num_layers, h_hidden)'''\n",
    "        self.linear_out = nn.Linear(hidden_size, output_feature)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        ''' X is in the shape of (N,L,input_feature) '''\n",
    "        output = F.relu(self.linear(input))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear_out(F.relu(output))\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros((self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "495d13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss += criterion(output[:,-1,:],y).item() * len(x)\n",
    "            n += len(x)\n",
    "        loss /= n\n",
    "    return loss\n",
    "\n",
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "\n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            #print(output[-1,-1,:],y[-1])\n",
    "            loss = criterion(output[:,-1,:], y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss += loss.item()*len(x)\n",
    "            n += len(x)\n",
    "\n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        test_loss = evaluate(test_loader)\n",
    "        if (epoch % 20 == 0):  print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "\n",
    "        if epoch > best_n_epoches + 50:\n",
    "            print('early stop')\n",
    "            break\n",
    "    return best_n_epoches\n",
    "\n",
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "\n",
    "        curr_loss = 0.0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            loss = criterion(output[:,-1,:], y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss += loss.item()*len(x)\n",
    "\n",
    "    curr_loss /= len(train_loader.dataset)\n",
    "    return curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ac83997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current 0 training loss=1.7029120922088623 test loss = 1.8074543476104736\n",
      "updating best loss 0 training loss=1.7029120922088623 test loss = 1.8074543476104736\n",
      "updating best loss 1 training loss=1.518017292022705 test loss = 1.5907343626022339\n",
      "updating best loss 2 training loss=1.37233304977417 test loss = 1.4240084886550903\n",
      "updating best loss 3 training loss=1.2918810844421387 test loss = 1.3511874675750732\n",
      "updating best loss 4 training loss=1.244005799293518 test loss = 1.3190361261367798\n",
      "updating best loss 5 training loss=1.2235629558563232 test loss = 1.2956069707870483\n",
      "updating best loss 6 training loss=1.201930284500122 test loss = 1.2684249877929688\n",
      "updating best loss 7 training loss=1.1919437646865845 test loss = 1.2425135374069214\n",
      "updating best loss 8 training loss=1.1770130395889282 test loss = 1.2295634746551514\n",
      "updating best loss 9 training loss=1.1745543479919434 test loss = 1.2066650390625\n",
      "current 0 training loss=0.9216070175170898 test loss = 0.8999358415603638\n",
      "updating best loss 0 training loss=0.9216070175170898 test loss = 0.8999358415603638\n",
      "updating best loss 1 training loss=0.8751259446144104 test loss = 0.8685234189033508\n",
      "updating best loss 2 training loss=0.8525383472442627 test loss = 0.8124837875366211\n",
      "updating best loss 3 training loss=0.831932544708252 test loss = 0.7827246189117432\n",
      "current 0 training loss=1.0998996496200562 test loss = 0.9064019322395325\n",
      "updating best loss 0 training loss=1.0998996496200562 test loss = 0.9064019322395325\n",
      "updating best loss 1 training loss=1.0266298055648804 test loss = 0.8383778929710388\n",
      "updating best loss 2 training loss=0.9971324801445007 test loss = 0.8061153888702393\n",
      "updating best loss 3 training loss=0.9719278216362 test loss = 0.7968602776527405\n",
      "updating best loss 4 training loss=0.9485164284706116 test loss = 0.7920863628387451\n",
      "updating best loss 5 training loss=0.9236516952514648 test loss = 0.7860193848609924\n",
      "updating best loss 6 training loss=0.90395188331604 test loss = 0.7813836932182312\n",
      "updating best loss 7 training loss=0.8970034122467041 test loss = 0.77726811170578\n",
      "updating best loss 8 training loss=0.8857394456863403 test loss = 0.7746496796607971\n",
      "updating best loss 9 training loss=0.8733572959899902 test loss = 0.7733572721481323\n",
      "current 0 training loss=0.7828058004379272 test loss = 0.883988618850708\n",
      "updating best loss 0 training loss=0.7828058004379272 test loss = 0.883988618850708\n",
      "updating best loss 1 training loss=0.7661303877830505 test loss = 0.8418172001838684\n",
      "updating best loss 2 training loss=0.761433482170105 test loss = 0.8101171255111694\n",
      "updating best loss 3 training loss=0.7507696747779846 test loss = 0.7921558022499084\n",
      "updating best loss 4 training loss=0.7389330863952637 test loss = 0.7854079604148865\n",
      "updating best loss 5 training loss=0.7425622940063477 test loss = 0.7844670414924622\n",
      "updating best loss 6 training loss=0.7264010310173035 test loss = 0.7762904763221741\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m train_loaders, test_loaders \u001b[38;5;241m=\u001b[39m assemble(data_all_pca, icol)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)    \n\u001b[0;32m----> 5\u001b[0m best_n_epoches \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_epoches\u001b[39m\u001b[38;5;124m'\u001b[39m: best_n_epoches,\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict()},\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_pca_train_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(icol)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(\u001b[38;5;241m1\u001b[39m, linear_node, \u001b[38;5;241m1\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)    \n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n_epoches, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#print(output[-1,-1,:],y[-1])\u001b[39;00m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:], y)\n\u001b[0;32m---> 30\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m()\n\u001b[1;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for icol in range(65):\n",
    "    train_loaders, test_loaders = assemble(data_all_pca, icol)\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)    \n",
    "    best_n_epoches = train(10, train_loaders[0], test_loaders[0])\n",
    "    torch.save({'best_epoches': best_n_epoches,\n",
    "                'model': model.state_dict()},\n",
    "                'model_pca_train_'+str(icol)+'.pickle')\n",
    "    \n",
    "    model = MyModel(1, linear_node, 1, num_layers=3)    \n",
    "    loss = retrain(best_n_epoches, train_loaders[1])\n",
    "    torch.save({'loss': loss,\n",
    "                'best_epoches': best_n_epoches,\n",
    "                'model': model.state_dict()},\n",
    "                'model_pca_all_'+str(icol)+'.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
