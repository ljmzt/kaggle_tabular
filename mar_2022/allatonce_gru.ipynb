{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e298c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7764c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 72*3\n",
    "n_features = 65\n",
    "linear_node = 32\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c79162",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('train.csv', index_col='row_id')\n",
    "def preprocess(dat):\n",
    "    time_mapper = {}\n",
    "    ii = 0\n",
    "    for h in range(24):\n",
    "        for mm in ['00','20','40']:\n",
    "            hh = '{0:02d}'.format(h)\n",
    "            time_mapper[hh+':'+mm] = ii\n",
    "            ii += 1\n",
    "\n",
    "    dat['unique'] = dat['x'].astype(str) + dat['y'].astype(str) + dat['direction']\n",
    "    uniques = dat['unique'].unique()\n",
    "    dat['day'] = pd.to_datetime(dat['time']).dt.weekday\n",
    "    dat['time_stamp'] = dat['time'].apply(lambda x:time_mapper[x.split()[1][:5]])\n",
    "\n",
    "    tmp = dat.groupby(['unique','day','time_stamp']).agg({'congestion':np.median})\n",
    "    median_mapper = tmp.to_dict()['congestion']\n",
    "    dat['median'] = dat.apply(lambda x: \\\n",
    "                              median_mapper[x['unique'],x['day'],x['time_stamp']], axis=1)\n",
    "    dat['congestion-median'] = dat['congestion'] - dat['median']\n",
    "    \n",
    "    all_time = pd.DataFrame(pd.date_range('1991-04-01 00:00:00', '1991-09-30 11:40:00', freq='20Min'), columns=['time'])\n",
    "    all_time['time'] = all_time['time'].astype(str)\n",
    "    \n",
    "    return uniques, median_mapper, time_mapper, all_time\n",
    "uniques, median_mapper, time_mapper, all_time = preprocess(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb05e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getseries(unique):\n",
    "    df = dat.loc[dat['unique']==unique, ['time', 'congestion-median']]\n",
    "    df = pd.merge(all_time, df, left_on='time', right_on='time', how='outer')\n",
    "    df = df.set_index('time')\n",
    "    df['congestion-median'] = df['congestion-median'].fillna(0)\n",
    "    ss = StandardScaler()\n",
    "    df['congestion-median-normalized'] = ss.fit_transform(df['congestion-median'].values.reshape(-1,1)).reshape(-1)\n",
    "    return df, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b3d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df,_ = getseries('00EB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "651b08f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12636, 13140)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_periods = [\n",
    "    ['1991-09-16 12:00:00', '1991-09-16 24:00:00'],\n",
    "    ['1991-09-23 12:00:00', '1991-09-23 24:00:00']]\n",
    "id1 = df.index.to_list().index(test_periods[1][0])\n",
    "id1, len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92a24378",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = []\n",
    "for unique in uniques:\n",
    "    df, ss = getseries(unique)\n",
    "    data_all.append(df['congestion-median-normalized'].values)\n",
    "data_all = np.array(data_all).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5bd696e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13140, 65)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e013393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=5):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back),:]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i+1:i+look_back+1,:])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ce73210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 5, 65), (4, 5, 65))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = create_dataset(data_all[:10])\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c82ed4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(dat):\n",
    "    train_loaders, test_loaders = [], []\n",
    "    \n",
    "    # for train/test\n",
    "    train = dat[:12636]\n",
    "    test = dat[12636:]\n",
    "\n",
    "    X, y = create_dataset(train, look_back=look_back)\n",
    "    train_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        train_dataset.append((torch.tensor(X[i],dtype=torch.float32),\n",
    "                              torch.tensor(y[i],dtype=torch.float32)))\n",
    "    train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False))\n",
    "\n",
    "    X, y = create_dataset(test, look_back=look_back)\n",
    "    test_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        test_dataset.append((torch.tensor(X[i],dtype=torch.float32),\n",
    "                             torch.tensor(y[i],dtype=torch.float32)))\n",
    "    test_loaders.append(DataLoader(test_dataset, batch_size=batch_size, drop_last=False))\n",
    "    \n",
    "    train = dat[:]\n",
    "    X, y = create_dataset(train, look_back=look_back)\n",
    "    train_dataset = []\n",
    "    for i in range(len(X)):\n",
    "        train_dataset.append((torch.tensor(X[i],dtype=torch.float32),\n",
    "                              torch.tensor(y[i],dtype=torch.float32)))\n",
    "    train_loaders.append(DataLoader(train_dataset, batch_size=batch_size, drop_last=False))\n",
    "        \n",
    "    return train_loaders, test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c506be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders, test_loaders = assemble(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72f07a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 72, 65]), torch.Size([32, 65]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = next(iter(train_loaders[0]))\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1824f956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 72, 65]), torch.Size([32, 65]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = next(iter(test_loaders[0]))\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7f20980",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_size, output_feature, num_layers=1):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_feature, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=num_layers, dropout=0.2)\n",
    "        ''' gru input is (N,L,H_in=H_hidden), output is (N,L,H_hidden), hidden is (num_layers, h_hidden)'''\n",
    "        self.linear_out = nn.Linear(hidden_size, output_feature)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        ''' X is in the shape of (N,L,input_feature) '''\n",
    "        output = F.relu(self.linear(input))\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.linear_out(F.relu(output))\n",
    "        return output\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros((self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "495d13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        loss = 0\n",
    "        n = 0\n",
    "        for batch, (x, y) in enumerate(test_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "#             loss += criterion(output[:,-1,:],y).item() * len(x)\n",
    "            loss += criterion(output,y).item() * len(x)\n",
    "            n += len(x)\n",
    "        loss /= n\n",
    "    return loss\n",
    "\n",
    "def train(n_epoches, train_loader, test_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    best_test_loss = 100.0\n",
    "    for epoch in range(n_epoches):\n",
    "\n",
    "        curr_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "            #print(output[-1,-1,:],y[-1])\n",
    "#             loss = criterion(output[:,-1,:], y)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss += loss.item()*len(x)\n",
    "#             print(f\"{batch} {loss.item()}\")\n",
    "\n",
    "        curr_loss /= len(train_loader.dataset)\n",
    "        test_loss = evaluate(test_loader)\n",
    "#         if (epoch % 20 == 0):  print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        print(f'current {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "        if test_loss < best_test_loss:\n",
    "            best_n_epoches = epoch + 1\n",
    "            best_test_loss = test_loss\n",
    "            print(f'updating best loss {epoch} training loss={loss.item()} test loss = {test_loss}')\n",
    "\n",
    "        if epoch > best_n_epoches + 50:\n",
    "            print('early stop')\n",
    "            break\n",
    "    return best_n_epoches\n",
    "\n",
    "def retrain(n_epoches, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(n_epoches):\n",
    "\n",
    "        curr_loss = 0.0\n",
    "        for batch, (x, y) in enumerate(train_loader):\n",
    "            h0 = model.initHidden(len(x))\n",
    "            output = model.forward(x, h0)\n",
    "#             loss = criterion(output[:,-1,:], y)\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            curr_loss += loss.item()*len(x)\n",
    "\n",
    "    curr_loss /= len(train_loader.dataset)\n",
    "    return curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ac83997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current 0 training loss=0.7065339684486389 test loss = 0.6783859962370338\n",
      "updating best loss 0 training loss=0.7065339684486389 test loss = 0.6783859962370338\n",
      "current 1 training loss=0.7038140892982483 test loss = 0.6750827041237196\n",
      "updating best loss 1 training loss=0.7038140892982483 test loss = 0.6750827041237196\n",
      "current 2 training loss=0.7022724151611328 test loss = 0.6743493736413297\n",
      "updating best loss 2 training loss=0.7022724151611328 test loss = 0.6743493736413297\n",
      "current 3 training loss=0.7011995315551758 test loss = 0.6734472149755897\n",
      "updating best loss 3 training loss=0.7011995315551758 test loss = 0.6734472149755897\n",
      "current 4 training loss=0.7005960941314697 test loss = 0.6725911743134156\n",
      "updating best loss 4 training loss=0.7005960941314697 test loss = 0.6725911743134156\n",
      "current 5 training loss=0.6997776031494141 test loss = 0.6719902728908154\n",
      "updating best loss 5 training loss=0.6997776031494141 test loss = 0.6719902728908154\n",
      "current 6 training loss=0.6982415318489075 test loss = 0.6718038119090144\n",
      "updating best loss 6 training loss=0.6982415318489075 test loss = 0.6718038119090144\n",
      "current 7 training loss=0.6975725293159485 test loss = 0.6719136861143212\n",
      "current 8 training loss=0.6969476342201233 test loss = 0.6722885505247614\n",
      "current 9 training loss=0.6961270570755005 test loss = 0.672324699184205\n",
      "current 10 training loss=0.6965385675430298 test loss = 0.6726434949383088\n",
      "current 11 training loss=0.695724606513977 test loss = 0.6728879763689606\n",
      "current 12 training loss=0.6957146525382996 test loss = 0.672863880308663\n",
      "current 13 training loss=0.6954588294029236 test loss = 0.6729223516344609\n",
      "current 14 training loss=0.6951419711112976 test loss = 0.6731155230193188\n",
      "current 15 training loss=0.6954981088638306 test loss = 0.6730493209503253\n",
      "current 16 training loss=0.6943120956420898 test loss = 0.6732039590745853\n",
      "current 17 training loss=0.6938097476959229 test loss = 0.6729585036168115\n",
      "current 18 training loss=0.693743109703064 test loss = 0.6730819047535753\n",
      "current 19 training loss=0.6931799054145813 test loss = 0.673264248861253\n",
      "current 20 training loss=0.6928407549858093 test loss = 0.6733567473780403\n",
      "current 21 training loss=0.6921943426132202 test loss = 0.673653051205213\n",
      "current 22 training loss=0.6914079785346985 test loss = 0.6742372122375807\n",
      "current 23 training loss=0.6920326352119446 test loss = 0.6743949560338196\n",
      "current 24 training loss=0.6916043758392334 test loss = 0.6754712761486864\n",
      "current 25 training loss=0.6900983452796936 test loss = 0.6761061896015127\n",
      "current 26 training loss=0.6906968951225281 test loss = 0.6768968805203455\n",
      "current 27 training loss=0.6910236477851868 test loss = 0.6771821319433871\n",
      "current 28 training loss=0.6896421313285828 test loss = 0.6774896354210086\n",
      "current 29 training loss=0.6899136900901794 test loss = 0.677759158902052\n",
      "current 30 training loss=0.6902744174003601 test loss = 0.6784084673957957\n",
      "current 31 training loss=0.6886263489723206 test loss = 0.6787484778344424\n",
      "current 32 training loss=0.6885119676589966 test loss = 0.6790775648392867\n",
      "current 33 training loss=0.6883538365364075 test loss = 0.6786159789936053\n",
      "current 34 training loss=0.6881745457649231 test loss = 0.6789529184431149\n",
      "current 35 training loss=0.688179612159729 test loss = 0.6782700726794865\n",
      "current 36 training loss=0.6877571940422058 test loss = 0.6785733772487175\n",
      "current 37 training loss=0.687395453453064 test loss = 0.6781852843454075\n",
      "current 38 training loss=0.6884681582450867 test loss = 0.6778918722780739\n",
      "current 39 training loss=0.6886276602745056 test loss = 0.6780221429020685\n",
      "current 40 training loss=0.6861250996589661 test loss = 0.6785783915154194\n",
      "current 41 training loss=0.6881232857704163 test loss = 0.6784633703348113\n",
      "current 42 training loss=0.6871790289878845 test loss = 0.6783434042531854\n",
      "current 43 training loss=0.6874053478240967 test loss = 0.6790968514070278\n",
      "current 44 training loss=0.6868765950202942 test loss = 0.6793779423427914\n",
      "current 45 training loss=0.6874709725379944 test loss = 0.6790068758073999\n",
      "current 46 training loss=0.6860417723655701 test loss = 0.6794254389374098\n",
      "current 47 training loss=0.6865800023078918 test loss = 0.6813662531899243\n",
      "current 48 training loss=0.6863784790039062 test loss = 0.6810968474643987\n",
      "current 49 training loss=0.6868790984153748 test loss = 0.6809777941437963\n",
      "current 50 training loss=0.6861847639083862 test loss = 0.6822194097764817\n",
      "current 51 training loss=0.687105119228363 test loss = 0.6813894576727305\n",
      "current 52 training loss=0.6860682368278503 test loss = 0.6812577039937939\n",
      "current 53 training loss=0.6869094371795654 test loss = 0.6837731961173878\n",
      "current 54 training loss=0.6860057711601257 test loss = 0.682804505584132\n",
      "current 55 training loss=0.6860190033912659 test loss = 0.6814362633103693\n",
      "current 56 training loss=0.6852176189422607 test loss = 0.6838708274871215\n",
      "current 57 training loss=0.6861656308174133 test loss = 0.6822530440337151\n",
      "current 58 training loss=0.6852048635482788 test loss = 0.6831536936843021\n",
      "early stop\n"
     ]
    }
   ],
   "source": [
    "train_loaders, test_loaders = assemble(data_all)\n",
    "\n",
    "model = MyModel(n_features, linear_node, n_features, num_layers=3)    \n",
    "best_n_epoches = train(300, train_loaders[0], test_loaders[0])\n",
    "torch.save({'best_epoches': best_n_epoches,\n",
    "            'model': model.state_dict()},\n",
    "            'model_allgru_train.pickle')\n",
    "\n",
    "model = MyModel(n_features, linear_node, n_features, num_layers=3)    \n",
    "loss = retrain(best_n_epoches, train_loaders[1])\n",
    "torch.save({'loss': loss,\n",
    "            'best_epoches': best_n_epoches,\n",
    "            'model': model.state_dict()},\n",
    "            'model_allgru.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "007206b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (linear): Linear(in_features=65, out_features=16, bias=True)\n",
       "  (gru): GRU(16, 16, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (linear_out): Linear(in_features=16, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
